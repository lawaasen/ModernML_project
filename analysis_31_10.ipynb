{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b60edcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPREHENSIVE DIAGNOSTIC ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "[1] LOADING DATA\n",
      "Total receivals: 122383\n",
      "Date range: 2004-06-15 11:34:00 to 2024-12-19 13:36:00\n",
      "Unique rm_ids: 203\n",
      "\n",
      "[2] RECREATING TRAINING DATA\n",
      "--------------------------------------------------------------------------------\n",
      "Total training samples: 2725\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS 1: TARGET DISTRIBUTION\n",
      "================================================================================\n",
      "\n",
      "Target Statistics:\n",
      "count    2.725000e+03\n",
      "mean     3.048785e+05\n",
      "std      7.993253e+05\n",
      "min      0.000000e+00\n",
      "25%      0.000000e+00\n",
      "50%      2.556000e+04\n",
      "75%      1.681800e+05\n",
      "max      8.900645e+06\n",
      "Name: target, dtype: float64\n",
      "\n",
      "Percentage of ZERO targets: 33.4%\n",
      "Percentage of NON-ZERO targets: 66.6%\n",
      "\n",
      "Non-zero target statistics:\n",
      "count    1.816000e+03\n",
      "mean     4.574856e+05\n",
      "std      9.428974e+05\n",
      "min      1.370000e+02\n",
      "25%      2.556000e+04\n",
      "50%      9.659700e+04\n",
      "75%      4.062238e+05\n",
      "max      8.900645e+06\n",
      "Name: target, dtype: float64\n",
      "\n",
      "Target percentiles:\n",
      "  10th percentile: 0 kg\n",
      "  25th percentile: 0 kg\n",
      "  50th percentile: 25,560 kg\n",
      "  75th percentile: 168,180 kg\n",
      "  90th percentile: 938,148 kg\n",
      "  95th percentile: 1,684,552 kg\n",
      "  99th percentile: 4,120,451 kg\n",
      "\n",
      "✅ Saved: 01_target_distribution.png\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS 2: TARGET BY FORECAST HORIZON\n",
      "================================================================================\n",
      "\n",
      "Target statistics by horizon:\n",
      "                  count           mean           std   median   zero_pct\n",
      "forecast_horizon                                                        \n",
      "7                   545   31630.220183  7.233981e+04      0.0  59.082569\n",
      "30                  545  147938.988991  3.149113e+05  23316.0  36.697248\n",
      "60                  545  290364.943119  5.971223e+05  47580.0  27.889908\n",
      "90                  545  420068.598165  8.636388e+05  69301.0  23.486239\n",
      "150                 545  634389.563303  1.330682e+06  96360.0  19.633028\n",
      "\n",
      "✅ Saved: 02_target_by_horizon.png\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS 3: TARGET BY RM_ID\n",
      "================================================================================\n",
      "\n",
      "Top 20 rm_ids by frequency:\n",
      "        count          mean           std   zero_pct        total\n",
      "rm_id                                                            \n",
      "2123.0     55  2.500000e+03  5.046084e+03  80.000000     137500.0\n",
      "3621.0     55  5.556255e+03  9.156965e+03  72.727273     305594.0\n",
      "3142.0     55  4.352400e+04  5.595492e+04  49.090909    2393820.0\n",
      "3201.0     55  2.410400e+04  3.344974e+04  52.727273    1325720.0\n",
      "3265.0     55  9.532255e+04  1.378993e+05  38.181818    5242740.0\n",
      "3282.0     55  8.540833e+05  6.585782e+05   1.818182   46974580.0\n",
      "2124.0     55  3.534545e+03  4.038073e+03  47.272727     194400.0\n",
      "3381.0     55  1.408013e+04  1.431219e+04  34.545455     774407.0\n",
      "3421.0     55  7.379247e+04  7.302639e+04   1.818182    4058586.0\n",
      "3581.0     55  4.232727e+03  9.061728e+03  81.818182     232800.0\n",
      "3601.0     55  1.524564e+03  1.650757e+03  49.090909      83851.0\n",
      "3642.0     55  4.227055e+04  4.778945e+04  34.545455    2324880.0\n",
      "3125.0     55  1.112236e+06  8.429038e+05   1.818182   61172980.0\n",
      "3701.0     55  1.842073e+04  3.207849e+04  69.090909    1013140.0\n",
      "3761.0     55  4.373684e+04  1.120426e+05  85.454545    2405526.0\n",
      "3781.0     55  1.844640e+06  1.725195e+06   1.818182  101455221.0\n",
      "3865.0     55  2.142564e+06  1.669997e+06   1.818182  117841010.0\n",
      "3883.0     55  6.247200e+04  5.587253e+04  25.454545    3435960.0\n",
      "3901.0     55  4.212696e+05  3.382253e+05   3.636364   23169830.0\n",
      "4021.0     55  1.995382e+04  2.724905e+04  54.545455    1097460.0\n",
      "\n",
      "Top 20 rm_ids by total weight:\n",
      "        count          mean           std   zero_pct        total\n",
      "rm_id                                                            \n",
      "2130.0     55  2.777169e+06  2.394289e+06   1.818182  152744304.0\n",
      "3865.0     55  2.142564e+06  1.669997e+06   1.818182  117841010.0\n",
      "3781.0     55  1.844640e+06  1.725195e+06   1.818182  101455221.0\n",
      "3125.0     55  1.112236e+06  8.429038e+05   1.818182   61172980.0\n",
      "3126.0     55  1.093197e+06  8.275720e+05   0.000000   60125840.0\n",
      "3124.0     55  8.867898e+05  6.967074e+05   3.636364   48773440.0\n",
      "3282.0     55  8.540833e+05  6.585782e+05   1.818182   46974580.0\n",
      "3122.0     55  8.401182e+05  6.304277e+05   0.000000   46206500.0\n",
      "3123.0     55  7.147531e+05  5.414060e+05   1.818182   39311420.0\n",
      "3901.0     55  4.212696e+05  3.382253e+05   3.636364   23169830.0\n",
      "2140.0     55  4.107975e+05  4.847345e+05  25.454545   22593860.0\n",
      "2134.0     55  2.943960e+05  2.208919e+05   1.818182   16191780.0\n",
      "2142.0     55  2.166078e+05  1.651766e+05   1.818182   11913428.0\n",
      "2135.0     55  1.594871e+05  1.298644e+05   5.454545    8771789.0\n",
      "2131.0     55  1.094975e+05  8.573198e+04   9.090909    6022360.0\n",
      "3362.0     55  1.074170e+05  1.021480e+05  16.363636    5907936.0\n",
      "3265.0     55  9.532255e+04  1.378993e+05  38.181818    5242740.0\n",
      "2144.0     55  9.367896e+04  7.395682e+04   3.636364    5152343.0\n",
      "2741.0     55  8.764473e+04  7.022563e+04  16.363636    4820460.0\n",
      "4222.0     35  1.368709e+05  1.269635e+05  17.142857    4790480.0\n",
      "\n",
      "RM_IDs that are 100% zeros: 7\n",
      "RM_IDs that are <50% zeros: 36\n",
      "\n",
      "✅ Saved: 03_target_by_rm_id.png\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS 4: TEMPORAL PATTERNS\n",
      "================================================================================\n",
      "\n",
      "Target statistics by month:\n",
      "            count           mean   median   zero_pct\n",
      "train_date                                          \n",
      "2024-01       225  352232.915556  31264.0  31.111111\n",
      "2024-02       225  388659.524444  48328.0  23.555556\n",
      "2024-03       225  393837.155556  48700.0  28.444444\n",
      "2024-04       230  371579.530435  47796.0  31.739130\n",
      "2024-05       240  357007.345833  48320.0  30.000000\n",
      "2024-06       245  340171.930612  48260.0  25.306122\n",
      "2024-07       245  316568.693878  39344.0  26.530612\n",
      "2024-08       260  233179.092308  17280.0  37.307692\n",
      "2024-09       260  291620.526923  24160.0  34.230769\n",
      "2024-10       275  235664.680000  11120.0  40.727273\n",
      "2024-11       295  142974.427119      0.0  51.525424\n",
      "\n",
      "✅ Saved: 04_temporal_patterns.png\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS 5: FEATURE-TARGET RELATIONSHIPS\n",
      "================================================================================\n",
      "\n",
      "Feature correlations with target:\n",
      "target               1.000000\n",
      "rate_90              0.707991\n",
      "total_weight_90d     0.707991\n",
      "total_weight_365d    0.670567\n",
      "recency_weighted     0.659198\n",
      "count_365d           0.627747\n",
      "active_ratio_90      0.542286\n",
      "forecast_horizon     0.262460\n",
      "rm_id               -0.001444\n",
      "days_since_last     -0.201473\n",
      "Name: target, dtype: float64\n",
      "\n",
      "✅ Saved: 05_feature_target_relationships.png\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS 6: TRAIN/VAL SPLIT COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Training set (before 2024-09-01):\n",
      "  Samples: 1895\n",
      "  Mean target: 341,946 kg\n",
      "  Median target: 40,654 kg\n",
      "  Zero percentage: 29.3%\n",
      "\n",
      "Validation set (>= 2024-09-01):\n",
      "  Samples: 830\n",
      "  Mean target: 220,249 kg\n",
      "  Median target: 12,500 kg\n",
      "  Zero percentage: 42.5%\n",
      "\n",
      "Feature distribution comparison (train vs val):\n",
      "  forecast_horizon    : Train=       67.40, Val=       67.40, Diff=  +0.0%\n",
      "  total_weight_365d   : Train=  1686746.35, Val=  1532133.33, Diff=  -9.2%\n",
      "  rate_90             : Train=     4994.28, Val=     3918.22, Diff= -21.5%\n",
      "  days_since_last     : Train=       32.71, Val=       47.60, Diff= +45.5%\n",
      "  total_weight_90d    : Train=   449485.45, Val=   352639.59, Diff= -21.5%\n",
      "  count_365d          : Train=      117.34, Val=      108.09, Diff=  -7.9%\n",
      "  recency_weighted    : Train=    22723.36, Val=    20162.04, Diff= -11.3%\n",
      "  active_ratio_90     : Train=        0.19, Val=        0.14, Diff= -25.8%\n",
      "  rm_id               : Train=     3027.23, Val=     3209.22, Diff=  +6.0%\n",
      "\n",
      "✅ Saved: 06_train_val_comparison.png\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS 7: TEST SET (2025) CHARACTERISTICS\n",
      "================================================================================\n",
      "\n",
      "Test set RM_IDs: 203\n",
      "RM_IDs with no history: 0\n",
      "RM_IDs with history: 203\n",
      "\n",
      "Days since last delivery (for RM_IDs with history):\n",
      "count     203.000000\n",
      "mean     2843.724138\n",
      "std      2598.229824\n",
      "min        12.000000\n",
      "25%       155.500000\n",
      "50%      2365.000000\n",
      "75%      4806.500000\n",
      "max      7496.000000\n",
      "Name: days_since_last, dtype: float64\n",
      "\n",
      "RM_IDs inactive >365 days: 143\n",
      "RM_IDs inactive >180 days: 148\n",
      "RM_IDs inactive >90 days: 162\n",
      "\n",
      "✅ Saved: 07_test_set_characteristics.png\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS 8: DATA LEAKAGE CHECK\n",
      "================================================================================\n",
      "\n",
      "Checking forecast_horizon relationship:\n",
      "                  count           mean           std\n",
      "forecast_horizon                                    \n",
      "7                   545   31630.220183  7.233981e+04\n",
      "30                  545  147938.988991  3.149113e+05\n",
      "60                  545  290364.943119  5.971223e+05\n",
      "90                  545  420068.598165  8.636388e+05\n",
      "150                 545  634389.563303  1.330682e+06\n",
      "\n",
      "Correlation between forecast_horizon and target:\n",
      "  Pearson: 0.262\n",
      "\n",
      "Sample of data to check for leakage:\n",
      "     rm_id train_date  forecast_horizon  total_weight_365d  days_since_last  \\\n",
      "0   3125.0 2024-01-01                 7          3851600.0               31   \n",
      "1   3125.0 2024-01-01                30          3851600.0               31   \n",
      "2   3125.0 2024-01-01                60          3851600.0               31   \n",
      "3   3125.0 2024-01-01                90          3851600.0               31   \n",
      "4   3125.0 2024-01-01               150          3851600.0               31   \n",
      "5   3123.0 2024-01-01                 7          2710020.0               31   \n",
      "6   3123.0 2024-01-01                30          2710020.0               31   \n",
      "7   3123.0 2024-01-01                60          2710020.0               31   \n",
      "8   3123.0 2024-01-01                90          2710020.0               31   \n",
      "9   3123.0 2024-01-01               150          2710020.0               31   \n",
      "10  3282.0 2024-01-01                 7          3915520.0               38   \n",
      "11  3282.0 2024-01-01                30          3915520.0               38   \n",
      "12  3282.0 2024-01-01                60          3915520.0               38   \n",
      "13  3282.0 2024-01-01                90          3915520.0               38   \n",
      "14  3282.0 2024-01-01               150          3915520.0               38   \n",
      "15  3126.0 2024-01-01                 7          7570890.0               16   \n",
      "16  3126.0 2024-01-01                30          7570890.0               16   \n",
      "17  3126.0 2024-01-01                60          7570890.0               16   \n",
      "18  3126.0 2024-01-01                90          7570890.0               16   \n",
      "19  3126.0 2024-01-01               150          7570890.0               16   \n",
      "\n",
      "       target  \n",
      "0    143000.0  \n",
      "1    485820.0  \n",
      "2   1117300.0  \n",
      "3   1772500.0  \n",
      "4   3028260.0  \n",
      "5     73820.0  \n",
      "6    300220.0  \n",
      "7    729180.0  \n",
      "8   1056260.0  \n",
      "9   1782880.0  \n",
      "10    75120.0  \n",
      "11   496740.0  \n",
      "12   988920.0  \n",
      "13  1458220.0  \n",
      "14  2373080.0  \n",
      "15    91480.0  \n",
      "16   322520.0  \n",
      "17  1195560.0  \n",
      "18  1823460.0  \n",
      "19  2998700.0  \n",
      "\n",
      "================================================================================\n",
      "SUMMARY AND KEY FINDINGS\n",
      "================================================================================\n",
      "\n",
      "1. ZERO INFLATION:\n",
      "   - 33.4% of training targets are exactly zero\n",
      "   - This is MODERATE\n",
      "\n",
      "2. TARGET DISTRIBUTION:\n",
      "   - Mean: 304,878 kg\n",
      "   - Median: 25,560 kg\n",
      "   - High skew\n",
      "\n",
      "3. TEMPORAL STABILITY:\n",
      "   - Train mean: 341,946 kg\n",
      "   - Val mean: 220,249 kg\n",
      "   - Difference: 35.6% (UNSTABLE)\n",
      "\n",
      "4. TEST SET CHARACTERISTICS:\n",
      "   - 70.4% of test RM_IDs inactive >365 days\n",
      "   - This is VERY HIGH\n",
      "\n",
      "5. FEATURE IMPORTANCE:\n",
      "   - forecast_horizon correlation: 0.262\n",
      "   - This is SUSPICIOUS\n",
      "\n",
      "================================================================================\n",
      "DIAGNOSTIC COMPLETE\n",
      "================================================================================\n",
      "\n",
      "All plots saved:\n",
      "  - 01_target_distribution.png\n",
      "  - 02_target_by_horizon.png\n",
      "  - 03_target_by_rm_id.png\n",
      "  - 04_temporal_patterns.png\n",
      "  - 05_feature_target_relationships.png\n",
      "  - 06_train_val_comparison.png\n",
      "  - 07_test_set_characteristics.png\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE DIAGNOSTIC ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD DATA\n",
    "# ============================================================================\n",
    "print(\"\\n[1] LOADING DATA\")\n",
    "receivals = pd.read_csv('./Project_materials/data/kernel/receivals.csv')\n",
    "purchase_orders = pd.read_csv('./Project_materials/data/kernel/purchase_orders.csv')\n",
    "prediction_mapping = pd.read_csv('./Project_materials/data/prediction_mapping.csv')\n",
    "\n",
    "# Convert dates\n",
    "receivals['date_arrival'] = pd.to_datetime(receivals['date_arrival'], utc=True).dt.tz_localize(None)\n",
    "receivals = receivals[receivals['net_weight'] > 0]\n",
    "receivals = receivals[receivals['rm_id'].notna()]\n",
    "receivals = receivals.sort_values('date_arrival')\n",
    "\n",
    "print(f\"Total receivals: {len(receivals)}\")\n",
    "print(f\"Date range: {receivals['date_arrival'].min()} to {receivals['date_arrival'].max()}\")\n",
    "print(f\"Unique rm_ids: {receivals['rm_id'].nunique()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# RECREATE TRAINING DATA (YOUR EXACT PROCESS)\n",
    "# ============================================================================\n",
    "print(\"\\n[2] RECREATING TRAINING DATA\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "train_dates = pd.date_range(start='2024-01-01', end='2024-11-30', freq='MS')\n",
    "forecast_horizons = [7, 30, 60, 90, 150]\n",
    "\n",
    "training_data = []\n",
    "active_rm_ids = receivals[receivals['date_arrival'] >= '2024-01-01']['rm_id'].unique()\n",
    "\n",
    "for i, train_date in enumerate(train_dates):\n",
    "    for rm_id in active_rm_ids:\n",
    "        hist = receivals[\n",
    "            (receivals['rm_id'] == rm_id) &\n",
    "            (receivals['date_arrival'] < train_date)\n",
    "        ]\n",
    "        \n",
    "        if len(hist) == 0:\n",
    "            continue\n",
    "        \n",
    "        cutoff_365 = train_date - timedelta(days=365)\n",
    "        cutoff_180 = train_date - timedelta(days=180)\n",
    "        cutoff_90 = train_date - timedelta(days=90)\n",
    "        cutoff_30 = train_date - timedelta(days=30)\n",
    "        \n",
    "        recent_365 = hist[hist['date_arrival'] >= cutoff_365]\n",
    "        recent_180 = hist[hist['date_arrival'] >= cutoff_180]\n",
    "        recent_90 = hist[hist['date_arrival'] >= cutoff_90]\n",
    "        recent_30 = hist[hist['date_arrival'] >= cutoff_30]\n",
    "        \n",
    "        if len(recent_365) > 0:\n",
    "            total_365 = recent_365['net_weight'].sum()\n",
    "            count_365 = len(recent_365)\n",
    "            days_since = (train_date - recent_365['date_arrival'].max()).days\n",
    "        else:\n",
    "            total_365 = count_365 = days_since = 0\n",
    "        \n",
    "        if len(recent_180) > 0:\n",
    "            total_180 = recent_180['net_weight'].sum()\n",
    "            count_180 = len(recent_180)\n",
    "        else:\n",
    "            total_180 = count_180 = 0\n",
    "        \n",
    "        if len(recent_90) > 0:\n",
    "            total_90 = recent_90['net_weight'].sum()\n",
    "            count_90 = len(recent_90)\n",
    "        else:\n",
    "            total_90 = count_90 = 0\n",
    "        \n",
    "        if len(recent_30) > 0:\n",
    "            total_30 = recent_30['net_weight'].sum()\n",
    "            count_30 = len(recent_30)\n",
    "            rate_30 = total_30 / 30\n",
    "        else:\n",
    "            total_30 = count_30 = rate_30 = 0\n",
    "        \n",
    "        rate_90 = total_90 / 90 if total_90 > 0 else 0\n",
    "        \n",
    "        if len(recent_90) > 0:\n",
    "            days_ago = (train_date - recent_90['date_arrival']).dt.days\n",
    "            weights = 1.0 / (days_ago + 1)\n",
    "            recency_weighted = (recent_90['net_weight'] * weights).sum()\n",
    "        else:\n",
    "            recency_weighted = 0\n",
    "        \n",
    "        if len(recent_90) > 0:\n",
    "            active_days_90 = recent_90['date_arrival'].dt.date.nunique()\n",
    "            active_ratio_90 = active_days_90 / 90\n",
    "        else:\n",
    "            active_ratio_90 = 0\n",
    "        \n",
    "        for horizon in forecast_horizons:\n",
    "            forecast_end = train_date + timedelta(days=horizon)\n",
    "            \n",
    "            actual = receivals[\n",
    "                (receivals['rm_id'] == rm_id) &\n",
    "                (receivals['date_arrival'] >= train_date) &\n",
    "                (receivals['date_arrival'] <= forecast_end)\n",
    "            ]\n",
    "            target = actual['net_weight'].sum()\n",
    "            \n",
    "            training_data.append({\n",
    "                'rm_id': rm_id,\n",
    "                'train_date': train_date,\n",
    "                'forecast_horizon': horizon,\n",
    "                'total_weight_365d': total_365,\n",
    "                'count_365d': count_365,\n",
    "                'days_since_last': days_since,\n",
    "                'total_weight_90d': total_90,\n",
    "                'count_90d': count_90,\n",
    "                'rate_90': rate_90,\n",
    "                'total_weight_180d': total_180,\n",
    "                'count_180d': count_180,\n",
    "                'total_30': total_30,\n",
    "                'count_30': count_30,\n",
    "                'rate_30': rate_30,\n",
    "                'recency_weighted': recency_weighted,\n",
    "                'active_ratio_90': active_ratio_90,\n",
    "                'target': target\n",
    "            })\n",
    "\n",
    "train_df = pd.DataFrame(training_data)\n",
    "print(f\"Total training samples: {len(train_df)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS 1: TARGET DISTRIBUTION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS 1: TARGET DISTRIBUTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nTarget Statistics:\")\n",
    "print(train_df['target'].describe())\n",
    "\n",
    "zero_pct = (train_df['target'] == 0).mean() * 100\n",
    "print(f\"\\nPercentage of ZERO targets: {zero_pct:.1f}%\")\n",
    "print(f\"Percentage of NON-ZERO targets: {100-zero_pct:.1f}%\")\n",
    "\n",
    "non_zero_targets = train_df[train_df['target'] > 0]['target']\n",
    "print(f\"\\nNon-zero target statistics:\")\n",
    "print(non_zero_targets.describe())\n",
    "\n",
    "# Percentiles\n",
    "print(\"\\nTarget percentiles:\")\n",
    "for p in [10, 25, 50, 75, 90, 95, 99]:\n",
    "    val = train_df['target'].quantile(p/100)\n",
    "    print(f\"  {p}th percentile: {val:,.0f} kg\")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# All targets\n",
    "axes[0].hist(train_df['target'], bins=100, edgecolor='black')\n",
    "axes[0].set_xlabel('Target (kg)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title(f'All Targets (n={len(train_df)})\\nZeros: {zero_pct:.1f}%')\n",
    "axes[0].axvline(train_df['target'].mean(), color='red', linestyle='--', label=f'Mean: {train_df[\"target\"].mean():,.0f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Non-zero targets only\n",
    "if len(non_zero_targets) > 0:\n",
    "    axes[1].hist(non_zero_targets, bins=100, edgecolor='black')\n",
    "    axes[1].set_xlabel('Target (kg)')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title(f'Non-Zero Targets Only (n={len(non_zero_targets)})')\n",
    "    axes[1].axvline(non_zero_targets.mean(), color='red', linestyle='--', label=f'Mean: {non_zero_targets.mean():,.0f}')\n",
    "    axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('01_target_distribution.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\n✅ Saved: 01_target_distribution.png\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS 2: TARGET BY FORECAST HORIZON\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS 2: TARGET BY FORECAST HORIZON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "horizon_analysis = train_df.groupby('forecast_horizon').agg({\n",
    "    'target': ['count', 'mean', 'std', 'median', lambda x: (x == 0).mean() * 100]\n",
    "})\n",
    "horizon_analysis.columns = ['count', 'mean', 'std', 'median', 'zero_pct']\n",
    "print(\"\\nTarget statistics by horizon:\")\n",
    "print(horizon_analysis)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].bar(horizon_analysis.index, horizon_analysis['mean'])\n",
    "axes[0].set_xlabel('Forecast Horizon (days)')\n",
    "axes[0].set_ylabel('Mean Target (kg)')\n",
    "axes[0].set_title('Mean Target by Horizon')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "axes[1].bar(horizon_analysis.index, horizon_analysis['zero_pct'], color='orange')\n",
    "axes[1].set_xlabel('Forecast Horizon (days)')\n",
    "axes[1].set_ylabel('Percentage Zero (%)')\n",
    "axes[1].set_title('Percentage of Zero Targets by Horizon')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('02_target_by_horizon.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\n✅ Saved: 02_target_by_horizon.png\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS 3: TARGET BY RM_ID\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS 3: TARGET BY RM_ID\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "rm_analysis = train_df.groupby('rm_id').agg({\n",
    "    'target': ['count', 'mean', 'std', lambda x: (x == 0).mean() * 100, 'sum']\n",
    "})\n",
    "rm_analysis.columns = ['count', 'mean', 'std', 'zero_pct', 'total']\n",
    "rm_analysis = rm_analysis.sort_values('count', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 rm_ids by frequency:\")\n",
    "print(rm_analysis.head(20))\n",
    "\n",
    "print(\"\\nTop 20 rm_ids by total weight:\")\n",
    "print(rm_analysis.sort_values('total', ascending=False).head(20))\n",
    "\n",
    "print(f\"\\nRM_IDs that are 100% zeros: {(rm_analysis['zero_pct'] == 100).sum()}\")\n",
    "print(f\"RM_IDs that are <50% zeros: {(rm_analysis['zero_pct'] < 50).sum()}\")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Count by rm_id\n",
    "top_20_counts = rm_analysis.head(20)\n",
    "axes[0, 0].barh(range(len(top_20_counts)), top_20_counts['count'])\n",
    "axes[0, 0].set_yticks(range(len(top_20_counts)))\n",
    "axes[0, 0].set_yticklabels([f\"rm_{int(x)}\" for x in top_20_counts.index])\n",
    "axes[0, 0].set_xlabel('Number of Samples')\n",
    "axes[0, 0].set_title('Top 20 RM_IDs by Sample Count')\n",
    "axes[0, 0].invert_yaxis()\n",
    "\n",
    "# Mean by rm_id\n",
    "top_20_mean = rm_analysis.sort_values('mean', ascending=False).head(20)\n",
    "axes[0, 1].barh(range(len(top_20_mean)), top_20_mean['mean'])\n",
    "axes[0, 1].set_yticks(range(len(top_20_mean)))\n",
    "axes[0, 1].set_yticklabels([f\"rm_{int(x)}\" for x in top_20_mean.index])\n",
    "axes[0, 1].set_xlabel('Mean Target (kg)')\n",
    "axes[0, 1].set_title('Top 20 RM_IDs by Mean Target')\n",
    "axes[0, 1].invert_yaxis()\n",
    "\n",
    "# Zero percentage distribution\n",
    "axes[1, 0].hist(rm_analysis['zero_pct'], bins=50, edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Zero Percentage (%)')\n",
    "axes[1, 0].set_ylabel('Number of RM_IDs')\n",
    "axes[1, 0].set_title('Distribution of Zero Percentage Across RM_IDs')\n",
    "axes[1, 0].axvline(rm_analysis['zero_pct'].median(), color='red', linestyle='--', label=f'Median: {rm_analysis[\"zero_pct\"].median():.1f}%')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Sample count distribution\n",
    "axes[1, 1].hist(rm_analysis['count'], bins=50, edgecolor='black')\n",
    "axes[1, 1].set_xlabel('Number of Samples')\n",
    "axes[1, 1].set_ylabel('Number of RM_IDs')\n",
    "axes[1, 1].set_title('Distribution of Sample Counts Across RM_IDs')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('03_target_by_rm_id.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\n✅ Saved: 03_target_by_rm_id.png\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS 4: TEMPORAL PATTERNS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS 4: TEMPORAL PATTERNS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "monthly_analysis = train_df.groupby(train_df['train_date'].dt.to_period('M')).agg({\n",
    "    'target': ['count', 'mean', 'median', lambda x: (x == 0).mean() * 100]\n",
    "})\n",
    "monthly_analysis.columns = ['count', 'mean', 'median', 'zero_pct']\n",
    "print(\"\\nTarget statistics by month:\")\n",
    "print(monthly_analysis)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Mean by month\n",
    "monthly_analysis['mean'].plot(ax=axes[0, 0], marker='o')\n",
    "axes[0, 0].set_xlabel('Month')\n",
    "axes[0, 0].set_ylabel('Mean Target (kg)')\n",
    "axes[0, 0].set_title('Mean Target by Month')\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Zero percentage by month\n",
    "monthly_analysis['zero_pct'].plot(ax=axes[0, 1], marker='o', color='orange')\n",
    "axes[0, 1].set_xlabel('Month')\n",
    "axes[0, 1].set_ylabel('Zero Percentage (%)')\n",
    "axes[0, 1].set_title('Percentage of Zero Targets by Month')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Sample count by month\n",
    "monthly_analysis['count'].plot(ax=axes[1, 0], marker='o', color='green')\n",
    "axes[1, 0].set_xlabel('Month')\n",
    "axes[1, 0].set_ylabel('Sample Count')\n",
    "axes[1, 0].set_title('Number of Samples by Month')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Median by month\n",
    "monthly_analysis['median'].plot(ax=axes[1, 1], marker='o', color='purple')\n",
    "axes[1, 1].set_xlabel('Month')\n",
    "axes[1, 1].set_ylabel('Median Target (kg)')\n",
    "axes[1, 1].set_title('Median Target by Month')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('04_temporal_patterns.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\n✅ Saved: 04_temporal_patterns.png\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS 5: FEATURE-TARGET RELATIONSHIPS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS 5: FEATURE-TARGET RELATIONSHIPS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "features = ['forecast_horizon', 'total_weight_365d', 'rate_90', 'days_since_last',\n",
    "            'total_weight_90d', 'count_365d', 'recency_weighted', 'active_ratio_90', 'rm_id']\n",
    "\n",
    "# Correlations\n",
    "correlations = train_df[features + ['target']].corr()['target'].sort_values(ascending=False)\n",
    "print(\"\\nFeature correlations with target:\")\n",
    "print(correlations)\n",
    "\n",
    "# Plot scatter plots\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "\n",
    "for i, col in enumerate(features):\n",
    "    row, col_idx = i // 3, i % 3\n",
    "    \n",
    "    # Sample for plotting (too many points)\n",
    "    sample = train_df.sample(min(5000, len(train_df)))\n",
    "    \n",
    "    axes[row, col_idx].scatter(sample[col], sample['target'], alpha=0.3, s=10)\n",
    "    axes[row, col_idx].set_xlabel(col)\n",
    "    axes[row, col_idx].set_ylabel('Target (kg)')\n",
    "    axes[row, col_idx].set_title(f'{col} vs Target\\nCorr: {correlations[col]:.3f}')\n",
    "    axes[row, col_idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('05_feature_target_relationships.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\n✅ Saved: 05_feature_target_relationships.png\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS 6: TRAIN/VAL SPLIT COMPARISON\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS 6: TRAIN/VAL SPLIT COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "split_date = pd.to_datetime('2024-09-01')\n",
    "train_mask = train_df['train_date'] < split_date\n",
    "val_mask = train_df['train_date'] >= split_date\n",
    "\n",
    "train_split = train_df[train_mask]\n",
    "val_split = train_df[val_mask]\n",
    "\n",
    "print(f\"\\nTraining set (before {split_date.date()}):\")\n",
    "print(f\"  Samples: {len(train_split)}\")\n",
    "print(f\"  Mean target: {train_split['target'].mean():,.0f} kg\")\n",
    "print(f\"  Median target: {train_split['target'].median():,.0f} kg\")\n",
    "print(f\"  Zero percentage: {(train_split['target'] == 0).mean() * 100:.1f}%\")\n",
    "\n",
    "print(f\"\\nValidation set (>= {split_date.date()}):\")\n",
    "print(f\"  Samples: {len(val_split)}\")\n",
    "print(f\"  Mean target: {val_split['target'].mean():,.0f} kg\")\n",
    "print(f\"  Median target: {val_split['target'].median():,.0f} kg\")\n",
    "print(f\"  Zero percentage: {(val_split['target'] == 0).mean() * 100:.1f}%\")\n",
    "\n",
    "# Feature distributions comparison\n",
    "print(\"\\nFeature distribution comparison (train vs val):\")\n",
    "for feat in features:\n",
    "    train_mean = train_split[feat].mean()\n",
    "    val_mean = val_split[feat].mean()\n",
    "    diff_pct = ((val_mean - train_mean) / (train_mean + 1e-10)) * 100\n",
    "    print(f\"  {feat:20s}: Train={train_mean:12.2f}, Val={val_mean:12.2f}, Diff={diff_pct:+6.1f}%\")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Target distributions\n",
    "axes[0, 0].hist(train_split['target'], bins=100, alpha=0.5, label='Train', edgecolor='black')\n",
    "axes[0, 0].hist(val_split['target'], bins=100, alpha=0.5, label='Val', edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Target (kg)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Target Distribution: Train vs Val')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Non-zero target distributions\n",
    "train_nonzero = train_split[train_split['target'] > 0]['target']\n",
    "val_nonzero = val_split[val_split['target'] > 0]['target']\n",
    "axes[0, 1].hist(train_nonzero, bins=100, alpha=0.5, label='Train', edgecolor='black')\n",
    "axes[0, 1].hist(val_nonzero, bins=100, alpha=0.5, label='Val', edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Target (kg)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Non-Zero Target Distribution: Train vs Val')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Zero percentage by horizon\n",
    "train_zero_pct = train_split.groupby('forecast_horizon').apply(lambda x: (x['target'] == 0).mean() * 100)\n",
    "val_zero_pct = val_split.groupby('forecast_horizon').apply(lambda x: (x['target'] == 0).mean() * 100)\n",
    "x = np.arange(len(train_zero_pct))\n",
    "width = 0.35\n",
    "axes[1, 0].bar(x - width/2, train_zero_pct.values, width, label='Train')\n",
    "axes[1, 0].bar(x + width/2, val_zero_pct.values, width, label='Val')\n",
    "axes[1, 0].set_xlabel('Forecast Horizon (days)')\n",
    "axes[1, 0].set_ylabel('Zero Percentage (%)')\n",
    "axes[1, 0].set_title('Zero Percentage by Horizon: Train vs Val')\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels(train_zero_pct.index)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Mean target by horizon\n",
    "train_mean = train_split.groupby('forecast_horizon')['target'].mean()\n",
    "val_mean = val_split.groupby('forecast_horizon')['target'].mean()\n",
    "axes[1, 1].bar(x - width/2, train_mean.values, width, label='Train')\n",
    "axes[1, 1].bar(x + width/2, val_mean.values, width, label='Val')\n",
    "axes[1, 1].set_xlabel('Forecast Horizon (days)')\n",
    "axes[1, 1].set_ylabel('Mean Target (kg)')\n",
    "axes[1, 1].set_title('Mean Target by Horizon: Train vs Val')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(train_mean.index)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('06_train_val_comparison.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\n✅ Saved: 06_train_val_comparison.png\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS 7: TEST SET (2025) CHARACTERISTICS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS 7: TEST SET (2025) CHARACTERISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "forecast_start = pd.to_datetime('2025-01-01')\n",
    "\n",
    "# Compute features for test set\n",
    "test_features = []\n",
    "\n",
    "for rm_id in prediction_mapping['rm_id'].unique():\n",
    "    hist = receivals[\n",
    "        (receivals['rm_id'] == rm_id) &\n",
    "        (receivals['date_arrival'] < forecast_start)\n",
    "    ]\n",
    "    \n",
    "    if len(hist) == 0:\n",
    "        test_features.append({\n",
    "            'rm_id': rm_id,\n",
    "            'has_history': False,\n",
    "            'days_since_last': 9999,\n",
    "            'total_weight_365d': 0,\n",
    "            'count_365d': 0\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    cutoff_365 = forecast_start - timedelta(days=365)\n",
    "    recent_365 = hist[hist['date_arrival'] >= cutoff_365]\n",
    "    \n",
    "    test_features.append({\n",
    "        'rm_id': rm_id,\n",
    "        'has_history': True,\n",
    "        'days_since_last': (forecast_start - hist['date_arrival'].max()).days,\n",
    "        'total_weight_365d': recent_365['net_weight'].sum() if len(recent_365) > 0 else 0,\n",
    "        'count_365d': len(recent_365) if len(recent_365) > 0 else 0\n",
    "    })\n",
    "\n",
    "test_features_df = pd.DataFrame(test_features)\n",
    "\n",
    "print(f\"\\nTest set RM_IDs: {len(test_features_df)}\")\n",
    "print(f\"RM_IDs with no history: {(~test_features_df['has_history']).sum()}\")\n",
    "print(f\"RM_IDs with history: {test_features_df['has_history'].sum()}\")\n",
    "\n",
    "print(f\"\\nDays since last delivery (for RM_IDs with history):\")\n",
    "days_since_stats = test_features_df[test_features_df['has_history']]['days_since_last'].describe()\n",
    "print(days_since_stats)\n",
    "\n",
    "print(f\"\\nRM_IDs inactive >365 days: {(test_features_df['days_since_last'] > 365).sum()}\")\n",
    "print(f\"RM_IDs inactive >180 days: {(test_features_df['days_since_last'] > 180).sum()}\")\n",
    "print(f\"RM_IDs inactive >90 days: {(test_features_df['days_since_last'] > 90).sum()}\")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Days since last\n",
    "axes[0].hist(test_features_df[test_features_df['has_history']]['days_since_last'], bins=50, edgecolor='black')\n",
    "axes[0].set_xlabel('Days Since Last Delivery')\n",
    "axes[0].set_ylabel('Number of RM_IDs')\n",
    "axes[0].set_title('Test Set: Days Since Last Delivery')\n",
    "axes[0].axvline(365, color='red', linestyle='--', label='365 days')\n",
    "axes[0].axvline(180, color='orange', linestyle='--', label='180 days')\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Total weight 365d\n",
    "axes[1].hist(test_features_df[test_features_df['total_weight_365d'] > 0]['total_weight_365d'], \n",
    "             bins=50, edgecolor='black')\n",
    "axes[1].set_xlabel('Total Weight in Last 365 Days (kg)')\n",
    "axes[1].set_ylabel('Number of RM_IDs')\n",
    "axes[1].set_title('Test Set: Historical Weight (Last 365 Days)')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('07_test_set_characteristics.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\n✅ Saved: 07_test_set_characteristics.png\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS 8: DATA LEAKAGE CHECK\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS 8: DATA LEAKAGE CHECK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check if forecast_horizon dominance makes sense\n",
    "print(\"\\nChecking forecast_horizon relationship:\")\n",
    "print(train_df.groupby('forecast_horizon')['target'].agg(['count', 'mean', 'std']))\n",
    "\n",
    "print(\"\\nCorrelation between forecast_horizon and target:\")\n",
    "print(f\"  Pearson: {train_df['forecast_horizon'].corr(train_df['target']):.3f}\")\n",
    "\n",
    "# Check for any obvious leakage patterns\n",
    "print(\"\\nSample of data to check for leakage:\")\n",
    "print(train_df[['rm_id', 'train_date', 'forecast_horizon', 'total_weight_365d', \n",
    "                'days_since_last', 'target']].head(20))\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY AND RECOMMENDATIONS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY AND KEY FINDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. ZERO INFLATION:\")\n",
    "print(f\"   - {zero_pct:.1f}% of training targets are exactly zero\")\n",
    "print(f\"   - This is {'SEVERE' if zero_pct > 50 else 'MODERATE' if zero_pct > 30 else 'MILD'}\")\n",
    "\n",
    "print(f\"\\n2. TARGET DISTRIBUTION:\")\n",
    "print(f\"   - Mean: {train_df['target'].mean():,.0f} kg\")\n",
    "print(f\"   - Median: {train_df['target'].median():,.0f} kg\")\n",
    "print(f\"   - High {'skew' if train_df['target'].mean() > 2 * train_df['target'].median() else 'variance'}\")\n",
    "\n",
    "print(f\"\\n3. TEMPORAL STABILITY:\")\n",
    "train_mean = train_split['target'].mean()\n",
    "val_mean = val_split['target'].mean()\n",
    "temporal_diff = abs(val_mean - train_mean) / train_mean * 100\n",
    "print(f\"   - Train mean: {train_mean:,.0f} kg\")\n",
    "print(f\"   - Val mean: {val_mean:,.0f} kg\")\n",
    "print(f\"   - Difference: {temporal_diff:.1f}% ({'UNSTABLE' if temporal_diff > 20 else 'STABLE'})\")\n",
    "\n",
    "print(f\"\\n4. TEST SET CHARACTERISTICS:\")\n",
    "inactive_pct = (test_features_df['days_since_last'] > 365).sum() / len(test_features_df) * 100\n",
    "print(f\"   - {inactive_pct:.1f}% of test RM_IDs inactive >365 days\")\n",
    "print(f\"   - This is {'VERY HIGH' if inactive_pct > 50 else 'HIGH' if inactive_pct > 30 else 'MODERATE'}\")\n",
    "\n",
    "print(f\"\\n5. FEATURE IMPORTANCE:\")\n",
    "print(f\"   - forecast_horizon correlation: {correlations['forecast_horizon']:.3f}\")\n",
    "print(f\"   - This is {'EXPECTED' if abs(correlations['forecast_horizon']) > 0.5 else 'SUSPICIOUS'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIAGNOSTIC COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nAll plots saved:\")\n",
    "print(\"  - 01_target_distribution.png\")\n",
    "print(\"  - 02_target_by_horizon.png\")\n",
    "print(\"  - 03_target_by_rm_id.png\")\n",
    "print(\"  - 04_temporal_patterns.png\")\n",
    "print(\"  - 05_feature_target_relationships.png\")\n",
    "print(\"  - 06_train_val_comparison.png\")\n",
    "print(\"  - 07_test_set_characteristics.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
