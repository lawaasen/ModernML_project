{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a03d2c99",
   "metadata": {},
   "source": [
    "# Project Report TDT4173\n",
    "\n",
    "The purpose of this report is to summarize all steps taken in our group in order to find fitting models/algorithms to the problem at hand. This will include exploratory data analysis, feature engineering, variuos predictors including boosting/bagging, as well as feature and model interpretations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271a3a5a",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "1. [Planned actions](#planned-actions)\n",
    "1. [Exploratory data analysis](#Exploratory-data-analysis)\n",
    "    1. [Important observations](#Important-observations)\n",
    "    2. [Steps taken](#Exploratory-steps)\n",
    "2. [Feature engineering](#Feature-engineering)\n",
    "    1. [Important observations](#Feature-engineering-Important-observations)\n",
    "    2. [Steps taken](#Feature-engineering-Steps-taken)\n",
    "3. [Model training](#Model-training)\n",
    "    1. [Important observations](#Model-training-Important-observations)\n",
    "    2. [Steps taken](#Model-training-Steps-taken)\n",
    "4. [Model evaluation and interpretation](#Model-evaluation-and-interpretation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32fda31",
   "metadata": {},
   "source": [
    "## Planned actions\n",
    "\n",
    "1. Perform exploratory data analysis in order to get an understanding of the data and notice patters/dependencies.\n",
    "2. Using the results in data analysis, perform feature engineering on a simple model (xgboost and random forest).\n",
    "3. Use the engineered features on better models (boosting/bagging). \n",
    "4. When model performs to satisfaction, perform model interpretation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0d562e",
   "metadata": {},
   "source": [
    "## Exploratory data analysis: \n",
    "\n",
    "The purpose of performing an exploratory data analysis is to get an understanding of the different types of data included in the problem and their relations. This will be useful when creating models in order to understand why different models perform a certain way as how feature engineering can help improve performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dabe6a",
   "metadata": {},
   "source": [
    "### Important observations\n",
    "\n",
    "This section will summarize the steps taken in next subsection, and will include the most important observations taken during testing.\n",
    "\n",
    "- There are 33171 purchase orders and 122590 receivals. It is thus evident that there is a tendency that purchase orders are split into several receivals, either due to stock unavailability, large orders or other reasons. We may try to merge the two dataframes together using purchase order id´s, but this is dependent on them existing in both dataframes. If they do not, we may consider deleting the id´s. \n",
    "\n",
    "- In both receivals and purchase orders, there are a few NaN values in the different features. Proposed solution is to drop these rows. The column `batch_id` in the receivals dataframe has about half of its values as NaN. Proposed solution is to either drop this column, or combine the non-NaN rows (aka the batches) and then drop the column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab14e180",
   "metadata": {},
   "source": [
    "\n",
    "### Steps taken\n",
    "\n",
    "This section describes all the steps taken in the exploratory data analysis. All steps taken are to be included here and may include old steps not discussed further. In further sections we may observe new data patterns, which will be noted here. \n",
    "\n",
    "#### 07.10.:\n",
    "- Converted date columns to datetime format, and visualized the head of the dataframes to get an understanding of the data.\n",
    "\n",
    "- Checked the amount of NaN values in the receivals dataframe. About half of the values in the `batch_id` column in the dataframe are NaN values. We could drop the entire column. A handful of NaN values in all other columns. There could be overlap between NaN values across features, but I propose to drop all rows with NaN values in the receivals dataframe. \n",
    "\n",
    "- Checked the amount of NaN values in the purchase orders dataframe. A few NaN values in the `unit` and `unit_id` columns. Could remove these rows as we cannot be certain of the unit of the purchase order. Most of the units are in 'kg' and a handful in 'pund'. Could either remove the rows with 'pund' or convert them to 'kg'.\n",
    "\n",
    "- In the `receival_status` column in the receivals dataframe, there are 142 orders that are not 'Completed'.\n",
    "\n",
    "- There are about 4400 purchase orders which are not 'Closed' in the `status` column in the purchase orders dataframe. This could be an important observation as these orders are not completed, and could be a reason for delay.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "971b5636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "plt.style.use('seaborn-v0_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0db5e41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_receivals = pd.read_csv('./Project_materials/data/kernel/receivals.csv')\n",
    "data_purchase_orders = pd.read_csv('./Project_materials/data/kernel/purchase_orders.csv')\n",
    "# data_materials = pd.read_csv('./Project_materials/data/extended/materials.csv')\n",
    "# data_transportation = pd.read_csv('./Project_materials/data/extended/transportation.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc070676",
   "metadata": {},
   "source": [
    "#### Printing dataframe heads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "838ab5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of receivals:  122590\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rm_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>purchase_order_id</th>\n",
       "      <th>purchase_order_item_no</th>\n",
       "      <th>receival_item_no</th>\n",
       "      <th>batch_id</th>\n",
       "      <th>date_arrival</th>\n",
       "      <th>receival_status</th>\n",
       "      <th>net_weight</th>\n",
       "      <th>supplier_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>365.0</td>\n",
       "      <td>91900143.0</td>\n",
       "      <td>208545.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2004-06-15 11:34:00</td>\n",
       "      <td>Completed</td>\n",
       "      <td>11420.0</td>\n",
       "      <td>52062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>365.0</td>\n",
       "      <td>91900143.0</td>\n",
       "      <td>208545.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2004-06-15 11:34:00</td>\n",
       "      <td>Completed</td>\n",
       "      <td>13760.0</td>\n",
       "      <td>52062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>365.0</td>\n",
       "      <td>91900143.0</td>\n",
       "      <td>208490.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2004-06-15 11:38:00</td>\n",
       "      <td>Completed</td>\n",
       "      <td>11281.0</td>\n",
       "      <td>50468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>365.0</td>\n",
       "      <td>91900143.0</td>\n",
       "      <td>208490.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2004-06-15 11:38:00</td>\n",
       "      <td>Completed</td>\n",
       "      <td>13083.0</td>\n",
       "      <td>50468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>379.0</td>\n",
       "      <td>91900296.0</td>\n",
       "      <td>210435.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2004-06-15 11:40:00</td>\n",
       "      <td>Completed</td>\n",
       "      <td>23910.0</td>\n",
       "      <td>52577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rm_id  product_id  purchase_order_id  purchase_order_item_no  \\\n",
       "0  365.0  91900143.0           208545.0                    10.0   \n",
       "1  365.0  91900143.0           208545.0                    10.0   \n",
       "2  365.0  91900143.0           208490.0                    10.0   \n",
       "3  365.0  91900143.0           208490.0                    10.0   \n",
       "4  379.0  91900296.0           210435.0                    20.0   \n",
       "\n",
       "   receival_item_no  batch_id        date_arrival receival_status  net_weight  \\\n",
       "0                 1       NaN 2004-06-15 11:34:00       Completed     11420.0   \n",
       "1                 2       NaN 2004-06-15 11:34:00       Completed     13760.0   \n",
       "2                 1       NaN 2004-06-15 11:38:00       Completed     11281.0   \n",
       "3                 2       NaN 2004-06-15 11:38:00       Completed     13083.0   \n",
       "4                 1       NaN 2004-06-15 11:40:00       Completed     23910.0   \n",
       "\n",
       "   supplier_id  \n",
       "0        52062  \n",
       "1        52062  \n",
       "2        50468  \n",
       "3        50468  \n",
       "4        52577  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_receivals['date_arrival'] = pd.to_datetime(data_receivals['date_arrival'], utc=True).dt.tz_localize(None)\n",
    "print(\"Amount of receivals: \", len(data_receivals['rm_id']))\n",
    "data_receivals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c1022a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of purchase orders:  33171\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>purchase_order_id</th>\n",
       "      <th>purchase_order_item_no</th>\n",
       "      <th>quantity</th>\n",
       "      <th>delivery_date</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_version</th>\n",
       "      <th>created_date_time</th>\n",
       "      <th>modified_date_time</th>\n",
       "      <th>unit_id</th>\n",
       "      <th>unit</th>\n",
       "      <th>status_id</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>2003-05-11 22:00:00</td>\n",
       "      <td>91900143</td>\n",
       "      <td>1</td>\n",
       "      <td>2003-05-12 10:00:48</td>\n",
       "      <td>2004-06-15 06:16:18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Closed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>23880.0</td>\n",
       "      <td>2003-05-26 22:00:00</td>\n",
       "      <td>91900160</td>\n",
       "      <td>1</td>\n",
       "      <td>2003-05-27 12:42:07</td>\n",
       "      <td>2012-06-29 09:41:13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Closed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2004-03-07 23:00:00</td>\n",
       "      <td>91900143</td>\n",
       "      <td>1</td>\n",
       "      <td>2004-03-08 13:44:31</td>\n",
       "      <td>2012-07-04 13:51:02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Closed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2004-03-09 23:00:00</td>\n",
       "      <td>91900143</td>\n",
       "      <td>1</td>\n",
       "      <td>2004-03-10 11:39:06</td>\n",
       "      <td>2012-07-04 13:50:59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Closed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>141</td>\n",
       "      <td>10</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>2004-10-27 22:00:00</td>\n",
       "      <td>91900143</td>\n",
       "      <td>1</td>\n",
       "      <td>2004-10-22 12:21:54</td>\n",
       "      <td>2012-07-04 13:50:55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Closed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   purchase_order_id  purchase_order_item_no  quantity       delivery_date  \\\n",
       "0                  1                       1     -14.0 2003-05-11 22:00:00   \n",
       "1                 22                       1   23880.0 2003-05-26 22:00:00   \n",
       "2                 41                       1       0.0 2004-03-07 23:00:00   \n",
       "3                 61                       1       0.0 2004-03-09 23:00:00   \n",
       "4                141                      10   25000.0 2004-10-27 22:00:00   \n",
       "\n",
       "   product_id  product_version   created_date_time  modified_date_time  \\\n",
       "0    91900143                1 2003-05-12 10:00:48 2004-06-15 06:16:18   \n",
       "1    91900160                1 2003-05-27 12:42:07 2012-06-29 09:41:13   \n",
       "2    91900143                1 2004-03-08 13:44:31 2012-07-04 13:51:02   \n",
       "3    91900143                1 2004-03-10 11:39:06 2012-07-04 13:50:59   \n",
       "4    91900143                1 2004-10-22 12:21:54 2012-07-04 13:50:55   \n",
       "\n",
       "   unit_id unit  status_id  status  \n",
       "0      NaN  NaN          2  Closed  \n",
       "1      NaN  NaN          2  Closed  \n",
       "2      NaN  NaN          2  Closed  \n",
       "3      NaN  NaN          2  Closed  \n",
       "4      NaN  NaN          2  Closed  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_purchase_orders['delivery_date'] = pd.to_datetime(data_purchase_orders['delivery_date'], utc=True).dt.tz_localize(None)\n",
    "data_purchase_orders['created_date_time'] = pd.to_datetime(data_purchase_orders['created_date_time'], utc=True).dt.tz_localize(None)\n",
    "data_purchase_orders['modified_date_time'] = pd.to_datetime(data_purchase_orders['modified_date_time'], utc=True).dt.tz_localize(None)\n",
    "print(\"Amount of purchase orders: \", len(data_purchase_orders['purchase_order_id']))\n",
    "data_purchase_orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f66aa4",
   "metadata": {},
   "source": [
    "#### Study of column values, especially NaN values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42433e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_nan = 0\n",
    "for elmt in data_receivals['net_weight']:\n",
    "    if pd.isna(elmt):\n",
    "        num_nan += 1\n",
    "\n",
    "print(num_nan)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1218eb",
   "metadata": {},
   "source": [
    "#### 16.10.: Initial Modeling & Lag Analysis\n",
    "\n",
    "**Baseline Models**:\n",
    "- Started with simple rule-based baseline (0 for inactive, 75% of PO qty for active) → Score: 134,136 (beat 0 VTs) ❌\n",
    "- Moved to XGBoost with basic features (365d, 90d aggregates, PO data) + quantile regression (α=0.2) → Score: 10,135 (beat 2 VTs) ✅\n",
    "\n",
    "**Lag Pattern Discovery**:\n",
    "- Analyzed delivery lag (actual arrival - expected delivery date)\n",
    "- Key finding: Median lag = -15 days (deliveries arrive ~2 weeks EARLY!)\n",
    "- Supplier-specific variation: std dev of 47.3 days across suppliers → significant\n",
    "- Product-specific variation: std dev of 11.4 days\n",
    "- Temporal stability: 2020+ data is stable (2004-2006 had weird patterns, excluded from lag calc)\n",
    "\n",
    "**Lag Adjustment Implementation**:\n",
    "- Computed supplier-specific median lags from 2020+ data\n",
    "- Adjusted PO expected_arrival = delivery_date + supplier_lag\n",
    "- Only count POs with expected_arrival in forecast window (critical!)\n",
    "- XGBoost with lag adjustment → Score: 10,135 ✅\n",
    "- LightGBM with lag adjustment → Score: 9,600 ✅ (best so far!)\n",
    "\n",
    "**Failed Experiments**:\n",
    "- Random Forest with mean predictions → Score: 16,763 ❌ (RF needs 20th percentile extraction, too slow for iteration)\n",
    "- Ensemble XGBoost + LightGBM → No improvement (models too correlated, -0.2% on validation)\n",
    "\n",
    "**Feature Engineering Attempts**:\n",
    "1. **2023-2024 training data + trend features + supplier categorical** → Score: 11,800 ❌\n",
    "   - Added: 30d, 180d aggregates, trend_ratio, acceleration, supplier_id as categorical\n",
    "   - Problem: 2023 data created distribution shift (2023 patterns ≠ 2025 patterns)\n",
    "   - Validation improved (24,248) but Kaggle worse → classic overfitting\n",
    "   \n",
    "2. **2024 data + supplier as categorical** → Score: 15,000 ❌❌\n",
    "   - Problem: LightGBM's categorical feature handling overfits with 87 suppliers on small dataset\n",
    "   - Learned: categorical_feature parameter is dangerous with limited data\n",
    "\n",
    "**Key Observations**:\n",
    "- LightGBM (9,600) slightly beats XGBoost (10,135) with same features\n",
    "- Lag adjustment is critical (improves ~16% from baseline)\n",
    "- More data ≠ better (2023 data hurts due to distribution shift)\n",
    "- Categorical features in LightGBM overfit easily\n",
    "- Validation loss can be misleading (need time-based validation for forecasting)\n",
    "- Random 80/20 split includes old patterns, but Kaggle tests on 2025 (unseen conditions)\n",
    "\n",
    "**Current Best**: LightGBM + lag adjustment (2024 data, 13 features) → 9,600 (beats 2-3 VTs)\n",
    "\n",
    "**Next Steps**: Test supplier_id as numeric feature (not categorical), add trend features carefully with 2024 data only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e505cf",
   "metadata": {},
   "source": [
    "### 17.10.: Validation Strategy, Feature Analysis & Zero-Inflation\n",
    "\n",
    "**Starting Point**: LightGBM with lag adjustment → Score: 9,600 (2024 data, time-based features)\n",
    "\n",
    "**Phase 1: Diagnostic Deep Dive**\n",
    "- Problem: Score stuck at 9,600 despite alpha adjustments (0.15, 0.20, 0.25, 0.30)\n",
    "- Root cause: Random 80/20 split mixes all months → model learns \"average 2024\" instead of declining trend\n",
    "- Key finding: **-35.6% decline** from training (Jan-Aug: 342k kg mean) to validation (Sep-Nov: 220k kg mean)\n",
    "\n",
    "**Validation Strategy Experiments**:\n",
    "1. **Random split + alpha=0.15 + 0.88 adjustment** → Score: 9,500 ✅\n",
    "   - Predictions: 37k kg × 0.88 = 32.7k kg mean\n",
    "   - Issue: Still predicting \"average\" rather than learning trend\n",
    "   \n",
    "2. **Time-based split + alpha=0.15** → Score: 9,500\n",
    "   - Training: Jan-Aug 2024, Validation: Sep-Nov 2024\n",
    "   - Model sees -35.6% decline but predictions still ~37k kg\n",
    "   \n",
    "3. **Time-based split + alpha=0.25 (removed 0.88)** → Score: 9,960 ❌\n",
    "   - Predictions increased to 45k kg\n",
    "   - Proved over-predicting makes score worse → test set has LOW activity\n",
    "   \n",
    "4. **Time-based split + alpha=0.15 + 0.85 adjustment** → Score: ~9,500\n",
    "   - No real improvement, wasted submission\n",
    "\n",
    "**Key Insight**: Hyperparameter tuning won't get from 9,600 → 5,000. Need fundamental changes.\n",
    "\n",
    "**Phase 2: Deep EDA - The Breakthrough**\n",
    "\n",
    "**Critical Discoveries**:\n",
    "1. **Zero-Inflation Problem** ⚠️\n",
    "   - Test set: 203 unique rm_ids\n",
    "   - **Only 60 (29.6%) have ANY 2024 data**\n",
    "   - **143 rm_ids (70.4%) are DEAD** (last delivery 2004-2005)\n",
    "   - **72.9% of test predictions (22,200/30,450) are for DEAD rm_ids**\n",
    "   - Problem: Quantile regression predicts positive values for everything → massive over-prediction\n",
    "\n",
    "2. **Feature Correlation Analysis** (Sep-Nov 2024):\n",
    "```\n",
    "   Strong features (correlation > 0.65):\n",
    "   - total_90, rate_90: +0.9526 ***\n",
    "   - recency_weighted: +0.9327 ***\n",
    "   - total_180: +0.9131 ***\n",
    "   - rate_30, total_30: +0.8872 ***\n",
    "   - active_ratio_90: +0.6716 ***\n",
    "   \n",
    "   Weak features (remove):\n",
    "   - momentum: +0.0713\n",
    "   - slope_90: +0.0084\n",
    "   - cv_90: -0.0207\n",
    "   - PO quantity: +0.0767\n",
    "```\n",
    "\n",
    "3. **Seasonality**: January vs other months 1.03x ratio → month/quarter features likely noise\n",
    "\n",
    "**Phase 3: The Working Solution** ✅\n",
    "\n",
    "**Two-Stage Model + Guardrails**:\n",
    "1. **Two-Stage Architecture**:\n",
    "   - Stage 1: LGBMClassifier (n=400) predicts P(delivery > 0)\n",
    "   - Stage 2: LGBMRegressor (quantile α=0.15) predicts amount IF delivered\n",
    "   - Combined: prediction = regressor × classifier_probability\n",
    "   - **Why it works**: Dead rm_ids get low probability (~0.1) → effectively zero prediction\n",
    "\n",
    "2. **Per-Horizon Calibration**:\n",
    "   - Learned on Sep-Nov validation for horizons {7, 30, 60, 90, 150}\n",
    "   - factor_h = sum(actual) / sum(predicted), clipped to [0.70, 1.10]\n",
    "   - Fixes systematic over/under-prediction at different horizons\n",
    "\n",
    "3. **Activity-Based Guardrails**:\n",
    "   - days_since_last > 365 → force 0\n",
    "   - 180 < days_since_last ≤ 365 → soft cap at 8% of last year volume\n",
    "   - Upper cap: min(pred, (total_365/365) × horizon × 1.5)\n",
    "\n",
    "**Result**: 9,600 → **8,200** ✅✅ (14% improvement!)\n",
    "\n",
    "**Phase 4: Feature Engineering - Step 1** (In Progress)\n",
    "\n",
    "**Removed (9 weak features)**:\n",
    "- ❌ momentum, slope_90, cv_90 (correlation < 0.1)\n",
    "- ❌ future_po_quantity, future_po_count (correlation 0.08)\n",
    "- ❌ month, quarter (weak seasonality)\n",
    "- ❌ avg_weight_365d, daily_rate_365d (redundant)\n",
    "\n",
    "**Added (5 strong features)**:\n",
    "- ✅ recency_weighted (correlation 0.93)\n",
    "- ✅ total_30, rate_30, count_30 (correlation 0.89)\n",
    "- ✅ active_ratio_90 (correlation 0.67)\n",
    "\n",
    "**Net**: 16 features → 14 features (more focused, less noise)\n",
    "**Expected**: 8,200 → 7,200-7,500\n",
    "\n",
    "**Key Learnings**:\n",
    "- Zero-inflation critical: 73% of test should be ~0, quantile regression can't learn this\n",
    "- Domain knowledge > ML: Hard-coded rules (dead = 0) beat learned patterns\n",
    "- Feature correlation matters: momentum (0.07 correlation) = noise\n",
    "- Validation strategy crucial: Random split hides declining trend\n",
    "- EDA before iteration: Deep analysis revealed 73% dead rm_ids problem\n",
    "\n",
    "**Failed Experiments**:\n",
    "- Random split + alpha tuning → 9,500-9,960 ❌ (doesn't learn trend)\n",
    "- Remove 0.88 adjustment → 9,960 ❌ (predictions too high)\n",
    "- Time-based + various alphas → 9,500-9,960 ❌ (hyperparameter band-aid)\n",
    "\n",
    "**Current Best**: Two-stage + guardrails → 8,200 (beats ~3-4 VTs)\n",
    "\n",
    "**Next Steps** (if Step 1 works):\n",
    "1. Interpolate calibration for horizons 2-151\n",
    "2. Activity-aware guardrails (smooth decay)\n",
    "3. Ensemble with median predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cff16b1",
   "metadata": {},
   "source": [
    "### 18.10.: Feature Pruning Based on Correlation Analysis\n",
    "\n",
    "**Removed (9 weak features)**:\n",
    "- ❌ momentum, slope_90, cv_90 (correlation < 0.1)\n",
    "- ❌ future_po_quantity, future_po_count (correlation 0.08)\n",
    "- ❌ month, quarter (weak seasonality)\n",
    "- ❌ avg_weight_365d, daily_rate_365d (redundant)\n",
    "\n",
    "**Added (5 strong features)**:\n",
    "- ✅ recency_weighted (correlation 0.93) - weights recent deliveries higher\n",
    "- ✅ total_30, rate_30, count_30 (correlation 0.89) - captures very recent activity\n",
    "- ✅ active_ratio_90 (correlation 0.67) - ratio of active days in last 90d\n",
    "\n",
    "**Net**: 16 features → 14 features (more focused, less noise)\n",
    "\n",
    "**Result**: 8,200 → **6,200** ✅✅✅ (24% improvement!)\n",
    "\n",
    "**Key Insight**: Weak features weren't neutral - they were actively adding noise and confusing the model. Removing them + adding highly correlated features = massive gain.\n",
    "\n",
    "**Total Progress**: 9,600 (baseline) → 6,200 (35% improvement!)\n",
    "\n",
    "---\n",
    "\n",
    "#### Fine-Tuning Attempts (Hit Plateau)\n",
    "\n",
    "**Step 2: Interpolated Calibration** → 6,700 ❌ (worse by 500)\n",
    "- Tried to fill calibration for ALL horizons 2-151 (not just {7,30,60,90,150})\n",
    "- Used scipy.interpolate.interp1d to smooth factors across horizons\n",
    "- **Failed:** Overfitted to Sep-Nov 2024 patterns, didn't generalize to 2025 test set\n",
    "- Learning: Validation-learned adjustments don't always transfer to test\n",
    "\n",
    "**Step 3: Aggressive Dead Filtering** → 6,200 (no change)\n",
    "- Tightened guardrails: cold rm_ids (180-365d) from 8% cap → 5% cap\n",
    "- Added new warm rm_ids (90-180d) cap at 15%\n",
    "- **No effect:** Only affected 19/203 rm_ids, and their predictions were already low from two-stage model\n",
    "- Learning: Guardrails already optimal from ChatGPT's original implementation\n",
    "\n",
    "**Step 4: Lower Alpha (0.15 → 0.10)** → 6,140 ✅ (tiny improvement, -60 points)\n",
    "- Changed quantile target from 15th percentile to 10th percentile\n",
    "- More conservative predictions\n",
    "- **Minimal improvement:** Suggests we're close to optimal conservatism level\n",
    "\n",
    "**Step 5: Remove Calibration Entirely** → 6,138 ✅ (basically same, -2 points)\n",
    "- Removed all per-horizon calibration adjustments\n",
    "- Let model's forecast_horizon feature handle effects naturally\n",
    "- **No impact:** Calibration was irrelevant (factors too close to 1.0)\n",
    "- Learning: Sep-Nov calibration neither helped nor hurt\n",
    "\n",
    "**Key Observations**:\n",
    "- Hit a plateau around 6,100-6,200 - small tweaks (±60 points) don't move the needle\n",
    "- Feature engineering (Step 1) gave 24% improvement, everything else combined gave <1%\n",
    "- Calibration/guardrails/alpha tuning are all second-order effects\n",
    "- Need <5,000 for grade A (currently 6,138) - requires 18.5% more improvement\n",
    "\n",
    "**Failed Experiments**:\n",
    "- Interpolated calibration → overfitting to validation\n",
    "- Tighter guardrails → no effect (already optimal)\n",
    "- Lower alpha → diminishing returns\n",
    "\n",
    "**Current Best**: Step 5 (6,138 points) with alpha=0.10, no calibration, Step 1 features\n",
    "\n",
    "**Possible next steps** (untested):\n",
    "- Even lower alpha (0.05 or 0.08) for maximum conservatism\n",
    "- Nuclear option: predict 0 for >90 days inactive (high risk/reward)\n",
    "- Activity-aware upper caps (make hot rm_ids able to spike more, cold less)\n",
    "\n",
    "prob need somthing fundamentally different to actually improve our score further"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731b9e1d",
   "metadata": {},
   "source": [
    "## Feature engineering:\n",
    "\n",
    "The purpose of performing feature engineering on the datasets is to increase the performance of a predicting model. This can for example be done by removing features, merge features or giving features extra \"weight\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8558a4af",
   "metadata": {},
   "source": [
    "### Important observations\n",
    "\n",
    "This section will summarize the steps taken in next subsection, and will include the most important observations taken during feature engineering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18be8b81",
   "metadata": {},
   "source": [
    "### Steps taken\n",
    "\n",
    "This section describes all the steps taken during feature engineering. All steps taken are to be included here and may include old steps not discussed further. In further sections we may observe new feature behaviour, which will also be noted here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab8201e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d833710d",
   "metadata": {},
   "source": [
    "## Model training:\n",
    "\n",
    "This is the section where we will train different models on the data, and try to find the best model for the problem. At the end of this section we should have a model that performs well on the data, and is able to make good predictions on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e417bbb",
   "metadata": {},
   "source": [
    "### Important observations\n",
    "This section will summarize the steps taken in next subsection, and will include the most important observations taken during model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9603e94",
   "metadata": {},
   "source": [
    "### Steps taken\n",
    "This section describes all the steps taken during model training. All steps taken are to be included here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e05650e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10d4062f",
   "metadata": {},
   "source": [
    "## Model evaluation and interpretation:\n",
    "The purpose of this section is to evaluate the model performance and interpret the model. This will include feature importance, SHAP values and partial dependence plots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4712f58a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
