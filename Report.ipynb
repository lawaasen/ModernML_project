{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a03d2c99",
   "metadata": {},
   "source": [
    "# Project Report TDT4173\n",
    "\n",
    "The purpose of this report is to summarize all steps taken in our group in order to find fitting models/algorithms to the problem at hand. This will include exploratory data analysis, feature engineering, variuos predictors including boosting/bagging, as well as feature and model interpretations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271a3a5a",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "1. [Planned actions](#planned-actions)\n",
    "1. [Exploratory data analysis](#Exploratory-data-analysis)\n",
    "    1. [Important observations](#Important-observations)\n",
    "    2. [Steps taken](#Exploratory-steps)\n",
    "2. [Feature engineering](#Feature-engineering)\n",
    "    1. [Important observations](#Feature-engineering-Important-observations)\n",
    "    2. [Steps taken](#Feature-engineering-Steps-taken)\n",
    "3. [Model training](#Model-training)\n",
    "    1. [Important observations](#Model-training-Important-observations)\n",
    "    2. [Steps taken](#Model-training-Steps-taken)\n",
    "4. [Model evaluation and interpretation](#Model-evaluation-and-interpretation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32fda31",
   "metadata": {},
   "source": [
    "## Planned actions\n",
    "\n",
    "1. Perform exploratory data analysis in order to get an understanding of the data and notice patters/dependencies.\n",
    "2. Using the results in data analysis, perform feature engineering on a simple model (xgboost and random forest).\n",
    "3. Use the engineered features on better models (boosting/bagging). \n",
    "4. When model performs to satisfaction, perform model interpretation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0d562e",
   "metadata": {},
   "source": [
    "## Exploratory data analysis: \n",
    "\n",
    "The purpose of performing an exploratory data analysis is to get an understanding of the different types of data included in the problem and their relations. This will be useful when creating models in order to understand why different models perform a certain way as how feature engineering can help improve performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dabe6a",
   "metadata": {},
   "source": [
    "### Important observations\n",
    "\n",
    "This section will summarize the steps taken in next subsection, and will include the most important observations taken during testing.\n",
    "\n",
    "- There are 33171 purchase orders and 122590 receivals. It is thus evident that there is a tendency that purchase orders are split into several receivals, either due to stock unavailability, large orders or other reasons. We may try to merge the two dataframes together using purchase order id´s, but this is dependent on them existing in both dataframes. If they do not, we may consider deleting the id´s. \n",
    "\n",
    "- In both receivals and purchase orders, there are a few NaN values in the different features. Proposed solution is to drop these rows. The column `batch_id` in the receivals dataframe has about half of its values as NaN. Proposed solution is to either drop this column, or combine the non-NaN rows (aka the batches) and then drop the column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab14e180",
   "metadata": {},
   "source": [
    "\n",
    "### Steps taken\n",
    "\n",
    "This section describes all the steps taken in the exploratory data analysis. All steps taken are to be included here and may include old steps not discussed further. In further sections we may observe new data patterns, which will be noted here. \n",
    "\n",
    "#### 07.10.:\n",
    "- Converted date columns to datetime format, and visualized the head of the dataframes to get an understanding of the data.\n",
    "\n",
    "- Checked the amount of NaN values in the receivals dataframe. About half of the values in the `batch_id` column in the dataframe are NaN values. We could drop the entire column. A handful of NaN values in all other columns. There could be overlap between NaN values across features, but I propose to drop all rows with NaN values in the receivals dataframe. \n",
    "\n",
    "- Checked the amount of NaN values in the purchase orders dataframe. A few NaN values in the `unit` and `unit_id` columns. Could remove these rows as we cannot be certain of the unit of the purchase order. Most of the units are in 'kg' and a handful in 'pund'. Could either remove the rows with 'pund' or convert them to 'kg'.\n",
    "\n",
    "- In the `receival_status` column in the receivals dataframe, there are 142 orders that are not 'Completed'.\n",
    "\n",
    "- There are about 4400 purchase orders which are not 'Closed' in the `status` column in the purchase orders dataframe. This could be an important observation as these orders are not completed, and could be a reason for delay.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "971b5636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "plt.style.use('seaborn-v0_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0db5e41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_receivals = pd.read_csv('./Project_materials/data/kernel/receivals.csv')\n",
    "data_purchase_orders = pd.read_csv('./Project_materials/data/kernel/purchase_orders.csv')\n",
    "# data_materials = pd.read_csv('./Project_materials/data/extended/materials.csv')\n",
    "# data_transportation = pd.read_csv('./Project_materials/data/extended/transportation.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc070676",
   "metadata": {},
   "source": [
    "#### Printing dataframe heads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "838ab5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of receivals:  122590\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rm_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>purchase_order_id</th>\n",
       "      <th>purchase_order_item_no</th>\n",
       "      <th>receival_item_no</th>\n",
       "      <th>batch_id</th>\n",
       "      <th>date_arrival</th>\n",
       "      <th>receival_status</th>\n",
       "      <th>net_weight</th>\n",
       "      <th>supplier_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>365.0</td>\n",
       "      <td>91900143.0</td>\n",
       "      <td>208545.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2004-06-15 11:34:00</td>\n",
       "      <td>Completed</td>\n",
       "      <td>11420.0</td>\n",
       "      <td>52062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>365.0</td>\n",
       "      <td>91900143.0</td>\n",
       "      <td>208545.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2004-06-15 11:34:00</td>\n",
       "      <td>Completed</td>\n",
       "      <td>13760.0</td>\n",
       "      <td>52062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>365.0</td>\n",
       "      <td>91900143.0</td>\n",
       "      <td>208490.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2004-06-15 11:38:00</td>\n",
       "      <td>Completed</td>\n",
       "      <td>11281.0</td>\n",
       "      <td>50468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>365.0</td>\n",
       "      <td>91900143.0</td>\n",
       "      <td>208490.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2004-06-15 11:38:00</td>\n",
       "      <td>Completed</td>\n",
       "      <td>13083.0</td>\n",
       "      <td>50468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>379.0</td>\n",
       "      <td>91900296.0</td>\n",
       "      <td>210435.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2004-06-15 11:40:00</td>\n",
       "      <td>Completed</td>\n",
       "      <td>23910.0</td>\n",
       "      <td>52577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rm_id  product_id  purchase_order_id  purchase_order_item_no  \\\n",
       "0  365.0  91900143.0           208545.0                    10.0   \n",
       "1  365.0  91900143.0           208545.0                    10.0   \n",
       "2  365.0  91900143.0           208490.0                    10.0   \n",
       "3  365.0  91900143.0           208490.0                    10.0   \n",
       "4  379.0  91900296.0           210435.0                    20.0   \n",
       "\n",
       "   receival_item_no  batch_id        date_arrival receival_status  net_weight  \\\n",
       "0                 1       NaN 2004-06-15 11:34:00       Completed     11420.0   \n",
       "1                 2       NaN 2004-06-15 11:34:00       Completed     13760.0   \n",
       "2                 1       NaN 2004-06-15 11:38:00       Completed     11281.0   \n",
       "3                 2       NaN 2004-06-15 11:38:00       Completed     13083.0   \n",
       "4                 1       NaN 2004-06-15 11:40:00       Completed     23910.0   \n",
       "\n",
       "   supplier_id  \n",
       "0        52062  \n",
       "1        52062  \n",
       "2        50468  \n",
       "3        50468  \n",
       "4        52577  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_receivals['date_arrival'] = pd.to_datetime(data_receivals['date_arrival'], utc=True).dt.tz_localize(None)\n",
    "print(\"Amount of receivals: \", len(data_receivals['rm_id']))\n",
    "data_receivals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c1022a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of purchase orders:  33171\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>purchase_order_id</th>\n",
       "      <th>purchase_order_item_no</th>\n",
       "      <th>quantity</th>\n",
       "      <th>delivery_date</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_version</th>\n",
       "      <th>created_date_time</th>\n",
       "      <th>modified_date_time</th>\n",
       "      <th>unit_id</th>\n",
       "      <th>unit</th>\n",
       "      <th>status_id</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>2003-05-11 22:00:00</td>\n",
       "      <td>91900143</td>\n",
       "      <td>1</td>\n",
       "      <td>2003-05-12 10:00:48</td>\n",
       "      <td>2004-06-15 06:16:18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Closed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>23880.0</td>\n",
       "      <td>2003-05-26 22:00:00</td>\n",
       "      <td>91900160</td>\n",
       "      <td>1</td>\n",
       "      <td>2003-05-27 12:42:07</td>\n",
       "      <td>2012-06-29 09:41:13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Closed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2004-03-07 23:00:00</td>\n",
       "      <td>91900143</td>\n",
       "      <td>1</td>\n",
       "      <td>2004-03-08 13:44:31</td>\n",
       "      <td>2012-07-04 13:51:02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Closed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2004-03-09 23:00:00</td>\n",
       "      <td>91900143</td>\n",
       "      <td>1</td>\n",
       "      <td>2004-03-10 11:39:06</td>\n",
       "      <td>2012-07-04 13:50:59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Closed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>141</td>\n",
       "      <td>10</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>2004-10-27 22:00:00</td>\n",
       "      <td>91900143</td>\n",
       "      <td>1</td>\n",
       "      <td>2004-10-22 12:21:54</td>\n",
       "      <td>2012-07-04 13:50:55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Closed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   purchase_order_id  purchase_order_item_no  quantity       delivery_date  \\\n",
       "0                  1                       1     -14.0 2003-05-11 22:00:00   \n",
       "1                 22                       1   23880.0 2003-05-26 22:00:00   \n",
       "2                 41                       1       0.0 2004-03-07 23:00:00   \n",
       "3                 61                       1       0.0 2004-03-09 23:00:00   \n",
       "4                141                      10   25000.0 2004-10-27 22:00:00   \n",
       "\n",
       "   product_id  product_version   created_date_time  modified_date_time  \\\n",
       "0    91900143                1 2003-05-12 10:00:48 2004-06-15 06:16:18   \n",
       "1    91900160                1 2003-05-27 12:42:07 2012-06-29 09:41:13   \n",
       "2    91900143                1 2004-03-08 13:44:31 2012-07-04 13:51:02   \n",
       "3    91900143                1 2004-03-10 11:39:06 2012-07-04 13:50:59   \n",
       "4    91900143                1 2004-10-22 12:21:54 2012-07-04 13:50:55   \n",
       "\n",
       "   unit_id unit  status_id  status  \n",
       "0      NaN  NaN          2  Closed  \n",
       "1      NaN  NaN          2  Closed  \n",
       "2      NaN  NaN          2  Closed  \n",
       "3      NaN  NaN          2  Closed  \n",
       "4      NaN  NaN          2  Closed  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_purchase_orders['delivery_date'] = pd.to_datetime(data_purchase_orders['delivery_date'], utc=True).dt.tz_localize(None)\n",
    "data_purchase_orders['created_date_time'] = pd.to_datetime(data_purchase_orders['created_date_time'], utc=True).dt.tz_localize(None)\n",
    "data_purchase_orders['modified_date_time'] = pd.to_datetime(data_purchase_orders['modified_date_time'], utc=True).dt.tz_localize(None)\n",
    "print(\"Amount of purchase orders: \", len(data_purchase_orders['purchase_order_id']))\n",
    "data_purchase_orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f66aa4",
   "metadata": {},
   "source": [
    "#### Study of column values, especially NaN values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42433e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_nan = 0\n",
    "for elmt in data_receivals['net_weight']:\n",
    "    if pd.isna(elmt):\n",
    "        num_nan += 1\n",
    "\n",
    "print(num_nan)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1218eb",
   "metadata": {},
   "source": [
    "#### 16.10.: Initial Modeling & Lag Analysis\n",
    "\n",
    "**Baseline Models**:\n",
    "- Started with simple rule-based baseline (0 for inactive, 75% of PO qty for active) → Score: 134,136 (beat 0 VTs) ❌\n",
    "- Moved to XGBoost with basic features (365d, 90d aggregates, PO data) + quantile regression (α=0.2) → Score: 10,135 (beat 2 VTs) ✅\n",
    "\n",
    "**Lag Pattern Discovery**:\n",
    "- Analyzed delivery lag (actual arrival - expected delivery date)\n",
    "- Key finding: Median lag = -15 days (deliveries arrive ~2 weeks EARLY!)\n",
    "- Supplier-specific variation: std dev of 47.3 days across suppliers → significant\n",
    "- Product-specific variation: std dev of 11.4 days\n",
    "- Temporal stability: 2020+ data is stable (2004-2006 had weird patterns, excluded from lag calc)\n",
    "\n",
    "**Lag Adjustment Implementation**:\n",
    "- Computed supplier-specific median lags from 2020+ data\n",
    "- Adjusted PO expected_arrival = delivery_date + supplier_lag\n",
    "- Only count POs with expected_arrival in forecast window (critical!)\n",
    "- XGBoost with lag adjustment → Score: 10,135 ✅\n",
    "- LightGBM with lag adjustment → Score: 9,600 ✅ (best so far!)\n",
    "\n",
    "**Failed Experiments**:\n",
    "- Random Forest with mean predictions → Score: 16,763 ❌ (RF needs 20th percentile extraction, too slow for iteration)\n",
    "- Ensemble XGBoost + LightGBM → No improvement (models too correlated, -0.2% on validation)\n",
    "\n",
    "**Feature Engineering Attempts**:\n",
    "1. **2023-2024 training data + trend features + supplier categorical** → Score: 11,800 ❌\n",
    "   - Added: 30d, 180d aggregates, trend_ratio, acceleration, supplier_id as categorical\n",
    "   - Problem: 2023 data created distribution shift (2023 patterns ≠ 2025 patterns)\n",
    "   - Validation improved (24,248) but Kaggle worse → classic overfitting\n",
    "   \n",
    "2. **2024 data + supplier as categorical** → Score: 15,000 ❌❌\n",
    "   - Problem: LightGBM's categorical feature handling overfits with 87 suppliers on small dataset\n",
    "   - Learned: categorical_feature parameter is dangerous with limited data\n",
    "\n",
    "**Key Observations**:\n",
    "- LightGBM (9,600) slightly beats XGBoost (10,135) with same features\n",
    "- Lag adjustment is critical (improves ~16% from baseline)\n",
    "- More data ≠ better (2023 data hurts due to distribution shift)\n",
    "- Categorical features in LightGBM overfit easily\n",
    "- Validation loss can be misleading (need time-based validation for forecasting)\n",
    "- Random 80/20 split includes old patterns, but Kaggle tests on 2025 (unseen conditions)\n",
    "\n",
    "**Current Best**: LightGBM + lag adjustment (2024 data, 13 features) → 9,600 (beats 2-3 VTs)\n",
    "\n",
    "**Next Steps**: Test supplier_id as numeric feature (not categorical), add trend features carefully with 2024 data only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e505cf",
   "metadata": {},
   "source": [
    "### 17.10.: Validation Strategy, Feature Analysis & Zero-Inflation\n",
    "\n",
    "**Starting Point**: LightGBM with lag adjustment → Score: 9,600 (2024 data, time-based features)\n",
    "\n",
    "**Phase 1: Diagnostic Deep Dive**\n",
    "- Problem: Score stuck at 9,600 despite alpha adjustments (0.15, 0.20, 0.25, 0.30)\n",
    "- Root cause: Random 80/20 split mixes all months → model learns \"average 2024\" instead of declining trend\n",
    "- Key finding: **-35.6% decline** from training (Jan-Aug: 342k kg mean) to validation (Sep-Nov: 220k kg mean)\n",
    "\n",
    "**Validation Strategy Experiments**:\n",
    "1. **Random split + alpha=0.15 + 0.88 adjustment** → Score: 9,500 ✅\n",
    "   - Predictions: 37k kg × 0.88 = 32.7k kg mean\n",
    "   - Issue: Still predicting \"average\" rather than learning trend\n",
    "   \n",
    "2. **Time-based split + alpha=0.15** → Score: 9,500\n",
    "   - Training: Jan-Aug 2024, Validation: Sep-Nov 2024\n",
    "   - Model sees -35.6% decline but predictions still ~37k kg\n",
    "   \n",
    "3. **Time-based split + alpha=0.25 (removed 0.88)** → Score: 9,960 ❌\n",
    "   - Predictions increased to 45k kg\n",
    "   - Proved over-predicting makes score worse → test set has LOW activity\n",
    "   \n",
    "4. **Time-based split + alpha=0.15 + 0.85 adjustment** → Score: ~9,500\n",
    "   - No real improvement, wasted submission\n",
    "\n",
    "**Key Insight**: Hyperparameter tuning won't get from 9,600 → 5,000. Need fundamental changes.\n",
    "\n",
    "**Phase 2: Deep EDA - The Breakthrough**\n",
    "\n",
    "**Critical Discoveries**:\n",
    "1. **Zero-Inflation Problem** ⚠️\n",
    "   - Test set: 203 unique rm_ids\n",
    "   - **Only 60 (29.6%) have ANY 2024 data**\n",
    "   - **143 rm_ids (70.4%) are DEAD** (last delivery 2004-2005)\n",
    "   - **72.9% of test predictions (22,200/30,450) are for DEAD rm_ids**\n",
    "   - Problem: Quantile regression predicts positive values for everything → massive over-prediction\n",
    "\n",
    "2. **Feature Correlation Analysis** (Sep-Nov 2024):\n",
    "```\n",
    "   Strong features (correlation > 0.65):\n",
    "   - total_90, rate_90: +0.9526 ***\n",
    "   - recency_weighted: +0.9327 ***\n",
    "   - total_180: +0.9131 ***\n",
    "   - rate_30, total_30: +0.8872 ***\n",
    "   - active_ratio_90: +0.6716 ***\n",
    "   \n",
    "   Weak features (remove):\n",
    "   - momentum: +0.0713\n",
    "   - slope_90: +0.0084\n",
    "   - cv_90: -0.0207\n",
    "   - PO quantity: +0.0767\n",
    "```\n",
    "\n",
    "3. **Seasonality**: January vs other months 1.03x ratio → month/quarter features likely noise\n",
    "\n",
    "**Phase 3: The Working Solution** ✅\n",
    "\n",
    "**Two-Stage Model + Guardrails**:\n",
    "1. **Two-Stage Architecture**:\n",
    "   - Stage 1: LGBMClassifier (n=400) predicts P(delivery > 0)\n",
    "   - Stage 2: LGBMRegressor (quantile α=0.15) predicts amount IF delivered\n",
    "   - Combined: prediction = regressor × classifier_probability\n",
    "   - **Why it works**: Dead rm_ids get low probability (~0.1) → effectively zero prediction\n",
    "\n",
    "2. **Per-Horizon Calibration**:\n",
    "   - Learned on Sep-Nov validation for horizons {7, 30, 60, 90, 150}\n",
    "   - factor_h = sum(actual) / sum(predicted), clipped to [0.70, 1.10]\n",
    "   - Fixes systematic over/under-prediction at different horizons\n",
    "\n",
    "3. **Activity-Based Guardrails**:\n",
    "   - days_since_last > 365 → force 0\n",
    "   - 180 < days_since_last ≤ 365 → soft cap at 8% of last year volume\n",
    "   - Upper cap: min(pred, (total_365/365) × horizon × 1.5)\n",
    "\n",
    "**Result**: 9,600 → **8,200** ✅✅ (14% improvement!)\n",
    "\n",
    "**Phase 4: Feature Engineering - Step 1** (In Progress)\n",
    "\n",
    "**Removed (9 weak features)**:\n",
    "- ❌ momentum, slope_90, cv_90 (correlation < 0.1)\n",
    "- ❌ future_po_quantity, future_po_count (correlation 0.08)\n",
    "- ❌ month, quarter (weak seasonality)\n",
    "- ❌ avg_weight_365d, daily_rate_365d (redundant)\n",
    "\n",
    "**Added (5 strong features)**:\n",
    "- ✅ recency_weighted (correlation 0.93)\n",
    "- ✅ total_30, rate_30, count_30 (correlation 0.89)\n",
    "- ✅ active_ratio_90 (correlation 0.67)\n",
    "\n",
    "**Net**: 16 features → 14 features (more focused, less noise)\n",
    "**Expected**: 8,200 → 7,200-7,500\n",
    "\n",
    "**Key Learnings**:\n",
    "- Zero-inflation critical: 73% of test should be ~0, quantile regression can't learn this\n",
    "- Domain knowledge > ML: Hard-coded rules (dead = 0) beat learned patterns\n",
    "- Feature correlation matters: momentum (0.07 correlation) = noise\n",
    "- Validation strategy crucial: Random split hides declining trend\n",
    "- EDA before iteration: Deep analysis revealed 73% dead rm_ids problem\n",
    "\n",
    "**Failed Experiments**:\n",
    "- Random split + alpha tuning → 9,500-9,960 ❌ (doesn't learn trend)\n",
    "- Remove 0.88 adjustment → 9,960 ❌ (predictions too high)\n",
    "- Time-based + various alphas → 9,500-9,960 ❌ (hyperparameter band-aid)\n",
    "\n",
    "**Current Best**: Two-stage + guardrails → 8,200 (beats ~3-4 VTs)\n",
    "\n",
    "**Next Steps** (if Step 1 works):\n",
    "1. Interpolate calibration for horizons 2-151\n",
    "2. Activity-aware guardrails (smooth decay)\n",
    "3. Ensemble with median predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cff16b1",
   "metadata": {},
   "source": [
    "### 18.10.: Feature Pruning Based on Correlation Analysis\n",
    "\n",
    "**Removed (9 weak features)**:\n",
    "- ❌ momentum, slope_90, cv_90 (correlation < 0.1)\n",
    "- ❌ future_po_quantity, future_po_count (correlation 0.08)\n",
    "- ❌ month, quarter (weak seasonality)\n",
    "- ❌ avg_weight_365d, daily_rate_365d (redundant)\n",
    "\n",
    "**Added (5 strong features)**:\n",
    "- ✅ recency_weighted (correlation 0.93) - weights recent deliveries higher\n",
    "- ✅ total_30, rate_30, count_30 (correlation 0.89) - captures very recent activity\n",
    "- ✅ active_ratio_90 (correlation 0.67) - ratio of active days in last 90d\n",
    "\n",
    "**Net**: 16 features → 14 features (more focused, less noise)\n",
    "\n",
    "**Result**: 8,200 → **6,200** ✅✅✅ (24% improvement!)\n",
    "\n",
    "**Key Insight**: Weak features weren't neutral - they were actively adding noise and confusing the model. Removing them + adding highly correlated features = massive gain.\n",
    "\n",
    "**Total Progress**: 9,600 (baseline) → 6,200 (35% improvement!)\n",
    "\n",
    "---\n",
    "\n",
    "#### Fine-Tuning Attempts (Hit Plateau)\n",
    "\n",
    "**Step 2: Interpolated Calibration** → 6,700 ❌ (worse by 500)\n",
    "- Tried to fill calibration for ALL horizons 2-151 (not just {7,30,60,90,150})\n",
    "- Used scipy.interpolate.interp1d to smooth factors across horizons\n",
    "- **Failed:** Overfitted to Sep-Nov 2024 patterns, didn't generalize to 2025 test set\n",
    "- Learning: Validation-learned adjustments don't always transfer to test\n",
    "\n",
    "**Step 3: Aggressive Dead Filtering** → 6,200 (no change)\n",
    "- Tightened guardrails: cold rm_ids (180-365d) from 8% cap → 5% cap\n",
    "- Added new warm rm_ids (90-180d) cap at 15%\n",
    "- **No effect:** Only affected 19/203 rm_ids, and their predictions were already low from two-stage model\n",
    "- Learning: Guardrails already optimal from ChatGPT's original implementation\n",
    "\n",
    "**Step 4: Lower Alpha (0.15 → 0.10)** → 6,140 ✅ (tiny improvement, -60 points)\n",
    "- Changed quantile target from 15th percentile to 10th percentile\n",
    "- More conservative predictions\n",
    "- **Minimal improvement:** Suggests we're close to optimal conservatism level\n",
    "\n",
    "**Step 5: Remove Calibration Entirely** → 6,138 ✅ (basically same, -2 points)\n",
    "- Removed all per-horizon calibration adjustments\n",
    "- Let model's forecast_horizon feature handle effects naturally\n",
    "- **No impact:** Calibration was irrelevant (factors too close to 1.0)\n",
    "- Learning: Sep-Nov calibration neither helped nor hurt\n",
    "\n",
    "**Key Observations**:\n",
    "- Hit a plateau around 6,100-6,200 - small tweaks (±60 points) don't move the needle\n",
    "- Feature engineering (Step 1) gave 24% improvement, everything else combined gave <1%\n",
    "- Calibration/guardrails/alpha tuning are all second-order effects\n",
    "- Need <5,000 for grade A (currently 6,138) - requires 18.5% more improvement\n",
    "\n",
    "**Failed Experiments**:\n",
    "- Interpolated calibration → overfitting to validation\n",
    "- Tighter guardrails → no effect (already optimal)\n",
    "- Lower alpha → diminishing returns\n",
    "\n",
    "**Current Best**: Step 5 (6,138 points) with alpha=0.10, no calibration, Step 1 features\n",
    "\n",
    "**Possible next steps** (untested):\n",
    "- Even lower alpha (0.05 or 0.08) for maximum conservatism\n",
    "- Nuclear option: predict 0 for >90 days inactive (high risk/reward)\n",
    "- Activity-aware upper caps (make hot rm_ids able to spike more, cold less)\n",
    "\n",
    "prob need somthing fundamentally different to actually improve our score further"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04045a94",
   "metadata": {},
   "source": [
    "### 21.10.: Purchase Orders EDA → Tried “Forward-Looking” Features (…and why it blew up)\n",
    "\n",
    "**What we set out to do**\n",
    "- Deep-dive EDA on `purchase_orders` to see if future orders can guide Jan–May 2025 forecasts.\n",
    "- Add a small set of PO features (sum/count/has_po/avg) on top of **Step 5** (the 6,200 model), *without* touching anything else.\n",
    "\n",
    "---\n",
    "\n",
    "### EDA Highlights (looked very promising)\n",
    "- **We do have future POs** in the test window: **147** POs for Jan–May 2025 ✅  \n",
    "- **Strong historical link:** Ordered vs. actual **corr = 0.912** (on matched 2024 POs) ✅  \n",
    "- **Lead time**: median ~**51 days** (very wide spread, std ~55d) ⚠️  \n",
    "- **Coverage**: PO signal covers ~**45%** of test RMs (92/203) ⚠️\n",
    "\n",
    "**Takeaway:** POs looked like a real, forward-looking signal worth trying.\n",
    "\n",
    "---\n",
    "\n",
    "### What we tried (Step 10 variants)\n",
    "1) **Straight PO features:**  \n",
    "   - `po_total_in_horizon`, `po_count_in_horizon`, `has_future_po`, `po_avg_historical` (product-level, mapped to RM by mode product).  \n",
    "   - Precomputed cache for speed, kept the **exact same** Step 5 model + guardrails.\n",
    "\n",
    "2) **Fixed/safer version:**  \n",
    "   - Ignored “Deleted” POs, down-weighted “Open” vs “Closed”.  \n",
    "   - Allocated product POs to RMs using **historical product→RM shares** (to avoid double-counting).  \n",
    "   - Still no changes to the base model or caps besides existing Step 5 guardrails.\n",
    "\n",
    "---\n",
    "\n",
    "### What happened (ouch)\n",
    "- **Leaderboard exploded upward:** ~**9,000–10,000** (from 6,200). ❌❌❌  \n",
    "- **Feature importance:** PO features jumped to the **top** (e.g., `po_total_in_horizon` rank #1/#2).  \n",
    "- **Predictions became bimodal:**\n",
    "  - With PO signal (~35% rows): **~89k** avg prediction (huge spikes)\n",
    "  - Without PO (~65% rows): **~1.3k** avg prediction (collapsed to near zero)\n",
    "- **Train→test coverage shift:** Train windows had ~70% rows with PO; test only ~45% → the model over-trusts PO features and underpredicts where PO is missing.\n",
    "\n",
    "---\n",
    "\n",
    "### Why it went wrong (post-mortem)\n",
    "- **Allocation noise:** We spread product POs to RMs via historical shares → many RMs get “slivers” they won’t actually receive in 2025, and the few real recipients get diluted. The model learns a hard rule: “PO present ⇒ huge; no PO ⇒ tiny,” which doesn’t match reality.  \n",
    "- **Timing mismatch:** Features use **planned** `delivery_date`, but actual arrivals drift (median lead time ~51d, big variance). We dump all PO mass inside the horizon, even when the true receivals slip outside.  \n",
    "- **Coverage shift:** The model sees lots of PO rows in training (70%) and fewer in test (45%), so it overweights PO signals and forgets the baseline on “no-PO” rows.  \n",
    "- **Scale issue:** `po_total_in_horizon` is a blunt, un-normalized lump → huge spikes that Step 5’s guardrails don’t cap (upper cap is generous for high-volume RMs).  \n",
    "- **Mode product mapping:** RMs often have multiple products; using only the mode can attach the **wrong** PO signal to an RM.\n",
    "\n",
    "**Net effect:** The model became **over-confident** wherever PO existed and **under-confident** elsewhere. Big absolute errors on both tails ⇒ leaderboard tanks.\n",
    "\n",
    "---\n",
    "\n",
    "### What *did* work\n",
    "- The **EDA** was right: POs carry signal. The failure was not in “POs are useless,” but in how we **operationalized** them.  \n",
    "- Our “don’t touch the base model / keep logging / fast cache” discipline made it easy to diagnose the blow-ups quickly.\n",
    "\n",
    "---\n",
    "\n",
    "### What we’ll try tomorrow (surgical fixes, still minimal changes)\n",
    "1) **Time-spreading the PO mass**  \n",
    "   Convert PO quantity to **expected daily arrival** (e.g., distribute across the delivery month, or convolve with a lead-time kernel centered ~51d). Feature = **sum over overlap with horizon**, not the whole lump.\n",
    "2) **Normalize + cap PO features**  \n",
    "   Winsorize `po_total_in_horizon` and cap it by a **baseline capacity** (e.g., `min(po_expected, c × rate_90 × horizon)`), so trees can’t run away.\n",
    "3) **Keep baseline alive when PO is absent**  \n",
    "   Ensure “no-PO” rows still lean on `rate_30/90`, `recency_weighted`, etc. (e.g., PO features act as **additive deltas**, not replacements).\n",
    "4) **Tighter allocation set**  \n",
    "   Use **recent (2023–2024)** product→RM shares only, and restrict to **active RMs** for that product in 2024 to reduce smearing.\n",
    "5) **Reliability weighting**  \n",
    "   Down-weight Open/old/edited POs more aggressively; add “recency of PO creation/modification” as a reliability hint.\n",
    "6) **Sanity dashboards** (quick prints only)  \n",
    "   Compare preds/actuals by buckets: `(PO yes/no) × (horizon bucket)` to spot immediate drift before submitting.\n",
    "\n",
    "---\n",
    "\n",
    "### Current status\n",
    "- **Best:** Step 5 (no calibration, α=0.10) ≈ **6,200**  \n",
    "- **Today’s PO attempts:** **~9,000–10,000** (fail)  \n",
    "- **Conclusion:** POs are promising but brittle. We need time-aligned, capacity-capped, allocation-pruned features so they act as **soft guidance**, not a hard override.\n",
    "\n",
    "---\n",
    "\n",
    "### One-liner for the report\n",
    "> We tried adding forward-looking purchase order features (strong EDA signal), but naïve aggregation and rough allocation/timing caused bimodal predictions and a big LB spike. Tomorrow we’ll spread PO mass over time, cap by capacity, and restrict allocation to recent active RMs so PO acts as a stabilizer rather than a sledgehammer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52e7c24",
   "metadata": {},
   "source": [
    "### 01.11.25: Feature Engineering Attempts → Timing Features & Model Splitting (…and why they failed)\n",
    "\n",
    "**What we set out to do**\n",
    "- Address the 70% inactive RM_IDs in test set through specialized feature engineering and model architecture changes\n",
    "- Hypothesis: One model can't handle both active (recent data) and inactive (stale/unknown) RM_IDs well\n",
    "- Try multiple approaches on top of **BEST MODEL** (6,236 points, α=0.10, 15 features)\n",
    "\n",
    "---\n",
    "\n",
    "### Attempt 1: Timing Features (Step 8 & 9)\n",
    "**EDA Motivation:**\n",
    "- Delivery timing analysis showed high irregularity (CV = 2.75)\n",
    "- 42.9% of 7-day forecasts expected to be zero\n",
    "- Expected deliveries scale non-linearly with horizon (7d: 5.53 deliveries → 150d: 118.57 deliveries)\n",
    "\n",
    "**What we tried:**\n",
    "1. **Step 8 (4 timing features):**\n",
    "   - `avg_days_between`: Average interval between deliveries\n",
    "   - `cv_days_between`: Coefficient of variation (regularity metric)\n",
    "   - `expected_deliveries_in_horizon`: horizon / avg_days_between\n",
    "   - `delivery_overdue_ratio`: days_since_last / avg_days_between\n",
    "   - Total features: 19 (was 15)\n",
    "\n",
    "2. **Step 9 (2 \"safe\" timing features):**\n",
    "   - Kept only `avg_days_between` and `cv_days_between`\n",
    "   - Removed the \"aggressive\" features that encoded staleness\n",
    "   - Total features: 17\n",
    "\n",
    "**What happened:**\n",
    "- **Step 8 (4 features):** Score = **6,375** (+139 points worse) ❌\n",
    "- **Step 9 (2 features):** Score = **6,375** (no improvement) ❌\n",
    "- **Feature importance:** Timing features ranked #1 and #2 (very high)\n",
    "- **Predictions collapsed:**\n",
    "  - Predictions > 0: Only 6,884-7,236 (22-24% of test set)\n",
    "  - Should have been ~40-50%\n",
    "- **Validation improved** (-14%) but **test performance tanked**\n",
    "\n",
    "**Why it went wrong:**\n",
    "- **Overfitting to validation decline:** Sep-Nov 2024 showed declining activity; timing features learned this pattern\n",
    "- **Too literal on inactivity:** For unknown RM_IDs with sparse history:\n",
    "  - `avg_days_between = 0` → model learned \"no history = no delivery\"\n",
    "  - `cv_days_between = 999` → model learned \"irregular = no delivery\"\n",
    "  - `delivery_overdue_ratio = 56.88` → model learned \"extremely overdue = no delivery\"\n",
    "- **Train-test mismatch:** Validation (Sep-Nov 2024) was declining; test (Jan-May 2025) might have reactivations\n",
    "- **The model predicted ZERO for most inactive RM_IDs** when it should have given them small non-zero probabilities\n",
    "\n",
    "---\n",
    "\n",
    "### Attempt 2: Separate Models for Active/Inactive (Step 10)\n",
    "**Rationale:**\n",
    "- One model tries to serve two populations: active (good data) vs inactive (sparse/old data)\n",
    "- Train separate models with different alpha values based on inactivity level\n",
    "\n",
    "**What we tried:**\n",
    "- **Threshold:** days_since_last = 180 days\n",
    "- **Active model (≤180d):** α=0.15 (more aggressive)\n",
    "- **Inactive model (>180d):** α=0.05 (very conservative)\n",
    "- Route predictions based on days_since_last at test time\n",
    "\n",
    "**What happened:**\n",
    "- **Training data imbalance:**\n",
    "  - Active samples: 2,600 (95.4%)\n",
    "  - Inactive samples: 125 (4.6%)\n",
    "  - Inactive train set: Only **50 samples** after time split ❌\n",
    "- **Inactive model failed completely:**\n",
    "  - Mean prediction: 0.00 (predicted zero for everything)\n",
    "  - Bias: -100%\n",
    "  - Regressor loss didn't change across epochs → model didn't learn\n",
    "- **Test routing was useless:**\n",
    "  - 97.5% routed to active model\n",
    "  - Only 2.5% used inactive model\n",
    "\n",
    "**Why it went wrong:**\n",
    "- **Training only on 2024-active RM_IDs** means very few inactive samples in training\n",
    "- Can't train a meaningful model on 50 samples\n",
    "- Even if threshold raised to 365d, still not enough inactive training data\n",
    "- **Fundamental issue:** Training set (2024-active RMs) doesn't represent test set (70% unknown RMs)\n",
    "\n",
    "---\n",
    "\n",
    "### Attempt 3: Hybrid Rules (Step 11)\n",
    "**Pivot strategy:**\n",
    "- Accept we can't train on inactive RM_IDs (not enough data)\n",
    "- Train ONE strong model on all available data\n",
    "- Apply conservative **rules** at prediction time based on inactivity\n",
    "\n",
    "**What we tried:**\n",
    "```python\n",
    "if days_inactive ≤ 90:   pred = model_pred × 1.0  # No penalty\n",
    "elif days_inactive ≤ 180: pred = model_pred × 1.0  # No penalty\n",
    "elif days_inactive ≤ 365: pred = model_pred × 0.8  # 20% reduction\n",
    "elif days_inactive ≤ 730: pred = model_pred × 0.3  # 70% reduction\n",
    "else:                     pred = 0.0               # Zero for >2 years\n",
    "```\n",
    "\n",
    "**Critical bug discovered:**\n",
    "- Original `days_since_last` calculation was WRONG:\n",
    "```python\n",
    "  # BUG: Used recent_365 window\n",
    "  if len(recent_365) > 0:\n",
    "      days_since = (date - recent_365.max()).days\n",
    "  else:\n",
    "      days_since = 0  # ← Made inactive RMs look active!\n",
    "```\n",
    "- **Fixed:** Calculate from ALL history, not just recent window\n",
    "- After fix, test set correctly showed: 64.5% very inactive (>730d)\n",
    "\n",
    "**What happened:**\n",
    "- **Validation:** Minimal improvement (rules barely applied - only 0.6% inactive)\n",
    "- **Test routing:**\n",
    "  - 64.5% of RM_IDs got ZERO predictions\n",
    "  - These are the 131 RM_IDs NOT in training at all\n",
    "- **Not yet submitted** - awaiting decision on rule aggressiveness\n",
    "\n",
    "**Open question:**\n",
    "- Is zeroing out 64.5% of test RM_IDs correct?\n",
    "- Or should unknown RM_IDs get small non-zero predictions (e.g., 0.1× model)?\n",
    "- Quantile loss at 0.2 means underestimation is cheaper, but not free\n",
    "\n",
    "---\n",
    "\n",
    "### Root Cause Analysis\n",
    "\n",
    "**The fundamental problem:**\n",
    "1. **Training set:** 60 RM_IDs active in 2024 (by design - other years non-representative)\n",
    "2. **Test set:** 203 RM_IDs, of which 131-143 (64-70%) were NOT active in 2024\n",
    "3. **Coverage gap:** Model has never seen 64% of test RM_IDs\n",
    "4. **No signal for unknowns:** These RM_IDs have no recent activity (>730d inactive)\n",
    "\n",
    "**Why all attempts failed:**\n",
    "- **Timing features:** Learned to penalize unknowns too aggressively\n",
    "- **Separate models:** Can't train inactive model without inactive data\n",
    "- **Hybrid rules:** Unknown if forcing zeros is optimal strategy\n",
    "\n",
    "**What we know works:**\n",
    "- BEST MODEL (6,236): Lets model handle unknowns naturally through existing features\n",
    "- Any explicit \"inactive RM_ID\" handling makes things worse (so far)\n",
    "\n",
    "---\n",
    "\n",
    "### Current Status\n",
    "- **Best:** BEST MODEL (α=0.10, 15 features, no special inactive handling) = **6,236**\n",
    "- **Step 8 (4 timing features):** **6,375** (-139 points)\n",
    "- **Step 9 (2 timing features):** **6,375** (-139 points)\n",
    "- **Step 10 (separate models):** Not submitted (inactive model broken)\n",
    "- **Step 11 (hybrid rules):** Pending submission decision\n",
    "\n",
    "**Conclusion:** \n",
    "The 64% unknown RM_ID problem cannot be solved through feature engineering or model architecture alone when training only on 2024 data. Either:\n",
    "1. Find external signal for unknowns (product groups, supplier patterns, purchase orders)\n",
    "2. Accept model must extrapolate and optimize the extrapolation strategy\n",
    "3. Revisit \"2024-only training\" constraint if validation shows merit\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps (TBD)\n",
    "- Submit Step 11 and evaluate if aggressive zero-ing helps or hurts\n",
    "- Explore product/alloy grouping to give unknowns \"borrowed\" patterns from similar known RMs\n",
    "- Revisit purchase orders with lessons learned (avoid bimodal collapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731b9e1d",
   "metadata": {},
   "source": [
    "## Feature engineering:\n",
    "\n",
    "The purpose of performing feature engineering on the datasets is to increase the performance of a predicting model. This can for example be done by removing features, merge features or giving features extra \"weight\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8558a4af",
   "metadata": {},
   "source": [
    "### Important observations\n",
    "\n",
    "This section will summarize the steps taken in next subsection, and will include the most important observations taken during feature engineering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18be8b81",
   "metadata": {},
   "source": [
    "### Steps taken\n",
    "\n",
    "THE SMOKING GUN:\n",
    "text\n",
    "Average CV: 2.75 (HIGHLY IRREGULAR)\n",
    "Regular patterns (<CV 1.0): 20.4%\n",
    "7-day horizon: 42.9% zeros\n",
    "Your current features completely ignore delivery timing patterns, which is why your model can't scale predictions properly.\n",
    "\n",
    "THE FUNDAMENTAL PROBLEM:\n",
    "Your model is trying to predict \"how much\" without knowing \"when\". But with irregular deliveries (CV 2.75), timing is everything.\n",
    "\n",
    "THE SOLUTION: TWO-STAGE MODEL + TIMING FEATURES\n",
    "Stage 1: Binary Classification (Will there be ANY delivery?)\n",
    "Target: will_deliver = (target > 0)\n",
    "\n",
    "Use: Logistic regression, LightGBM classifier\n",
    "\n",
    "Key features: days_since_last / avg_days_between, horizon / avg_days_between\n",
    "\n",
    "Stage 2: Quantity Prediction (If yes, how much?)\n",
    "Target: delivery_amount (only for positive samples)\n",
    "\n",
    "Use: Quantile regression (0.2 quantile)\n",
    "\n",
    "Key features: Add timing-aware features\n",
    "\n",
    "CRITICAL NEW FEATURES TO ADD:\n",
    "python\n",
    "# Timing-aware features\n",
    "df['expected_deliveries_in_horizon'] = df['forecast_horizon'] / df['avg_days_between_deliveries']\n",
    "df['delivery_overdue_ratio'] = df['days_since_last'] / df['avg_days_between_deliveries']\n",
    "df['delivery_regularity'] = df['cv_days_between_deliveries']\n",
    "df['is_frequent_deliverer'] = (df['avg_days_between_deliveries'] < 7).astype(int)\n",
    "df['is_rare_deliverer'] = (df['avg_days_between_deliveries'] > 90).astype(int)\n",
    "\n",
    "# Horizon-adjusted rate features\n",
    "df['horizon_adjusted_rate'] = df['rate_90'] * (df['forecast_horizon'] / 90)\n",
    "df['expected_weight'] = df['avg_delivery_size'] * df['expected_deliveries_in_horizon']\n",
    "IMMEDIATE ACTION PLAN:\n",
    "Step 1: Create Timing Features\n",
    "Add these for each RM_ID:\n",
    "\n",
    "avg_days_between_deliveries\n",
    "\n",
    "cv_days_between_deliveries\n",
    "\n",
    "avg_delivery_size\n",
    "\n",
    "last_year_delivery_count\n",
    "\n",
    "Step 2: Two-Stage Training\n",
    "python\n",
    "# Stage 1: Classification\n",
    "clf_predictor = TabularPredictor(label='will_deliver', problem_type='binary').fit(...)\n",
    "\n",
    "# Stage 2: Regression (only positive samples)\n",
    "pos_samples = train_df[train_df['target'] > 0]\n",
    "reg_predictor = TabularPredictor(label='target', problem_type='quantile', quantile_levels=[0.2]).fit(pos_samples, ...)\n",
    "Step 3: Combined Prediction\n",
    "python\n",
    "# Predict probability of delivery\n",
    "delivery_proba = clf_predictor.predict_proba(test_data)['1']\n",
    "\n",
    "# Predict amount if delivered  \n",
    "delivery_amount = reg_predictor.predict(test_data)\n",
    "\n",
    "# Combine: amount * probability (conservative)\n",
    "final_prediction = delivery_amount * delivery_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab8201e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d833710d",
   "metadata": {},
   "source": [
    "## Model training:\n",
    "\n",
    "This is the section where we will train different models on the data, and try to find the best model for the problem. At the end of this section we should have a model that performs well on the data, and is able to make good predictions on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e417bbb",
   "metadata": {},
   "source": [
    "### Important observations\n",
    "This section will summarize the steps taken in next subsection, and will include the most important observations taken during model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9603e94",
   "metadata": {},
   "source": [
    "### Steps taken\n",
    "This section describes all the steps taken during model training. All steps taken are to be included here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e05650e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10d4062f",
   "metadata": {},
   "source": [
    "## Model evaluation and interpretation:\n",
    "The purpose of this section is to evaluate the model performance and interpret the model. This will include feature importance, SHAP values and partial dependence plots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4712f58a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
