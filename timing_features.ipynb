{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9dd2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 9: SAFE TIMING FEATURES ONLY\n",
      "================================================================================\n",
      "\n",
      "[1] DATA CLEANING\n",
      "Clean receivals: 122383\n",
      "\n",
      "[2] PRE-COMPUTING TIMING STATISTICS\n",
      "--------------------------------------------------------------------------------\n",
      "Computed timing stats for 203 rm_ids\n",
      "RM_IDs with computable intervals: 174\n",
      "\n",
      "[3] CREATING TRAINING DATA WITH SAFE TIMING FEATURES\n",
      "--------------------------------------------------------------------------------\n",
      "Using 11 training dates x 5 horizons\n",
      "Active rm_ids in 2024: 60\n",
      "Processing date 1/11: 2024-01-01...\n",
      "Processing date 2/11: 2024-02-01...\n",
      "Processing date 3/11: 2024-03-01...\n",
      "Processing date 4/11: 2024-04-01...\n",
      "Processing date 5/11: 2024-05-01...\n",
      "Processing date 6/11: 2024-06-01...\n",
      "Processing date 7/11: 2024-07-01...\n",
      "Processing date 8/11: 2024-08-01...\n",
      "Processing date 9/11: 2024-09-01...\n",
      "Processing date 10/11: 2024-10-01...\n",
      "Processing date 11/11: 2024-11-01...\n",
      "\n",
      "Generated 2725 training samples\n",
      "Samples with target > 0: 1816 (66.6%)\n",
      "\n",
      "[4] TIME-BASED TRAIN/VAL SPLIT\n",
      "--------------------------------------------------------------------------------\n",
      "Training samples (before 2024-09-01): 1895\n",
      "Validation samples (>= 2024-09-01): 830\n",
      "\n",
      "Target statistics:\n",
      "  Training mean: 341,946 kg\n",
      "  Validation mean: 220,249 kg\n",
      "  Difference: -35.6%\n",
      "\n",
      "Number of features: 17\n",
      "Features: ['rm_id', 'forecast_horizon', 'total_weight_365d', 'count_365d', 'days_since_last', 'total_weight_90d', 'count_90d', 'rate_90', 'total_weight_180d', 'count_180d', 'total_30', 'count_30', 'rate_30', 'recency_weighted', 'active_ratio_90', 'avg_days_between', 'cv_days_between']\n",
      "\n",
      "[5] TRAINING LightGBM MODELS\n",
      "--------------------------------------------------------------------------------\n",
      "Training samples: 1895\n",
      "Validation samples: 830\n",
      "Training LightGBM Classifier...\n",
      "[100]\tvalid_0's binary_logloss: 0.44676\n",
      "[200]\tvalid_0's binary_logloss: 0.536346\n",
      "[300]\tvalid_0's binary_logloss: 0.639304\n",
      "[400]\tvalid_0's binary_logloss: 0.747306\n",
      "Training LightGBM Regressor with alpha=0.10...\n",
      "Confirmed alpha value: 0.1\n",
      "[100]\tvalid_0's quantile: 22865.6\n",
      "[200]\tvalid_0's quantile: 31047.1\n",
      "[300]\tvalid_0's quantile: 38335.9\n",
      "\n",
      "================================================================================\n",
      "FEATURE IMPORTANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "[A] CLASSIFIER FEATURE IMPORTANCE (Top 15)\n",
      "--------------------------------------------------------------------------------\n",
      "  days_since_last                    :   1559.0 (14.50%)\n",
      "  total_weight_365d                  :   1176.0 (10.94%)\n",
      "  cv_days_between                    :    924.0 ( 8.59%)\n",
      "  recency_weighted                   :    867.0 ( 8.06%)\n",
      "  total_weight_180d                  :    831.0 ( 7.73%)\n",
      "  count_365d                         :    761.0 ( 7.08%)\n",
      "  forecast_horizon                   :    741.0 ( 6.89%)\n",
      "  rm_id                              :    689.0 ( 6.41%)\n",
      "  avg_days_between                   :    656.0 ( 6.10%)\n",
      "  total_weight_90d                   :    614.0 ( 5.71%)\n",
      "  count_90d                          :    463.0 ( 4.31%)\n",
      "  count_180d                         :    441.0 ( 4.10%)\n",
      "  total_30                           :    420.0 ( 3.91%)\n",
      "  active_ratio_90                    :    293.0 ( 2.72%)\n",
      "  count_30                           :    161.0 ( 1.50%)\n",
      "\n",
      "[B] REGRESSOR FEATURE IMPORTANCE (Top 15)\n",
      "--------------------------------------------------------------------------------\n",
      "  total_weight_365d                  :   1011.0 (14.16%)\n",
      "  forecast_horizon                   :    914.0 (12.80%)\n",
      "  days_since_last                    :    802.0 (11.24%)\n",
      "  rm_id                              :    686.0 ( 9.61%)\n",
      "  total_weight_180d                  :    604.0 ( 8.46%)\n",
      "  cv_days_between                    :    545.0 ( 7.64%)\n",
      "  recency_weighted                   :    477.0 ( 6.68%)\n",
      "  total_weight_90d                   :    404.0 ( 5.66%)\n",
      "  count_365d                         :    382.0 ( 5.35%)\n",
      "  avg_days_between                   :    350.0 ( 4.90%)\n",
      "  total_30                           :    245.0 ( 3.43%)\n",
      "  count_90d                          :    186.0 ( 2.61%)\n",
      "  count_180d                         :    153.0 ( 2.14%)\n",
      "  active_ratio_90                    :    149.0 ( 2.09%)\n",
      "  count_30                           :    126.0 ( 1.77%)\n",
      "\n",
      "[C] COMBINED IMPORTANCE (Top 15)\n",
      "--------------------------------------------------------------------------------\n",
      "  days_since_last                    : Clf=14.50% | Reg=11.24% | Avg=12.87%\n",
      "  total_weight_365d                  : Clf=10.94% | Reg=14.16% | Avg=12.55%\n",
      "  forecast_horizon                   : Clf= 6.89% | Reg=12.80% | Avg= 9.85%\n",
      "  cv_days_between                    : Clf= 8.59% | Reg= 7.64% | Avg= 8.11%\n",
      "  total_weight_180d                  : Clf= 7.73% | Reg= 8.46% | Avg= 8.09%\n",
      "  rm_id                              : Clf= 6.41% | Reg= 9.61% | Avg= 8.01%\n",
      "  recency_weighted                   : Clf= 8.06% | Reg= 6.68% | Avg= 7.37%\n",
      "  count_365d                         : Clf= 7.08% | Reg= 5.35% | Avg= 6.21%\n",
      "  total_weight_90d                   : Clf= 5.71% | Reg= 5.66% | Avg= 5.68%\n",
      "  avg_days_between                   : Clf= 6.10% | Reg= 4.90% | Avg= 5.50%\n",
      "  total_30                           : Clf= 3.91% | Reg= 3.43% | Avg= 3.67%\n",
      "  count_90d                          : Clf= 4.31% | Reg= 2.61% | Avg= 3.46%\n",
      "  count_180d                         : Clf= 4.10% | Reg= 2.14% | Avg= 3.12%\n",
      "  active_ratio_90                    : Clf= 2.72% | Reg= 2.09% | Avg= 2.41%\n",
      "  count_30                           : Clf= 1.50% | Reg= 1.77% | Avg= 1.63%\n",
      "\n",
      "[D] LOW IMPORTANCE FEATURES (<1% avg)\n",
      "--------------------------------------------------------------------------------\n",
      "  rate_90                            :  0.99%\n",
      "  rate_30                            :  0.47%\n",
      "\n",
      "⚠️  Consider removing 2 features in next iteration\n",
      "\n",
      "[E] SAFE TIMING FEATURES PERFORMANCE\n",
      "--------------------------------------------------------------------------------\n",
      "  avg_days_between                   : Clf= 6.10% | Reg= 4.90% | Avg= 5.50%\n",
      "  cv_days_between                    : Clf= 8.59% | Reg= 7.64% | Avg= 8.11%\n",
      "\n",
      "================================================================================\n",
      "VALIDATION PERFORMANCE\n",
      "================================================================================\n",
      "Mean Quantile Loss (0.2): 43,459.92\n",
      "Median Quantile Loss: 3,599.81\n",
      "Mean Absolute Error: 118,739.12\n",
      "\n",
      "Mean Target: 220,248.89\n",
      "Mean Prediction: 167,216.75\n",
      "Bias: -24.1%\n",
      "\n",
      "[6] MAKING PREDICTIONS FOR 2025\n",
      "--------------------------------------------------------------------------------\n",
      "Pre-computed features for 203 rm_ids\n",
      "Processed 5000/30450...\n",
      "Processed 10000/30450...\n",
      "Processed 15000/30450...\n",
      "Processed 20000/30450...\n",
      "Processed 25000/30450...\n",
      "Processed 30000/30450...\n",
      "\n",
      "[7] PREDICTION STATISTICS\n",
      "--------------------------------------------------------------------------------\n",
      "Predictions mean: 44,582 kg\n",
      "\n",
      "Prediction statistics:\n",
      "count    3.045000e+04\n",
      "mean     4.458188e+04\n",
      "std      2.208414e+05\n",
      "min      0.000000e+00\n",
      "25%      0.000000e+00\n",
      "50%      0.000000e+00\n",
      "75%      0.000000e+00\n",
      "max      2.549432e+06\n",
      "Name: predicted_weight, dtype: float64\n",
      "Predictions > 0: 7236\n",
      "\n",
      "[8] CREATING SUBMISSION\n",
      "--------------------------------------------------------------------------------\n",
      "✅ Saved to 'STEP9_SAFE_TIMING.csv'\n",
      "\n",
      "================================================================================\n",
      "COMPLETE - STEP 9: SAFE TIMING FEATURES\n",
      "================================================================================\n",
      "\n",
      "Changes from BEST MODEL (6236):\n",
      "  ✅ Added 2 SAFE timing features:\n",
      "     - avg_days_between (general pattern)\n",
      "     - cv_days_between (regularity metric)\n",
      "  ❌ REMOVED aggressive timing features:\n",
      "     - expected_deliveries_in_horizon\n",
      "     - delivery_overdue_ratio\n",
      "  ✅ Kept alpha=0.10 (same as best model)\n",
      "  ✅ Kept original guardrails (same as best model)\n",
      "  ✅ Total features: 17 (was 15)\n",
      "\n",
      "  Expected: Better generalization to test set\n",
      "  These features add context WITHOUT forcing zeros on inactive RM_IDs\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from datetime import timedelta\n",
    "\n",
    "receivals = pd.read_csv('./Project_materials/data/kernel/receivals.csv')\n",
    "purchase_orders = pd.read_csv('./Project_materials/data/kernel/purchase_orders.csv')\n",
    "prediction_mapping = pd.read_csv('./Project_materials/data/prediction_mapping.csv')\n",
    "sample_submission = pd.read_csv('./Project_materials/data/sample_submission.csv')\n",
    "\n",
    "# Convert dates\n",
    "receivals['date_arrival'] = pd.to_datetime(receivals['date_arrival'], utc=True).dt.tz_localize(None)\n",
    "purchase_orders['delivery_date'] = pd.to_datetime(purchase_orders['delivery_date'], utc=True).dt.tz_localize(None)\n",
    "purchase_orders['created_date_time'] = pd.to_datetime(purchase_orders['created_date_time'], utc=True).dt.tz_localize(None)\n",
    "prediction_mapping['forecast_start_date'] = pd.to_datetime(prediction_mapping['forecast_start_date'])\n",
    "prediction_mapping['forecast_end_date'] = pd.to_datetime(prediction_mapping['forecast_end_date'])\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 8: ADD TIMING FEATURES TO BEST MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# DATA CLEANING\n",
    "# ============================================================================\n",
    "print(\"\\n[1] DATA CLEANING\")\n",
    "receivals = receivals[receivals['net_weight'] > 0]\n",
    "receivals = receivals[receivals['rm_id'].notna()]\n",
    "receivals = receivals.sort_values('date_arrival')\n",
    "print(f\"Clean receivals: {len(receivals)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PRE-COMPUTE TIMING STATISTICS FOR EACH RM_ID\n",
    "# ============================================================================\n",
    "print(\"\\n[2] PRE-COMPUTING TIMING STATISTICS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "timing_stats = {}\n",
    "\n",
    "for rm_id in receivals['rm_id'].unique():\n",
    "    rm_hist = receivals[receivals['rm_id'] == rm_id].sort_values('date_arrival')\n",
    "    \n",
    "    if len(rm_hist) >= 2:\n",
    "        # Calculate inter-arrival intervals\n",
    "        intervals = rm_hist['date_arrival'].diff().dt.days.dropna()\n",
    "        \n",
    "        if len(intervals) > 0:\n",
    "            avg_days_between = intervals.mean()\n",
    "            std_days_between = intervals.std()\n",
    "            cv_days_between = std_days_between / avg_days_between if avg_days_between > 0 else 999\n",
    "        else:\n",
    "            avg_days_between = 0\n",
    "            cv_days_between = 999\n",
    "    else:\n",
    "        # Only 1 or 0 deliveries - can't compute intervals\n",
    "        avg_days_between = 0\n",
    "        cv_days_between = 999\n",
    "    \n",
    "    timing_stats[rm_id] = {\n",
    "        'avg_days_between': avg_days_between,\n",
    "        'cv_days_between': cv_days_between\n",
    "    }\n",
    "\n",
    "print(f\"Computed timing stats for {len(timing_stats)} rm_ids\")\n",
    "print(f\"RM_IDs with computable intervals: {sum(1 for v in timing_stats.values() if v['avg_days_between'] > 0)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING DATA GENERATION (BEST MODEL FEATURES + TIMING FEATURES)\n",
    "# ============================================================================\n",
    "print(\"\\n[3] CREATING TRAINING DATA WITH TIMING FEATURES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "train_dates = pd.date_range(start='2024-01-01', end='2024-11-30', freq='MS')\n",
    "forecast_horizons = [7, 30, 60, 90, 150]\n",
    "\n",
    "print(f\"Using {len(train_dates)} training dates x {len(forecast_horizons)} horizons\")\n",
    "\n",
    "training_data = []\n",
    "active_rm_ids = receivals[receivals['date_arrival'] >= '2024-01-01']['rm_id'].unique()\n",
    "print(f\"Active rm_ids in 2024: {len(active_rm_ids)}\")\n",
    "\n",
    "for i, train_date in enumerate(train_dates):\n",
    "    print(f\"Processing date {i+1}/{len(train_dates)}: {train_date.date()}...\")\n",
    "    \n",
    "    for rm_id in active_rm_ids:\n",
    "        hist = receivals[\n",
    "            (receivals['rm_id'] == rm_id) &\n",
    "            (receivals['date_arrival'] < train_date)\n",
    "        ]\n",
    "        \n",
    "        if len(hist) == 0:\n",
    "            continue\n",
    "        \n",
    "        cutoff_365 = train_date - timedelta(days=365)\n",
    "        cutoff_180 = train_date - timedelta(days=180)\n",
    "        cutoff_90 = train_date - timedelta(days=90)\n",
    "        cutoff_30 = train_date - timedelta(days=30)\n",
    "        \n",
    "        recent_365 = hist[hist['date_arrival'] >= cutoff_365]\n",
    "        recent_180 = hist[hist['date_arrival'] >= cutoff_180]\n",
    "        recent_90 = hist[hist['date_arrival'] >= cutoff_90]\n",
    "        recent_30 = hist[hist['date_arrival'] >= cutoff_30]\n",
    "        \n",
    "        # Basic aggregations (ORIGINAL FEATURES)\n",
    "        if len(recent_365) > 0:\n",
    "            total_365 = recent_365['net_weight'].sum()\n",
    "            count_365 = len(recent_365)\n",
    "            days_since = (train_date - recent_365['date_arrival'].max()).days\n",
    "        else:\n",
    "            total_365 = count_365 = days_since = 0\n",
    "        \n",
    "        if len(recent_180) > 0:\n",
    "            total_180 = recent_180['net_weight'].sum()\n",
    "            count_180 = len(recent_180)\n",
    "        else:\n",
    "            total_180 = count_180 = 0\n",
    "        \n",
    "        if len(recent_90) > 0:\n",
    "            total_90 = recent_90['net_weight'].sum()\n",
    "            count_90 = len(recent_90)\n",
    "        else:\n",
    "            total_90 = count_90 = 0\n",
    "        \n",
    "        if len(recent_30) > 0:\n",
    "            total_30 = recent_30['net_weight'].sum()\n",
    "            count_30 = len(recent_30)\n",
    "            rate_30 = total_30 / 30\n",
    "        else:\n",
    "            total_30 = count_30 = rate_30 = 0\n",
    "        \n",
    "        # Rates\n",
    "        rate_90 = total_90 / 90 if total_90 > 0 else 0\n",
    "        \n",
    "        # Recency-weighted sum\n",
    "        if len(recent_90) > 0:\n",
    "            days_ago = (train_date - recent_90['date_arrival']).dt.days\n",
    "            weights = 1.0 / (days_ago + 1)\n",
    "            recency_weighted = (recent_90['net_weight'] * weights).sum()\n",
    "        else:\n",
    "            recency_weighted = 0\n",
    "        \n",
    "        # Active days ratio\n",
    "        if len(recent_90) > 0:\n",
    "            active_days_90 = recent_90['date_arrival'].dt.date.nunique()\n",
    "            active_ratio_90 = active_days_90 / 90\n",
    "        else:\n",
    "            active_ratio_90 = 0\n",
    "        \n",
    "        # NEW: TIMING FEATURES\n",
    "        # Get pre-computed timing stats\n",
    "        timing = timing_stats.get(rm_id, {'avg_days_between': 0, 'cv_days_between': 999})\n",
    "        avg_days_between = timing['avg_days_between']\n",
    "        cv_days_between = timing['cv_days_between']\n",
    "        \n",
    "        for horizon in forecast_horizons:\n",
    "            forecast_end = train_date + timedelta(days=horizon)\n",
    "            \n",
    "            actual = receivals[\n",
    "                (receivals['rm_id'] == rm_id) &\n",
    "                (receivals['date_arrival'] >= train_date) &\n",
    "                (receivals['date_arrival'] <= forecast_end)\n",
    "            ]\n",
    "            target = actual['net_weight'].sum()\n",
    "            \n",
    "            # NEW: Calculate horizon-dependent timing features\n",
    "            if avg_days_between > 0:\n",
    "                expected_deliveries_in_horizon = horizon / avg_days_between\n",
    "                delivery_overdue_ratio = days_since / avg_days_between if days_since > 0 else 0\n",
    "            else:\n",
    "                expected_deliveries_in_horizon = 0\n",
    "                delivery_overdue_ratio = 999  # No history of intervals\n",
    "            \n",
    "            training_data.append({\n",
    "                # Original features\n",
    "                'rm_id': rm_id,\n",
    "                'train_date': train_date,\n",
    "                'forecast_horizon': horizon,\n",
    "                'total_weight_365d': total_365,\n",
    "                'count_365d': count_365,\n",
    "                'days_since_last': days_since,\n",
    "                'total_weight_90d': total_90,\n",
    "                'count_90d': count_90,\n",
    "                'rate_90': rate_90,\n",
    "                'total_weight_180d': total_180,\n",
    "                'count_180d': count_180,\n",
    "                'total_30': total_30,\n",
    "                'count_30': count_30,\n",
    "                'rate_30': rate_30,\n",
    "                'recency_weighted': recency_weighted,\n",
    "                'active_ratio_90': active_ratio_90,\n",
    "                # NEW: Timing features\n",
    "                'avg_days_between': avg_days_between,\n",
    "                'cv_days_between': cv_days_between,\n",
    "                'expected_deliveries_in_horizon': expected_deliveries_in_horizon,\n",
    "                'delivery_overdue_ratio': delivery_overdue_ratio,\n",
    "                'target': target\n",
    "            })\n",
    "\n",
    "print(f\"\\nGenerated {len(training_data)} training samples\")\n",
    "train_df = pd.DataFrame(training_data)\n",
    "\n",
    "print(f\"Samples with target > 0: {(train_df['target'] > 0).sum()} ({(train_df['target'] > 0).sum() / len(train_df) * 100:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# TIME-BASED TRAIN/VAL SPLIT\n",
    "# ============================================================================\n",
    "print(\"\\n[4] TIME-BASED TRAIN/VAL SPLIT\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "split_date = pd.to_datetime('2024-09-01')\n",
    "\n",
    "train_mask = train_df['train_date'] < split_date\n",
    "val_mask = train_df['train_date'] >= split_date\n",
    "\n",
    "feature_cols = [c for c in train_df.columns if c not in ['target', 'train_date']]\n",
    "\n",
    "X_train = train_df[train_mask][feature_cols]\n",
    "y_train = train_df[train_mask]['target']\n",
    "X_val = train_df[val_mask][feature_cols]\n",
    "y_val = train_df[val_mask]['target']\n",
    "\n",
    "print(f\"Training samples (before {split_date.date()}): {len(X_train)}\")\n",
    "print(f\"Validation samples (>= {split_date.date()}): {len(X_val)}\")\n",
    "\n",
    "train_mean = y_train.mean()\n",
    "val_mean = y_val.mean()\n",
    "print(f\"\\nTarget statistics:\")\n",
    "print(f\"  Training mean: {train_mean:,.0f} kg\")\n",
    "print(f\"  Validation mean: {val_mean:,.0f} kg\")\n",
    "print(f\"  Difference: {((val_mean - train_mean) / train_mean * 100):+.1f}%\")\n",
    "\n",
    "print(f\"\\nNumber of features: {len(feature_cols)}\")\n",
    "print(\"Features:\", feature_cols)\n",
    "\n",
    "# ============================================================================\n",
    "# TRAIN LightGBM MODELS (IDENTICAL TO BEST MODEL)\n",
    "# ============================================================================\n",
    "print(\"\\n[5] TRAINING LightGBM MODELS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "\n",
    "# Classifier\n",
    "y_train_bin = (y_train > 0).astype(int)\n",
    "y_val_bin = (y_val > 0).astype(int)\n",
    "\n",
    "clf = lgb.LGBMClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "print(\"Training LightGBM Classifier...\")\n",
    "clf.fit(\n",
    "    X_train, y_train_bin,\n",
    "    eval_set=[(X_val, y_val_bin)],\n",
    "    callbacks=[lgb.log_evaluation(period=100)]\n",
    ")\n",
    "\n",
    "# Regressor with alpha=0.10 (KEEPING BEST MODEL VALUE)\n",
    "model = lgb.LGBMRegressor(\n",
    "    objective='quantile',\n",
    "    alpha=0.1,\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "print(f\"Training LightGBM Regressor (quantile={model.alpha})...\")\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    callbacks=[lgb.log_evaluation(period=100)]\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# FEATURE IMPORTANCE ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Classifier importance\n",
    "clf_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'clf_importance': clf.feature_importances_,\n",
    "    'clf_pct': clf.feature_importances_ / clf.feature_importances_.sum() * 100\n",
    "}).sort_values('clf_importance', ascending=False)\n",
    "\n",
    "print(\"\\n[A] CLASSIFIER FEATURE IMPORTANCE (Top 15)\")\n",
    "print(\"-\"*80)\n",
    "for idx, row in clf_importance.head(15).iterrows():\n",
    "    print(f\"  {row['feature']:35s}: {row['clf_importance']:8.1f} ({row['clf_pct']:5.2f}%)\")\n",
    "\n",
    "# Regressor importance\n",
    "reg_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'reg_importance': model.feature_importances_,\n",
    "    'reg_pct': model.feature_importances_ / model.feature_importances_.sum() * 100\n",
    "}).sort_values('reg_importance', ascending=False)\n",
    "\n",
    "print(\"\\n[B] REGRESSOR FEATURE IMPORTANCE (Top 15)\")\n",
    "print(\"-\"*80)\n",
    "for idx, row in reg_importance.head(15).iterrows():\n",
    "    print(f\"  {row['feature']:35s}: {row['reg_importance']:8.1f} ({row['reg_pct']:5.2f}%)\")\n",
    "\n",
    "# Combined ranking\n",
    "combined = pd.merge(\n",
    "    clf_importance[['feature', 'clf_pct']],\n",
    "    reg_importance[['feature', 'reg_pct']],\n",
    "    on='feature'\n",
    ")\n",
    "combined['avg_pct'] = (combined['clf_pct'] + combined['reg_pct']) / 2\n",
    "combined = combined.sort_values('avg_pct', ascending=False)\n",
    "\n",
    "print(\"\\n[C] COMBINED IMPORTANCE (Top 15)\")\n",
    "print(\"-\"*80)\n",
    "for idx, row in combined.head(15).iterrows():\n",
    "    print(f\"  {row['feature']:35s}: Clf={row['clf_pct']:5.2f}% | Reg={row['reg_pct']:5.2f}% | Avg={row['avg_pct']:5.2f}%\")\n",
    "\n",
    "# Identify low-importance features\n",
    "low_importance = combined[combined['avg_pct'] < 1.0]\n",
    "print(\"\\n[D] LOW IMPORTANCE FEATURES (<1% avg)\")\n",
    "print(\"-\"*80)\n",
    "if len(low_importance) > 0:\n",
    "    for idx, row in low_importance.iterrows():\n",
    "        print(f\"  {row['feature']:35s}: {row['avg_pct']:5.2f}%\")\n",
    "    print(f\"\\n⚠️  Consider removing {len(low_importance)} features in next iteration\")\n",
    "else:\n",
    "    print(\"  All features have >1% importance ✓\")\n",
    "\n",
    "# Highlight NEW timing features\n",
    "print(\"\\n[E] NEW TIMING FEATURES PERFORMANCE\")\n",
    "print(\"-\"*80)\n",
    "timing_features = ['avg_days_between', 'cv_days_between', 'expected_deliveries_in_horizon', 'delivery_overdue_ratio']\n",
    "for feat in timing_features:\n",
    "    row = combined[combined['feature'] == feat].iloc[0]\n",
    "    print(f\"  {row['feature']:35s}: Clf={row['clf_pct']:5.2f}% | Reg={row['reg_pct']:5.2f}% | Avg={row['avg_pct']:5.2f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# VALIDATION PERFORMANCE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VALIDATION PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Make predictions on validation set\n",
    "val_reg_pred = np.maximum(0, model.predict(X_val))\n",
    "val_clf_prob = clf.predict_proba(X_val)[:, 1]\n",
    "val_combined_pred = val_reg_pred * val_clf_prob\n",
    "\n",
    "# Calculate quantile loss\n",
    "def quantile_loss_0_2(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    return np.where(error >= 0, 0.2 * error, -0.8 * error)\n",
    "\n",
    "quantile_loss = quantile_loss_0_2(y_val, val_combined_pred)\n",
    "mean_quantile_loss = quantile_loss.mean()\n",
    "\n",
    "print(f\"Mean Quantile Loss (0.2): {float(mean_quantile_loss):,.2f}\")\n",
    "print(f\"Median Quantile Loss: {float(np.median(quantile_loss)):,.2f}\")\n",
    "print(f\"Mean Absolute Error: {float(np.abs(y_val - val_combined_pred).mean()):,.2f}\")\n",
    "print(f\"\\nMean Target: {float(y_val.mean()):,.2f}\")\n",
    "print(f\"Mean Prediction: {float(val_combined_pred.mean()):,.2f}\")\n",
    "bias_pct = float((val_combined_pred.mean() - y_val.mean()) / y_val.mean() * 100)\n",
    "print(f\"Bias: {bias_pct:+.1f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# MAKE PREDICTIONS FOR 2025\n",
    "# ============================================================================\n",
    "print(\"\\n[6] MAKING PREDICTIONS FOR 2025\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "forecast_start = pd.to_datetime('2025-01-01')\n",
    "\n",
    "# Pre-compute features for all rm_ids (INCLUDING TIMING FEATURES)\n",
    "rm_features = {}\n",
    "\n",
    "for rm_id in prediction_mapping['rm_id'].unique():\n",
    "    hist = receivals[\n",
    "        (receivals['rm_id'] == rm_id) &\n",
    "        (receivals['date_arrival'] < forecast_start)\n",
    "    ]\n",
    "    \n",
    "    if len(hist) == 0:\n",
    "        rm_features[rm_id] = {\n",
    "            'total_weight_365d': 0,\n",
    "            'count_365d': 0,\n",
    "            'days_since_last': 999,\n",
    "            'total_weight_90d': 0,\n",
    "            'count_90d': 0,\n",
    "            'rate_90': 0,\n",
    "            'total_weight_180d': 0,\n",
    "            'count_180d': 0,\n",
    "            'total_30': 0,\n",
    "            'count_30': 0,\n",
    "            'rate_30': 0,\n",
    "            'recency_weighted': 0,\n",
    "            'active_ratio_90': 0,\n",
    "            'avg_days_between': 0,\n",
    "            'cv_days_between': 999,\n",
    "        }\n",
    "        continue\n",
    "    \n",
    "    cutoff_365 = forecast_start - timedelta(days=365)\n",
    "    cutoff_180 = forecast_start - timedelta(days=180)\n",
    "    cutoff_90 = forecast_start - timedelta(days=90)\n",
    "    cutoff_30 = forecast_start - timedelta(days=30)\n",
    "    \n",
    "    recent_365 = hist[hist['date_arrival'] >= cutoff_365]\n",
    "    recent_180 = hist[hist['date_arrival'] >= cutoff_180]\n",
    "    recent_90 = hist[hist['date_arrival'] >= cutoff_90]\n",
    "    recent_30 = hist[hist['date_arrival'] >= cutoff_30]\n",
    "    \n",
    "    if len(recent_365) > 0:\n",
    "        total_365 = recent_365['net_weight'].sum()\n",
    "        count_365 = len(recent_365)\n",
    "        days_since = (forecast_start - recent_365['date_arrival'].max()).days\n",
    "    else:\n",
    "        total_365 = count_365 = days_since = 0\n",
    "    \n",
    "    if len(recent_180) > 0:\n",
    "        total_180 = recent_180['net_weight'].sum()\n",
    "        count_180 = len(recent_180)\n",
    "    else:\n",
    "        total_180 = count_180 = 0\n",
    "    \n",
    "    if len(recent_90) > 0:\n",
    "        total_90 = recent_90['net_weight'].sum()\n",
    "        count_90 = len(recent_90)\n",
    "        rate_90 = total_90 / 90\n",
    "    else:\n",
    "        total_90 = count_90 = rate_90 = 0\n",
    "    \n",
    "    if len(recent_30) > 0:\n",
    "        total_30 = recent_30['net_weight'].sum()\n",
    "        count_30 = len(recent_30)\n",
    "        rate_30 = total_30 / 30\n",
    "    else:\n",
    "        total_30 = count_30 = rate_30 = 0\n",
    "    \n",
    "    # Recency-weighted\n",
    "    if len(recent_90) > 0:\n",
    "        days_ago = (forecast_start - recent_90['date_arrival']).dt.days\n",
    "        weights = 1.0 / (days_ago + 1)\n",
    "        recency_weighted = (recent_90['net_weight'] * weights).sum()\n",
    "    else:\n",
    "        recency_weighted = 0\n",
    "    \n",
    "    # Active ratio\n",
    "    if len(recent_90) > 0:\n",
    "        active_days_90 = recent_90['date_arrival'].dt.date.nunique()\n",
    "        active_ratio_90 = active_days_90 / 90\n",
    "    else:\n",
    "        active_ratio_90 = 0\n",
    "    \n",
    "    # NEW: Get timing stats\n",
    "    timing = timing_stats.get(rm_id, {'avg_days_between': 0, 'cv_days_between': 999})\n",
    "    \n",
    "    rm_features[rm_id] = {\n",
    "        'total_weight_365d': total_365,\n",
    "        'count_365d': count_365,\n",
    "        'days_since_last': days_since,\n",
    "        'total_weight_90d': total_90,\n",
    "        'count_90d': count_90,\n",
    "        'rate_90': rate_90,\n",
    "        'total_weight_180d': total_180,\n",
    "        'count_180d': count_180,\n",
    "        'total_30': total_30,\n",
    "        'count_30': count_30,\n",
    "        'rate_30': rate_30,\n",
    "        'recency_weighted': recency_weighted,\n",
    "        'active_ratio_90': active_ratio_90,\n",
    "        'avg_days_between': timing['avg_days_between'],\n",
    "        'cv_days_between': timing['cv_days_between'],\n",
    "    }\n",
    "\n",
    "print(f\"Pre-computed features for {len(rm_features)} rm_ids\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = []\n",
    "\n",
    "for idx, row in prediction_mapping.iterrows():\n",
    "    rm_id = row['rm_id']\n",
    "    forecast_end = row['forecast_end_date']\n",
    "    horizon = (forecast_end - forecast_start).days + 1\n",
    "    \n",
    "    feat = rm_features[rm_id]\n",
    "    \n",
    "    # Calculate horizon-dependent timing features\n",
    "    avg_days_between = feat['avg_days_between']\n",
    "    days_since = feat['days_since_last']\n",
    "    \n",
    "    if avg_days_between > 0:\n",
    "        expected_deliveries_in_horizon = horizon / avg_days_between\n",
    "        delivery_overdue_ratio = days_since / avg_days_between if days_since > 0 else 0\n",
    "    else:\n",
    "        expected_deliveries_in_horizon = 0\n",
    "        delivery_overdue_ratio = 999\n",
    "    \n",
    "    feature_dict = {\n",
    "        'rm_id': rm_id,\n",
    "        'forecast_horizon': horizon,\n",
    "        'total_weight_365d': feat['total_weight_365d'],\n",
    "        'count_365d': feat['count_365d'],\n",
    "        'days_since_last': feat['days_since_last'],\n",
    "        'total_weight_90d': feat['total_weight_90d'],\n",
    "        'count_90d': feat['count_90d'],\n",
    "        'rate_90': feat['rate_90'],\n",
    "        'total_weight_180d': feat['total_weight_180d'],\n",
    "        'count_180d': feat['count_180d'],\n",
    "        'total_30': feat['total_30'],\n",
    "        'count_30': feat['count_30'],\n",
    "        'rate_30': feat['rate_30'],\n",
    "        'recency_weighted': feat['recency_weighted'],\n",
    "        'active_ratio_90': feat['active_ratio_90'],\n",
    "        'avg_days_between': feat['avg_days_between'],\n",
    "        'cv_days_between': feat['cv_days_between'],\n",
    "        'expected_deliveries_in_horizon': expected_deliveries_in_horizon,\n",
    "        'delivery_overdue_ratio': delivery_overdue_ratio,\n",
    "    }\n",
    "    \n",
    "    feature_vector = pd.DataFrame([feature_dict])[feature_cols]\n",
    "    \n",
    "    # Two-stage prediction\n",
    "    reg_pred = max(0, model.predict(feature_vector)[0])\n",
    "    prob_pos = clf.predict_proba(feature_vector)[:, 1][0]\n",
    "    pred = reg_pred * prob_pos\n",
    "    \n",
    "    # Guardrails (ORIGINAL FROM BEST MODEL)\n",
    "    days_inactive = feat['days_since_last']\n",
    "    total_365 = feat['total_weight_365d']\n",
    "    cap_upper = (total_365 / 365.0) * horizon * 1.5\n",
    "    \n",
    "    if days_inactive > 365:\n",
    "        pred = 0.0\n",
    "    elif 180 < days_inactive <= 365:\n",
    "        cold_cap = 0.08 * total_365\n",
    "        pred = min(pred, cold_cap)\n",
    "    \n",
    "    pred = max(0.0, min(pred, cap_upper))\n",
    "    \n",
    "    predictions.append({'ID': row['ID'], 'predicted_weight': pred})\n",
    "    \n",
    "    if (idx + 1) % 5000 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(prediction_mapping)}...\")\n",
    "\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "\n",
    "print(\"\\n[7] PREDICTION STATISTICS\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Predictions mean: {predictions_df['predicted_weight'].mean():,.0f} kg\")\n",
    "print(\"\\nPrediction statistics:\")\n",
    "print(predictions_df['predicted_weight'].describe())\n",
    "print(f\"Predictions > 0: {(predictions_df['predicted_weight'] > 0).sum()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE SUBMISSION\n",
    "# ============================================================================\n",
    "print(\"\\n[8] CREATING SUBMISSION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "submission = sample_submission.copy()\n",
    "submission['predicted_weight'] = predictions_df['predicted_weight'].values\n",
    "submission.to_csv('STEP8_TIMING_FEATURES.csv', index=False)\n",
    "print(\"✅ Saved to 'STEP8_TIMING_FEATURES.csv'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPLETE - STEP 8: TIMING FEATURES ADDED\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nChanges from BEST MODEL (6236):\")\n",
    "print(\"  ✅ Added 4 timing features:\")\n",
    "print(\"     - avg_days_between\")\n",
    "print(\"     - cv_days_between\")\n",
    "print(\"     - expected_deliveries_in_horizon\")\n",
    "print(\"     - delivery_overdue_ratio\")\n",
    "print(\"  ✅ Alpha=0.1 (same as best model)\")\n",
    "print(\"  ✅ Kept original guardrails (same as best model)\")\n",
    "print(\"  ✅ Kept two-stage architecture (same as best model)\")\n",
    "print(\"  ✅ Total features: 19 (was 15)\")\n",
    "print(\"\\n  Expected: +10 to +30 points from timing-aware features\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
