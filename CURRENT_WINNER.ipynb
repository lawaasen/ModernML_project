{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8d4fc8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib' has no attribute 'get_data_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlgb\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m timedelta\n\u001b[0;32m      6\u001b[0m receivals \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Project_materials/data/kernel/receivals.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\__init__.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# .basic is intentionally loaded as early as possible, to dlopen() lib_lightgbm.{dll,dylib,so}\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# and its dependencies as early as possible\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbasic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Booster, Dataset, Sequence, register_logger\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EarlyStopException, early_stopping, log_evaluation, record_evaluation, reset_parameter\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CVBooster, cv, train\n",
      "File \u001b[1;32mc:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\basic.py:29\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     30\u001b[0m     CFFI_INSTALLED,\n\u001b[0;32m     31\u001b[0m     PANDAS_INSTALLED,\n\u001b[0;32m     32\u001b[0m     PYARROW_INSTALLED,\n\u001b[0;32m     33\u001b[0m     arrow_cffi,\n\u001b[0;32m     34\u001b[0m     arrow_is_boolean,\n\u001b[0;32m     35\u001b[0m     arrow_is_floating,\n\u001b[0;32m     36\u001b[0m     arrow_is_integer,\n\u001b[0;32m     37\u001b[0m     concat,\n\u001b[0;32m     38\u001b[0m     dt_DataTable,\n\u001b[0;32m     39\u001b[0m     pa_Array,\n\u001b[0;32m     40\u001b[0m     pa_chunked_array,\n\u001b[0;32m     41\u001b[0m     pa_ChunkedArray,\n\u001b[0;32m     42\u001b[0m     pa_compute,\n\u001b[0;32m     43\u001b[0m     pa_Table,\n\u001b[0;32m     44\u001b[0m     pd_CategoricalDtype,\n\u001b[0;32m     45\u001b[0m     pd_DataFrame,\n\u001b[0;32m     46\u001b[0m     pd_Series,\n\u001b[0;32m     47\u001b[0m )\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n",
      "File \u001b[1;32mc:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\compat.py:191\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03m\"\"\"matplotlib\"\"\"\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 191\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m    193\u001b[0m     MATPLOTLIB_INSTALLED \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\__init__.py:977\u001b[0m\n\u001b[0;32m    970\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# When constructing the global instances, we need to perform certain updates\u001b[39;00m\n\u001b[0;32m    974\u001b[0m \u001b[38;5;66;03m# by explicitly calling the superclass (dict.update, dict.items) to avoid\u001b[39;00m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;66;03m# triggering resolution of _auto_backend_sentinel.\u001b[39;00m\n\u001b[0;32m    976\u001b[0m rcParamsDefault \u001b[38;5;241m=\u001b[39m _rc_params_in_file(\n\u001b[1;32m--> 977\u001b[0m     \u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmatplotlibrc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    978\u001b[0m     \u001b[38;5;66;03m# Strip leading comment.\u001b[39;00m\n\u001b[0;32m    979\u001b[0m     transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m line: line[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m line,\n\u001b[0;32m    980\u001b[0m     fail_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(rcParamsDefault, rcsetup\u001b[38;5;241m.\u001b[39m_hardcoded_defaults)\n\u001b[0;32m    982\u001b[0m \u001b[38;5;66;03m# Normally, the default matplotlibrc file contains *no* entry for backend (the\u001b[39;00m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;66;03m# corresponding line starts with ##, not #; we fill on _auto_backend_sentinel\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# in that case.  However, packagers can set a different default backend\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# (resulting in a normal `#backend: foo` line) in which case we should *not*\u001b[39;00m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;66;03m# fill in _auto_backend_sentinel.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\cbook.py:545\u001b[0m, in \u001b[0;36m_get_data_path\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_data_path\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m    540\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;124;03m    Return the `pathlib.Path` to a resource file provided by Matplotlib.\u001b[39;00m\n\u001b[0;32m    542\u001b[0m \n\u001b[0;32m    543\u001b[0m \u001b[38;5;124;03m    ``*args`` specify a path relative to the base data path.\u001b[39;00m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Path(\u001b[43mmatplotlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_path\u001b[49m(), \u001b[38;5;241m*\u001b[39margs)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'matplotlib' has no attribute 'get_data_path'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from datetime import timedelta\n",
    "\n",
    "receivals = pd.read_csv('./Project_materials/data/kernel/receivals.csv')\n",
    "purchase_orders = pd.read_csv('./Project_materials/data/kernel/purchase_orders.csv')\n",
    "prediction_mapping = pd.read_csv('./Project_materials/data/prediction_mapping.csv')\n",
    "sample_submission = pd.read_csv('./Project_materials/data/sample_submission.csv')\n",
    "\n",
    "# Convert dates\n",
    "receivals['date_arrival'] = pd.to_datetime(receivals['date_arrival'], utc=True).dt.tz_localize(None)\n",
    "purchase_orders['delivery_date'] = pd.to_datetime(purchase_orders['delivery_date'], utc=True).dt.tz_localize(None)\n",
    "purchase_orders['created_date_time'] = pd.to_datetime(purchase_orders['created_date_time'], utc=True).dt.tz_localize(None)\n",
    "prediction_mapping['forecast_start_date'] = pd.to_datetime(prediction_mapping['forecast_start_date'])\n",
    "prediction_mapping['forecast_end_date'] = pd.to_datetime(prediction_mapping['forecast_end_date'])\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LightGBM STEP 5: REMOVE CALIBRATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# DATA CLEANING\n",
    "# ============================================================================\n",
    "print(\"\\n[1] DATA CLEANING\")\n",
    "receivals = receivals[receivals['net_weight'] > 0]\n",
    "receivals = receivals[receivals['rm_id'].notna()]\n",
    "receivals = receivals.sort_values('date_arrival')\n",
    "print(f\"Clean receivals: {len(receivals)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING DATA GENERATION (STEP 1 FEATURES)\n",
    "# ============================================================================\n",
    "print(\"\\n[2] CREATING TRAINING DATA WITH STEP 1 FEATURES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "train_dates = pd.date_range(start='2024-01-01', end='2024-11-30', freq='MS')\n",
    "forecast_horizons = [7, 30, 60, 90, 150]\n",
    "\n",
    "print(f\"Using {len(train_dates)} training dates x {len(forecast_horizons)} horizons\")\n",
    "\n",
    "training_data = []\n",
    "active_rm_ids = receivals[receivals['date_arrival'] >= '2024-01-01']['rm_id'].unique()\n",
    "print(f\"Active rm_ids in 2024: {len(active_rm_ids)}\")\n",
    "\n",
    "for i, train_date in enumerate(train_dates):\n",
    "    print(f\"Processing date {i+1}/{len(train_dates)}: {train_date.date()}...\")\n",
    "    \n",
    "    for rm_id in active_rm_ids:\n",
    "        hist = receivals[\n",
    "            (receivals['rm_id'] == rm_id) &\n",
    "            (receivals['date_arrival'] < train_date)\n",
    "        ]\n",
    "        \n",
    "        if len(hist) == 0:\n",
    "            continue\n",
    "        \n",
    "        cutoff_365 = train_date - timedelta(days=365)\n",
    "        cutoff_180 = train_date - timedelta(days=180)\n",
    "        cutoff_90 = train_date - timedelta(days=90)\n",
    "        cutoff_30 = train_date - timedelta(days=30)\n",
    "        \n",
    "        recent_365 = hist[hist['date_arrival'] >= cutoff_365]\n",
    "        recent_180 = hist[hist['date_arrival'] >= cutoff_180]\n",
    "        recent_90 = hist[hist['date_arrival'] >= cutoff_90]\n",
    "        recent_30 = hist[hist['date_arrival'] >= cutoff_30]\n",
    "        \n",
    "        # Basic aggregations\n",
    "        if len(recent_365) > 0:\n",
    "            total_365 = recent_365['net_weight'].sum()\n",
    "            count_365 = len(recent_365)\n",
    "            days_since = (train_date - recent_365['date_arrival'].max()).days\n",
    "        else:\n",
    "            total_365 = count_365 = days_since = 0\n",
    "        \n",
    "        if len(recent_180) > 0:\n",
    "            total_180 = recent_180['net_weight'].sum()\n",
    "            count_180 = len(recent_180)\n",
    "        else:\n",
    "            total_180 = count_180 = 0\n",
    "        \n",
    "        if len(recent_90) > 0:\n",
    "            total_90 = recent_90['net_weight'].sum()\n",
    "            count_90 = len(recent_90)\n",
    "        else:\n",
    "            total_90 = count_90 = 0\n",
    "        \n",
    "        if len(recent_30) > 0:\n",
    "            total_30 = recent_30['net_weight'].sum()\n",
    "            count_30 = len(recent_30)\n",
    "            rate_30 = total_30 / 30\n",
    "        else:\n",
    "            total_30 = count_30 = rate_30 = 0\n",
    "        \n",
    "        # Rates\n",
    "        rate_90 = total_90 / 90 if total_90 > 0 else 0\n",
    "        \n",
    "        # Recency-weighted sum\n",
    "        if len(recent_90) > 0:\n",
    "            days_ago = (train_date - recent_90['date_arrival']).dt.days\n",
    "            weights = 1.0 / (days_ago + 1)\n",
    "            recency_weighted = (recent_90['net_weight'] * weights).sum()\n",
    "        else:\n",
    "            recency_weighted = 0\n",
    "        \n",
    "        # Active days ratio\n",
    "        if len(recent_90) > 0:\n",
    "            active_days_90 = recent_90['date_arrival'].dt.date.nunique()\n",
    "            active_ratio_90 = active_days_90 / 90\n",
    "        else:\n",
    "            active_ratio_90 = 0\n",
    "        \n",
    "        for horizon in forecast_horizons:\n",
    "            forecast_end = train_date + timedelta(days=horizon)\n",
    "            \n",
    "            actual = receivals[\n",
    "                (receivals['rm_id'] == rm_id) &\n",
    "                (receivals['date_arrival'] >= train_date) &\n",
    "                (receivals['date_arrival'] <= forecast_end)\n",
    "            ]\n",
    "            target = actual['net_weight'].sum()\n",
    "            \n",
    "            training_data.append({\n",
    "                'rm_id': rm_id,\n",
    "                'train_date': train_date,\n",
    "                'forecast_horizon': horizon,\n",
    "                'total_weight_365d': total_365,\n",
    "                'count_365d': count_365,\n",
    "                'days_since_last': days_since,\n",
    "                'total_weight_90d': total_90,\n",
    "                'count_90d': count_90,\n",
    "                'rate_90': rate_90,\n",
    "                'total_weight_180d': total_180,\n",
    "                'count_180d': count_180,\n",
    "                'total_30': total_30,\n",
    "                'count_30': count_30,\n",
    "                'rate_30': rate_30,\n",
    "                'recency_weighted': recency_weighted,\n",
    "                'active_ratio_90': active_ratio_90,\n",
    "                'target': target\n",
    "            })\n",
    "\n",
    "print(f\"\\nGenerated {len(training_data)} training samples\")\n",
    "train_df = pd.DataFrame(training_data)\n",
    "\n",
    "print(f\"Samples with target > 0: {(train_df['target'] > 0).sum()} ({(train_df['target'] > 0).sum() / len(train_df) * 100:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# TIME-BASED TRAIN/VAL SPLIT\n",
    "# ============================================================================\n",
    "print(\"\\n[3] TIME-BASED TRAIN/VAL SPLIT\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "split_date = pd.to_datetime('2024-09-01')\n",
    "\n",
    "train_mask = train_df['train_date'] < split_date\n",
    "val_mask = train_df['train_date'] >= split_date\n",
    "\n",
    "feature_cols = [c for c in train_df.columns if c not in ['target', 'train_date']]\n",
    "\n",
    "X_train = train_df[train_mask][feature_cols]\n",
    "y_train = train_df[train_mask]['target']\n",
    "X_val = train_df[val_mask][feature_cols]\n",
    "y_val = train_df[val_mask]['target']\n",
    "\n",
    "print(f\"Training samples (before {split_date.date()}): {len(X_train)}\")\n",
    "print(f\"Validation samples (>= {split_date.date()}): {len(X_val)}\")\n",
    "\n",
    "train_mean = y_train.mean()\n",
    "val_mean = y_val.mean()\n",
    "print(f\"\\nTarget statistics:\")\n",
    "print(f\"  Training mean: {train_mean:,.0f} kg\")\n",
    "print(f\"  Validation mean: {val_mean:,.0f} kg\")\n",
    "print(f\"  Difference: {((val_mean - train_mean) / train_mean * 100):+.1f}%\")\n",
    "\n",
    "print(f\"\\nNumber of features: {len(feature_cols)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# TRAIN LightGBM MODELS (TWO-STAGE WITH ALPHA=0.10)\n",
    "# ============================================================================\n",
    "print(\"\\n[4] TRAINING LightGBM MODELS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "\n",
    "# Classifier\n",
    "y_train_bin = (y_train > 0).astype(int)\n",
    "y_val_bin = (y_val > 0).astype(int)\n",
    "\n",
    "clf = lgb.LGBMClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "print(\"Training LightGBM Classifier...\")\n",
    "clf.fit(\n",
    "    X_train, y_train_bin,\n",
    "    eval_set=[(X_val, y_val_bin)],\n",
    "    callbacks=[lgb.log_evaluation(period=100)]\n",
    ")\n",
    "\n",
    "# Regressor with alpha=0.10\n",
    "model = lgb.LGBMRegressor(\n",
    "    objective='quantile',\n",
    "    alpha=0.10,\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "print(f\"Training LightGBM Regressor (quantile={model.alpha})...\")\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    callbacks=[lgb.log_evaluation(period=100)]\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# NO CALIBRATION (REMOVED!)\n",
    "# ============================================================================\n",
    "print(\"\\n[4c] CALIBRATION: REMOVED\")\n",
    "print(\"-\"*80)\n",
    "print(\"⚠️  Skipping per-horizon calibration entirely\")\n",
    "print(\"   Let the model's forecast_horizon feature handle horizon effects naturally\")\n",
    "print(\"   Hypothesis: Sep-Nov calibration doesn't generalize to 2025\")\n",
    "\n",
    "# ============================================================================\n",
    "# MAKE PREDICTIONS\n",
    "# ============================================================================\n",
    "print(\"\\n[5] MAKING PREDICTIONS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "forecast_start = pd.to_datetime('2025-01-01')\n",
    "\n",
    "# Pre-compute features for all rm_ids\n",
    "rm_features = {}\n",
    "\n",
    "for rm_id in prediction_mapping['rm_id'].unique():\n",
    "    hist = receivals[\n",
    "        (receivals['rm_id'] == rm_id) &\n",
    "        (receivals['date_arrival'] < forecast_start)\n",
    "    ]\n",
    "    \n",
    "    if len(hist) == 0:\n",
    "        rm_features[rm_id] = {\n",
    "            'total_weight_365d': 0,\n",
    "            'count_365d': 0,\n",
    "            'days_since_last': 999,\n",
    "            'total_weight_90d': 0,\n",
    "            'count_90d': 0,\n",
    "            'rate_90': 0,\n",
    "            'total_weight_180d': 0,\n",
    "            'count_180d': 0,\n",
    "            'total_30': 0,\n",
    "            'count_30': 0,\n",
    "            'rate_30': 0,\n",
    "            'recency_weighted': 0,\n",
    "            'active_ratio_90': 0\n",
    "        }\n",
    "        continue\n",
    "    \n",
    "    cutoff_365 = forecast_start - timedelta(days=365)\n",
    "    cutoff_180 = forecast_start - timedelta(days=180)\n",
    "    cutoff_90 = forecast_start - timedelta(days=90)\n",
    "    cutoff_30 = forecast_start - timedelta(days=30)\n",
    "    \n",
    "    recent_365 = hist[hist['date_arrival'] >= cutoff_365]\n",
    "    recent_180 = hist[hist['date_arrival'] >= cutoff_180]\n",
    "    recent_90 = hist[hist['date_arrival'] >= cutoff_90]\n",
    "    recent_30 = hist[hist['date_arrival'] >= cutoff_30]\n",
    "    \n",
    "    if len(recent_365) > 0:\n",
    "        total_365 = recent_365['net_weight'].sum()\n",
    "        count_365 = len(recent_365)\n",
    "        days_since = (forecast_start - recent_365['date_arrival'].max()).days\n",
    "    else:\n",
    "        total_365 = count_365 = days_since = 0\n",
    "    \n",
    "    if len(recent_180) > 0:\n",
    "        total_180 = recent_180['net_weight'].sum()\n",
    "        count_180 = len(recent_180)\n",
    "    else:\n",
    "        total_180 = count_180 = 0\n",
    "    \n",
    "    if len(recent_90) > 0:\n",
    "        total_90 = recent_90['net_weight'].sum()\n",
    "        count_90 = len(recent_90)\n",
    "        rate_90 = total_90 / 90\n",
    "    else:\n",
    "        total_90 = count_90 = rate_90 = 0\n",
    "    \n",
    "    if len(recent_30) > 0:\n",
    "        total_30 = recent_30['net_weight'].sum()\n",
    "        count_30 = len(recent_30)\n",
    "        rate_30 = total_30 / 30\n",
    "    else:\n",
    "        total_30 = count_30 = rate_30 = 0\n",
    "    \n",
    "    # Recency-weighted\n",
    "    if len(recent_90) > 0:\n",
    "        days_ago = (forecast_start - recent_90['date_arrival']).dt.days\n",
    "        weights = 1.0 / (days_ago + 1)\n",
    "        recency_weighted = (recent_90['net_weight'] * weights).sum()\n",
    "    else:\n",
    "        recency_weighted = 0\n",
    "    \n",
    "    # Active ratio\n",
    "    if len(recent_90) > 0:\n",
    "        active_days_90 = recent_90['date_arrival'].dt.date.nunique()\n",
    "        active_ratio_90 = active_days_90 / 90\n",
    "    else:\n",
    "        active_ratio_90 = 0\n",
    "    \n",
    "    rm_features[rm_id] = {\n",
    "        'total_weight_365d': total_365,\n",
    "        'count_365d': count_365,\n",
    "        'days_since_last': days_since,\n",
    "        'total_weight_90d': total_90,\n",
    "        'count_90d': count_90,\n",
    "        'rate_90': rate_90,\n",
    "        'total_weight_180d': total_180,\n",
    "        'count_180d': count_180,\n",
    "        'total_30': total_30,\n",
    "        'count_30': count_30,\n",
    "        'rate_30': rate_30,\n",
    "        'recency_weighted': recency_weighted,\n",
    "        'active_ratio_90': active_ratio_90\n",
    "    }\n",
    "\n",
    "print(f\"Pre-computed features for {len(rm_features)} rm_ids\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = []\n",
    "\n",
    "for idx, row in prediction_mapping.iterrows():\n",
    "    rm_id = row['rm_id']\n",
    "    forecast_end = row['forecast_end_date']\n",
    "    horizon = (forecast_end - forecast_start).days + 1\n",
    "    \n",
    "    feat = rm_features[rm_id]\n",
    "    \n",
    "    feature_dict = {\n",
    "        'rm_id': rm_id,\n",
    "        'forecast_horizon': horizon,\n",
    "        'total_weight_365d': feat['total_weight_365d'],\n",
    "        'count_365d': feat['count_365d'],\n",
    "        'days_since_last': feat['days_since_last'],\n",
    "        'total_weight_90d': feat['total_weight_90d'],\n",
    "        'count_90d': feat['count_90d'],\n",
    "        'rate_90': feat['rate_90'],\n",
    "        'total_weight_180d': feat['total_weight_180d'],\n",
    "        'count_180d': feat['count_180d'],\n",
    "        'total_30': feat['total_30'],\n",
    "        'count_30': feat['count_30'],\n",
    "        'rate_30': feat['rate_30'],\n",
    "        'recency_weighted': feat['recency_weighted'],\n",
    "        'active_ratio_90': feat['active_ratio_90']\n",
    "    }\n",
    "    \n",
    "    feature_vector = pd.DataFrame([feature_dict])[feature_cols]\n",
    "    \n",
    "    # Two-stage prediction\n",
    "    reg_pred = max(0, model.predict(feature_vector)[0])\n",
    "    prob_pos = clf.predict_proba(feature_vector)[:, 1][0]\n",
    "    pred = reg_pred * prob_pos\n",
    "    \n",
    "    # ========================================================================\n",
    "    # NO CALIBRATION - REMOVED THIS LINE:\n",
    "    # pred *= calibration.get(int(horizon), 1.0)\n",
    "    # ========================================================================\n",
    "    \n",
    "    # Guardrails (original Step 1 version)\n",
    "    days_inactive = feat['days_since_last']\n",
    "    total_365 = feat['total_weight_365d']\n",
    "    cap_upper = (total_365 / 365.0) * horizon * 1.5\n",
    "    \n",
    "    if days_inactive > 365:\n",
    "        pred = 0.0\n",
    "    elif 180 < days_inactive <= 365:\n",
    "        cold_cap = 0.08 * total_365\n",
    "        pred = min(pred, cold_cap)\n",
    "    \n",
    "    pred = max(0.0, min(pred, cap_upper))\n",
    "    \n",
    "    predictions.append({'ID': row['ID'], 'predicted_weight': pred})\n",
    "    \n",
    "    if (idx + 1) % 5000 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(prediction_mapping)}...\")\n",
    "\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "\n",
    "print(\"\\n[6] PREDICTION STATISTICS\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Predictions mean: {predictions_df['predicted_weight'].mean():,.0f} kg\")\n",
    "print(\"\\nPrediction statistics:\")\n",
    "print(predictions_df['predicted_weight'].describe())\n",
    "print(f\"Predictions > 0: {(predictions_df['predicted_weight'] > 0).sum()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE SUBMISSION\n",
    "# ============================================================================\n",
    "print(\"\\n[7] CREATING SUBMISSION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "submission = sample_submission.copy()\n",
    "submission['predicted_weight'] = predictions_df['predicted_weight'].values\n",
    "submission.to_csv('lightgbm_step5_no_calibration.csv', index=False)\n",
    "print(\"Saved to 'lightgbm_step5_no_calibration.csv'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPLETE - STEP 5: NO CALIBRATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nChanges from Step 4 (6,140):\")\n",
    "print(\"  ✅ Kept all Step 1 features and two-stage model\")\n",
    "print(\"  ✅ Kept alpha=0.10 from Step 4\")\n",
    "print(\"  ✅ Kept original guardrails from Step 1\")\n",
    "print(\"  ❌ REMOVED: Per-horizon calibration entirely\")\n",
    "print(\"  ✅ Hypothesis: Calibration overfits to Sep-Nov 2024, hurts 2025 generalization\")\n",
    "print(\"  ✅ Let forecast_horizon feature handle horizon effects naturally\")\n",
    "print(\"  ✅ Expected: Unknown direction, but could improve if calibration was harmful\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
