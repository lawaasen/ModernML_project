{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c0c4552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EMERGENCY DIAGNOSTIC: WHY DID TEST SCORE BLOW UP?\n",
      "================================================================================\n",
      "\n",
      "[1] LOADING DATA\n",
      "Receivals: 122383\n",
      "Prediction mapping: 30450\n",
      "Submission: 30450\n",
      "Training data: 2725\n",
      "\n",
      "================================================================================\n",
      "CHECK 1: PREDICTION MAPPING STRUCTURE\n",
      "================================================================================\n",
      "\n",
      "Prediction mapping columns:\n",
      "['ID', 'rm_id', 'forecast_start_date', 'forecast_end_date']\n",
      "\n",
      "First 10 rows:\n",
      "   ID  rm_id forecast_start_date forecast_end_date\n",
      "0   1    365          2025-01-01        2025-01-02\n",
      "1   2    365          2025-01-01        2025-01-03\n",
      "2   3    365          2025-01-01        2025-01-04\n",
      "3   4    365          2025-01-01        2025-01-05\n",
      "4   5    365          2025-01-01        2025-01-06\n",
      "5   6    365          2025-01-01        2025-01-07\n",
      "6   7    365          2025-01-01        2025-01-08\n",
      "7   8    365          2025-01-01        2025-01-09\n",
      "8   9    365          2025-01-01        2025-01-10\n",
      "9  10    365          2025-01-01        2025-01-11\n",
      "\n",
      "Unique RM_IDs in prediction mapping:\n",
      "Count: 203\n",
      "RM_IDs: [342, 343, 345, 346, 347, 348, 353, 354, 355, 357, 358, 360, 362, 364, 365, 366, 367, 368, 369, 374, 375, 378, 379, 380, 381, 383, 386, 387, 388, 389, 390, 1842, 1843, 1844, 1845, 1846, 1850, 1851, 1852, 1853, 1854, 1857, 1858, 1866, 1867, 1868, 1871, 1872, 1873, 1874, 1875, 1876, 1882, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1981, 1982, 2001, 2061, 2102, 2121, 2122, 2123, 2124, 2125, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2182, 2201, 2222, 2223, 2261, 2282, 2283, 2284, 2285, 2302, 2304, 2322, 2323, 2341, 2343, 2344, 2345, 2347, 2348, 2362, 2363, 2364, 2365, 2401, 2402, 2421, 2441, 2481, 2482, 2521, 2561, 2601, 2741, 2742, 2761, 2821, 2841, 2861, 2981, 3005, 3022, 3101, 3121, 3122, 3123, 3124, 3125, 3126, 3142, 3144, 3161, 3162, 3201, 3222, 3241, 3265, 3282, 3362, 3381, 3421, 3441, 3461, 3481, 3501, 3541, 3581, 3601, 3621, 3642, 3701, 3761, 3762, 3781, 3802, 3821, 3841, 3865, 3883, 3901, 3921, 3941, 4021, 4044, 4081, 4101, 4161, 4222, 4263, 4302, 4343, 4381, 4401, 4441, 4443, 4461, 4462, 4463, 4481, 4501]\n",
      "\n",
      "================================================================================\n",
      "CHECK 2: HORIZON DISTRIBUTION (CRITICAL!)\n",
      "================================================================================\n",
      "\n",
      "Horizon distribution in test set:\n",
      "horizon_calculated\n",
      "1      203\n",
      "2      203\n",
      "3      203\n",
      "4      203\n",
      "5      203\n",
      "      ... \n",
      "146    203\n",
      "147    203\n",
      "148    203\n",
      "149    203\n",
      "150    203\n",
      "Name: count, Length: 150, dtype: int64\n",
      "\n",
      "Horizon statistics:\n",
      "count    30450.000000\n",
      "mean        75.500000\n",
      "std         43.301019\n",
      "min          1.000000\n",
      "25%         38.000000\n",
      "50%         75.500000\n",
      "75%        113.000000\n",
      "max        150.000000\n",
      "Name: horizon_calculated, dtype: float64\n",
      "\n",
      "Compare to TRAINING horizons: [7, 30, 60, 90, 150]\n",
      "Training horizons: [7, 30, 60, 90, 150]\n",
      "\n",
      "‚ö†Ô∏è  CRITICAL CHECK:\n",
      "Max test horizon: 150\n",
      "Max train horizon: 150\n",
      "\n",
      "================================================================================\n",
      "CHECK 3: RM_ID OVERLAP BETWEEN TRAIN AND TEST\n",
      "================================================================================\n",
      "\n",
      "RM_IDs in training: 59\n",
      "RM_IDs in test: 203\n",
      "RM_IDs in BOTH: 59\n",
      "RM_IDs ONLY in test: 144\n",
      "RM_IDs ONLY in train: 0\n",
      "\n",
      "‚ö†Ô∏è  Test has 144 RM_IDs NOT seen in training!\n",
      "These are: [342, 343, 345, 346, 347, 348, 353, 354, 355, 357]...\n",
      "\n",
      "================================================================================\n",
      "CHECK 4: FEATURE DISTRIBUTION (TRAIN VS TEST)\n",
      "================================================================================\n",
      "\n",
      "Regenerating test features for comparison...\n",
      "\n",
      "Key feature comparison (sample):\n",
      "\n",
      "TRAIN (validation set):\n",
      "  days_since_last: mean=47.6, median=21.0\n",
      "  count_365d: mean=108.1, median=17.0\n",
      "  total_weight_365d: mean=1532133.3, median=303333.5\n",
      "\n",
      "TEST:\n",
      "  days_since_last: mean=999.0, median=999.0\n",
      "  count_365d: mean=0.0, median=0.0\n",
      "  total_weight_365d: mean=0.0, median=0.0\n",
      "\n",
      "================================================================================\n",
      "CHECK 5: SUBMISSION SANITY CHECKS\n",
      "================================================================================\n",
      "\n",
      "Submission statistics:\n",
      "count    3.045000e+04\n",
      "mean     5.399734e+04\n",
      "std      1.592215e+05\n",
      "min      0.000000e+00\n",
      "25%      7.017889e+03\n",
      "50%      1.657861e+04\n",
      "75%      3.170127e+04\n",
      "max      1.293209e+06\n",
      "Name: predicted_weight, dtype: float64\n",
      "\n",
      "Submission vs validation predictions:\n",
      "Val mean: 220,249 kg\n",
      "Submission mean: 53,997 kg\n",
      "Ratio: 0.25x\n",
      "\n",
      "Prediction distribution:\n",
      "  Zeros: 4558 (15.0%)\n",
      "  < 1000: 4558\n",
      "  1000-10000: 5927\n",
      "  10000-100000: 17938\n",
      "  > 100000: 2027\n",
      "\n",
      "================================================================================\n",
      "CHECK 6: SAMPLE PREDICTIONS INSPECTION\n",
      "================================================================================\n",
      "\n",
      "Sample predictions (first 20):\n",
      "    ID  rm_id  horizon  predicted_weight\n",
      "0    1    365        1      12814.621516\n",
      "1    2    365        2      13697.129494\n",
      "2    3    365        3      17327.703234\n",
      "3    4    365        4      17327.703234\n",
      "4    5    365        5      17327.703234\n",
      "5    6    365        6      17327.703234\n",
      "6    7    365        7      18104.509437\n",
      "7    8    365        8      18104.509437\n",
      "8    9    365        9      18104.509437\n",
      "9   10    365       10      18104.509437\n",
      "10  11    365       11      18104.509437\n",
      "11  12    365       12      18104.509437\n",
      "12  13    365       13      18104.509437\n",
      "13  14    365       14      18104.509437\n",
      "14  15    365       15      18104.509437\n",
      "15  16    365       16      18104.509437\n",
      "16  17    365       17      18104.509437\n",
      "17  18    365       18      18104.509437\n",
      "18  19    365       19      27173.959489\n",
      "19  20    365       20      27173.959489\n",
      "\n",
      "Predictions by horizon (test set):\n",
      "         count          mean        median  min           max\n",
      "horizon                                                      \n",
      "1          203   2549.810546      0.000000  0.0  2.642357e+04\n",
      "2          203   3766.451302      0.000000  0.0  5.606531e+04\n",
      "3          203   4186.682354      0.000000  0.0  5.775582e+04\n",
      "4          203   5061.878961      0.000000  0.0  1.013801e+05\n",
      "5          203   5861.824336      0.000000  0.0  1.102398e+05\n",
      "...        ...           ...           ...  ...           ...\n",
      "146        203  83530.167559  25343.403081  0.0  1.293209e+06\n",
      "147        203  83632.245679  25343.403081  0.0  1.293209e+06\n",
      "148        203  83975.733202  25343.403081  0.0  1.293209e+06\n",
      "149        203  83995.273715  25343.403081  0.0  1.293209e+06\n",
      "150        203  84074.331864  25343.403081  0.0  1.293209e+06\n",
      "\n",
      "[150 rows x 5 columns]\n",
      "\n",
      "Compare to TRAINING by horizon:\n",
      "                  count           mean   median  min        max\n",
      "forecast_horizon                                               \n",
      "7                   545   31630.220183      0.0  0.0   465093.0\n",
      "30                  545  147938.988991  23316.0  0.0  2421036.0\n",
      "60                  545  290364.943119  47580.0  0.0  4478833.0\n",
      "90                  545  420068.598165  69301.0  0.0  6272655.0\n",
      "150                 545  634389.563303  96360.0  0.0  8900645.0\n",
      "\n",
      "================================================================================\n",
      "LIKELY CAUSES OF SCORE EXPLOSION\n",
      "================================================================================\n",
      "\n",
      "üö® ISSUES FOUND:\n",
      "  2. NEW RM_IDs: 144 RM_IDs in test but not in training\n",
      "  3. STALE DATA: Test RM_IDs much more inactive (days_since ratio: 21.0x)\n",
      "  4. NOT ENOUGH ZEROS: Only 15.0% zeros (expected >30%)\n",
      "\n",
      "================================================================================\n",
      "DIAGNOSTIC COMPLETE - REVIEW RESULTS ABOVE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EMERGENCY DIAGNOSTIC: WHY DID TEST SCORE BLOW UP?\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD DATA\n",
    "# ============================================================================\n",
    "print(\"\\n[1] LOADING DATA\")\n",
    "receivals = pd.read_csv('./Project_materials/data/kernel/receivals.csv')\n",
    "prediction_mapping = pd.read_csv('./Project_materials/data/prediction_mapping.csv')\n",
    "submission = pd.read_csv('submission_hybrid.csv')\n",
    "train_df = pd.read_csv('training_data_hybrid.csv')\n",
    "\n",
    "# Convert dates\n",
    "receivals['date_arrival'] = pd.to_datetime(receivals['date_arrival'], utc=True).dt.tz_localize(None)\n",
    "receivals = receivals[receivals['net_weight'] > 0]\n",
    "receivals = receivals[receivals['rm_id'].notna()]\n",
    "\n",
    "print(f\"Receivals: {len(receivals)}\")\n",
    "print(f\"Prediction mapping: {len(prediction_mapping)}\")\n",
    "print(f\"Submission: {len(submission)}\")\n",
    "print(f\"Training data: {len(train_df)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CHECK 1: PREDICTION MAPPING FORMAT\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECK 1: PREDICTION MAPPING STRUCTURE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nPrediction mapping columns:\")\n",
    "print(prediction_mapping.columns.tolist())\n",
    "\n",
    "print(\"\\nFirst 10 rows:\")\n",
    "print(prediction_mapping.head(10))\n",
    "\n",
    "print(\"\\nUnique RM_IDs in prediction mapping:\")\n",
    "print(f\"Count: {prediction_mapping['rm_id'].nunique()}\")\n",
    "print(f\"RM_IDs: {sorted(prediction_mapping['rm_id'].unique())}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CHECK 2: HORIZON DISTRIBUTION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECK 2: HORIZON DISTRIBUTION (CRITICAL!)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate horizons from prediction_mapping\n",
    "forecast_start = pd.to_datetime('2025-01-01')\n",
    "prediction_mapping['forecast_end'] = pd.to_datetime(prediction_mapping['forecast_end_date'])\n",
    "prediction_mapping['horizon_calculated'] = (prediction_mapping['forecast_end'] - forecast_start).dt.days\n",
    "\n",
    "print(\"\\nHorizon distribution in test set:\")\n",
    "print(prediction_mapping['horizon_calculated'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nHorizon statistics:\")\n",
    "print(prediction_mapping['horizon_calculated'].describe())\n",
    "\n",
    "print(\"\\nCompare to TRAINING horizons: [7, 30, 60, 90, 150]\")\n",
    "train_horizons = train_df['forecast_horizon'].unique()\n",
    "print(f\"Training horizons: {sorted(train_horizons)}\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  CRITICAL CHECK:\")\n",
    "max_test_horizon = prediction_mapping['horizon_calculated'].max()\n",
    "max_train_horizon = train_df['forecast_horizon'].max()\n",
    "print(f\"Max test horizon: {max_test_horizon}\")\n",
    "print(f\"Max train horizon: {max_train_horizon}\")\n",
    "\n",
    "if max_test_horizon > max_train_horizon:\n",
    "    print(\"üö® EXTRAPOLATION ALERT: Test horizons exceed training horizons!\")\n",
    "    print(\"   Model is extrapolating beyond what it was trained on!\")\n",
    "\n",
    "# ============================================================================\n",
    "# CHECK 3: RM_ID OVERLAP\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECK 3: RM_ID OVERLAP BETWEEN TRAIN AND TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "train_rm_ids = set(train_df['rm_id'].unique())\n",
    "test_rm_ids = set(prediction_mapping['rm_id'].unique())\n",
    "\n",
    "overlap = train_rm_ids & test_rm_ids\n",
    "only_test = test_rm_ids - train_rm_ids\n",
    "only_train = train_rm_ids - test_rm_ids\n",
    "\n",
    "print(f\"\\nRM_IDs in training: {len(train_rm_ids)}\")\n",
    "print(f\"RM_IDs in test: {len(test_rm_ids)}\")\n",
    "print(f\"RM_IDs in BOTH: {len(overlap)}\")\n",
    "print(f\"RM_IDs ONLY in test: {len(only_test)}\")\n",
    "print(f\"RM_IDs ONLY in train: {len(only_train)}\")\n",
    "\n",
    "if len(only_test) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  Test has {len(only_test)} RM_IDs NOT seen in training!\")\n",
    "    print(f\"These are: {sorted(only_test)[:10]}...\")\n",
    "\n",
    "# ============================================================================\n",
    "# CHECK 4: FEATURE DISTRIBUTION COMPARISON\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECK 4: FEATURE DISTRIBUTION (TRAIN VS TEST)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load test features (need to regenerate with diagnostics)\n",
    "print(\"\\nRegenerating test features for comparison...\")\n",
    "\n",
    "# Quick feature generation for test set\n",
    "forecast_start = pd.to_datetime('2025-01-01')\n",
    "test_features = []\n",
    "\n",
    "for idx, row in prediction_mapping.head(100).iterrows():  # Sample first 100\n",
    "    rm_id = row['rm_id']\n",
    "    hist = receivals[\n",
    "        (receivals['rm_id'] == rm_id) &\n",
    "        (receivals['date_arrival'] < forecast_start)\n",
    "    ]\n",
    "    \n",
    "    cutoff_365 = forecast_start - timedelta(days=365)\n",
    "    recent_365 = hist[hist['date_arrival'] >= cutoff_365]\n",
    "    \n",
    "    if len(recent_365) > 0:\n",
    "        days_since = (forecast_start - recent_365['date_arrival'].max()).days\n",
    "        count_365 = len(recent_365)\n",
    "        total_365 = recent_365['net_weight'].sum()\n",
    "    else:\n",
    "        days_since = 999\n",
    "        count_365 = 0\n",
    "        total_365 = 0\n",
    "    \n",
    "    test_features.append({\n",
    "        'days_since_last': days_since,\n",
    "        'count_365d': count_365,\n",
    "        'total_weight_365d': total_365\n",
    "    })\n",
    "\n",
    "test_feat_df = pd.DataFrame(test_features)\n",
    "\n",
    "print(\"\\nKey feature comparison (sample):\")\n",
    "print(\"\\nTRAIN (validation set):\")\n",
    "val_data = train_df[train_df['train_date'] >= '2024-09-01']\n",
    "print(f\"  days_since_last: mean={val_data['days_since_last'].mean():.1f}, median={val_data['days_since_last'].median():.1f}\")\n",
    "print(f\"  count_365d: mean={val_data['count_365d'].mean():.1f}, median={val_data['count_365d'].median():.1f}\")\n",
    "print(f\"  total_weight_365d: mean={val_data['total_weight_365d'].mean():.1f}, median={val_data['total_weight_365d'].median():.1f}\")\n",
    "\n",
    "print(\"\\nTEST:\")\n",
    "print(f\"  days_since_last: mean={test_feat_df['days_since_last'].mean():.1f}, median={test_feat_df['days_since_last'].median():.1f}\")\n",
    "print(f\"  count_365d: mean={test_feat_df['count_365d'].mean():.1f}, median={test_feat_df['count_365d'].median():.1f}\")\n",
    "print(f\"  total_weight_365d: mean={test_feat_df['total_weight_365d'].mean():.1f}, median={test_feat_df['total_weight_365d'].median():.1f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CHECK 5: SUBMISSION SANITY CHECKS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECK 5: SUBMISSION SANITY CHECKS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nSubmission statistics:\")\n",
    "print(submission['predicted_weight'].describe())\n",
    "\n",
    "print(\"\\nSubmission vs validation predictions:\")\n",
    "print(f\"Val mean: {val_data['target'].mean():,.0f} kg\")\n",
    "print(f\"Submission mean: {submission['predicted_weight'].mean():,.0f} kg\")\n",
    "print(f\"Ratio: {submission['predicted_weight'].mean() / val_data['target'].mean():.2f}x\")\n",
    "\n",
    "print(\"\\nPrediction distribution:\")\n",
    "print(f\"  Zeros: {(submission['predicted_weight'] == 0).sum()} ({(submission['predicted_weight'] == 0).mean()*100:.1f}%)\")\n",
    "print(f\"  < 1000: {(submission['predicted_weight'] < 1000).sum()}\")\n",
    "print(f\"  1000-10000: {((submission['predicted_weight'] >= 1000) & (submission['predicted_weight'] < 10000)).sum()}\")\n",
    "print(f\"  10000-100000: {((submission['predicted_weight'] >= 10000) & (submission['predicted_weight'] < 100000)).sum()}\")\n",
    "print(f\"  > 100000: {(submission['predicted_weight'] >= 100000).sum()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CHECK 6: SAMPLE PREDICTIONS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECK 6: SAMPLE PREDICTIONS INSPECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Merge submission with prediction_mapping\n",
    "merged = prediction_mapping.merge(submission, on='ID')\n",
    "merged['horizon'] = (pd.to_datetime(merged['forecast_end_date']) - forecast_start).dt.days\n",
    "\n",
    "print(\"\\nSample predictions (first 20):\")\n",
    "print(merged[['ID', 'rm_id', 'horizon', 'predicted_weight']].head(20))\n",
    "\n",
    "print(\"\\nPredictions by horizon (test set):\")\n",
    "horizon_stats = merged.groupby('horizon')['predicted_weight'].agg(['count', 'mean', 'median', 'min', 'max'])\n",
    "print(horizon_stats)\n",
    "\n",
    "print(\"\\nCompare to TRAINING by horizon:\")\n",
    "train_horizon_stats = train_df.groupby('forecast_horizon')['target'].agg(['count', 'mean', 'median', 'min', 'max'])\n",
    "print(train_horizon_stats)\n",
    "\n",
    "# ============================================================================\n",
    "# HYPOTHESIS GENERATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LIKELY CAUSES OF SCORE EXPLOSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "issues = []\n",
    "\n",
    "if max_test_horizon > max_train_horizon:\n",
    "    issues.append(f\"1. EXTRAPOLATION: Test horizons ({max_test_horizon}) > Training horizons ({max_train_horizon})\")\n",
    "\n",
    "if len(only_test) > 0:\n",
    "    issues.append(f\"2. NEW RM_IDs: {len(only_test)} RM_IDs in test but not in training\")\n",
    "\n",
    "days_since_ratio = test_feat_df['days_since_last'].mean() / val_data['days_since_last'].mean()\n",
    "if days_since_ratio > 2:\n",
    "    issues.append(f\"3. STALE DATA: Test RM_IDs much more inactive (days_since ratio: {days_since_ratio:.1f}x)\")\n",
    "\n",
    "if (submission['predicted_weight'] == 0).mean() < 0.30:\n",
    "    issues.append(f\"4. NOT ENOUGH ZEROS: Only {(submission['predicted_weight'] == 0).mean()*100:.1f}% zeros (expected >30%)\")\n",
    "\n",
    "pred_ratio = submission['predicted_weight'].mean() / val_data['target'].mean()\n",
    "if pred_ratio > 1.5:\n",
    "    issues.append(f\"5. OVERESTIMATING: Test predictions {pred_ratio:.1f}x higher than validation targets\")\n",
    "\n",
    "if len(issues) > 0:\n",
    "    print(\"\\nüö® ISSUES FOUND:\")\n",
    "    for issue in issues:\n",
    "        print(f\"  {issue}\")\n",
    "else:\n",
    "    print(\"\\n‚úì No obvious issues found - deeper investigation needed\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIAGNOSTIC COMPLETE - REVIEW RESULTS ABOVE\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
