{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aa383ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can only use .dt accessor with datetimelike values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m receivals \u001b[38;5;241m=\u001b[39m receivals\u001b[38;5;241m.\u001b[39msort_values([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrm_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_arrival\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Focus on 2024 data\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m receivals_2024 \u001b[38;5;241m=\u001b[39m receivals[\u001b[43mreceivals\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate_arrival\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[38;5;241m.\u001b[39myear \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2024\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     18\u001b[0m active_rms \u001b[38;5;241m=\u001b[39m receivals_2024[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrm_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mActive RMs in 2024: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(active_rms)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:6299\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   6293\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   6294\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   6295\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   6296\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   6297\u001b[0m ):\n\u001b[0;32m   6298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 6299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\accessor.py:224\u001b[0m, in \u001b[0;36mCachedAccessor.__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessor\n\u001b[1;32m--> 224\u001b[0m accessor_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;66;03m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;66;03m# NDFrame\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, accessor_obj)\n",
      "File \u001b[1;32mc:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\accessors.py:643\u001b[0m, in \u001b[0;36mCombinedDatetimelikeProperties.__new__\u001b[1;34m(cls, data)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data\u001b[38;5;241m.\u001b[39mdtype, PeriodDtype):\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PeriodProperties(data, orig)\n\u001b[1;32m--> 643\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only use .dt accessor with datetimelike values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can only use .dt accessor with datetimelike values"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Comprehensive EDA to test breakthrough hypotheses for breaking the 6,000 barrier\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "receivals = pd.read_csv('./Project_materials/data/kernel/receivals.csv', parse_dates=['date_arrival'])\n",
    "receivals = receivals.sort_values(['rm_id', 'date_arrival'])\n",
    "\n",
    "# Focus on 2024 data\n",
    "receivals_2024 = receivals[receivals['date_arrival'].dt.year == 2024].copy()\n",
    "active_rms = receivals_2024['rm_id'].unique()\n",
    "print(f\"Active RMs in 2024: {len(active_rms)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# HYPOTHESIS 1: DELIVERY MOMENTUM PATTERNS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPOTHESIS 1: DELIVERY MOMENTUM PATTERNS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def analyze_delivery_bursts(df):\n",
    "    \"\"\"Analyze burst patterns in deliveries\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for rm_id in active_rms[:20]:  # Sample for speed\n",
    "        rm_data = df[df['rm_id'] == rm_id].copy()\n",
    "        if len(rm_data) < 5:\n",
    "            continue\n",
    "            \n",
    "        # Calculate gaps between deliveries\n",
    "        rm_data = rm_data.sort_values('date_arrival')\n",
    "        gaps = rm_data['date_arrival'].diff().dt.days.dropna()\n",
    "        \n",
    "        # Define burst as gap < 7 days\n",
    "        burst_threshold = 7\n",
    "        \n",
    "        # Find bursts\n",
    "        bursts = []\n",
    "        current_burst = []\n",
    "        \n",
    "        for i, gap in enumerate(gaps):\n",
    "            if gap <= burst_threshold:\n",
    "                if not current_burst:\n",
    "                    current_burst.append(i)\n",
    "                current_burst.append(i+1)\n",
    "            else:\n",
    "                if len(current_burst) > 1:\n",
    "                    bursts.append(len(current_burst))\n",
    "                current_burst = []\n",
    "        \n",
    "        if len(current_burst) > 1:\n",
    "            bursts.append(len(current_burst))\n",
    "        \n",
    "        results.append({\n",
    "            'rm_id': rm_id,\n",
    "            'total_deliveries': len(rm_data),\n",
    "            'num_bursts': len(bursts),\n",
    "            'avg_burst_length': np.mean(bursts) if bursts else 0,\n",
    "            'max_burst_length': max(bursts) if bursts else 0,\n",
    "            'avg_gap_days': gaps.mean(),\n",
    "            'median_gap_days': gaps.median(),\n",
    "            'gap_std': gaps.std(),\n",
    "            'pct_in_bursts': sum(bursts) / len(rm_data) * 100 if bursts else 0\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "burst_analysis = analyze_delivery_bursts(receivals_2024)\n",
    "print(\"\\nBurst Analysis Summary:\")\n",
    "print(burst_analysis.describe())\n",
    "print(\"\\nKey Findings:\")\n",
    "print(f\"- Average % of deliveries in bursts: {burst_analysis['pct_in_bursts'].mean():.1f}%\")\n",
    "print(f\"- Average burst length: {burst_analysis['avg_burst_length'].mean():.1f} deliveries\")\n",
    "print(f\"- Median gap between deliveries: {burst_analysis['median_gap_days'].median():.1f} days\")\n",
    "\n",
    "# Momentum Analysis\n",
    "print(\"\\nMomentum Analysis:\")\n",
    "for rm_id in active_rms[:10]:\n",
    "    rm_data = receivals_2024[receivals_2024['rm_id'] == rm_id].copy()\n",
    "    if len(rm_data) < 10:\n",
    "        continue\n",
    "    \n",
    "    # Calculate rolling momentum (acceleration of delivery rate)\n",
    "    rm_data = rm_data.sort_values('date_arrival')\n",
    "    rm_data['days_since_start'] = (rm_data['date_arrival'] - rm_data['date_arrival'].min()).dt.days\n",
    "    \n",
    "    # 30-day rolling sum\n",
    "    rm_data.set_index('date_arrival', inplace=True)\n",
    "    rm_data['rolling_30d'] = rm_data['net_weight'].rolling('30D').sum()\n",
    "    rm_data['rolling_7d'] = rm_data['net_weight'].rolling('7D').sum()\n",
    "    \n",
    "    # Momentum = recent/longer-term\n",
    "    rm_data['momentum'] = rm_data['rolling_7d'] / (rm_data['rolling_30d'] + 1)\n",
    "    \n",
    "    # Check if momentum predicts next period\n",
    "    rm_data['next_7d'] = rm_data['net_weight'].shift(-1).rolling('7D').sum()\n",
    "    \n",
    "    correlation = rm_data[['momentum', 'next_7d']].corr().iloc[0, 1]\n",
    "    if not np.isnan(correlation):\n",
    "        print(f\"RM {rm_id}: Momentum-Future correlation = {correlation:.3f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# HYPOTHESIS 2: RM INTERDEPENDENCIES\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPOTHESIS 2: RM INTERDEPENDENCIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create daily delivery matrix\n",
    "daily_deliveries = receivals_2024.groupby(['date_arrival', 'rm_id'])['net_weight'].sum().unstack(fill_value=0)\n",
    "daily_binary = (daily_deliveries > 0).astype(int)\n",
    "\n",
    "# Co-occurrence analysis\n",
    "print(\"\\nCo-occurrence Analysis (sampling 20 RMs):\")\n",
    "sample_rms = active_rms[:20]\n",
    "cooccurrence = pd.DataFrame(index=sample_rms, columns=sample_rms, dtype=float)\n",
    "\n",
    "for rm1 in sample_rms:\n",
    "    for rm2 in sample_rms:\n",
    "        if rm1 != rm2 and rm1 in daily_binary.columns and rm2 in daily_binary.columns:\n",
    "            # Count days both delivered\n",
    "            both_delivered = ((daily_binary[rm1] == 1) & (daily_binary[rm2] == 1)).sum()\n",
    "            rm1_delivered = (daily_binary[rm1] == 1).sum()\n",
    "            if rm1_delivered > 0:\n",
    "                cooccurrence.loc[rm1, rm2] = both_delivered / rm1_delivered\n",
    "\n",
    "# Find strong co-occurrences\n",
    "strong_pairs = []\n",
    "for i, rm1 in enumerate(sample_rms):\n",
    "    for j, rm2 in enumerate(sample_rms):\n",
    "        if i < j and not pd.isna(cooccurrence.loc[rm1, rm2]):\n",
    "            score = cooccurrence.loc[rm1, rm2]\n",
    "            if score > 0.3:  # 30% co-occurrence\n",
    "                strong_pairs.append((rm1, rm2, score))\n",
    "\n",
    "strong_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "print(f\"\\nFound {len(strong_pairs)} strong co-occurrence pairs (>30%)\")\n",
    "if strong_pairs:\n",
    "    print(\"Top 5 co-occurring pairs:\")\n",
    "    for rm1, rm2, score in strong_pairs[:5]:\n",
    "        print(f\"  RM {rm1} - RM {rm2}: {score:.1%} co-occurrence\")\n",
    "\n",
    "# Supplier analysis\n",
    "print(\"\\nSupplier Clustering:\")\n",
    "supplier_groups = receivals_2024.groupby('supplier_id')['rm_id'].unique()\n",
    "multi_rm_suppliers = supplier_groups[supplier_groups.apply(len) > 1]\n",
    "print(f\"Suppliers with multiple RMs: {len(multi_rm_suppliers)}\")\n",
    "\n",
    "if len(multi_rm_suppliers) > 0:\n",
    "    print(\"\\nTop suppliers by RM diversity:\")\n",
    "    for supplier_id in multi_rm_suppliers.head().index:\n",
    "        rms = supplier_groups[supplier_id]\n",
    "        print(f\"  Supplier {supplier_id}: {len(rms)} RMs\")\n",
    "        \n",
    "        # Check if these RMs deliver together\n",
    "        supplier_deliveries = receivals_2024[receivals_2024['supplier_id'] == supplier_id]\n",
    "        dates_by_rm = supplier_deliveries.groupby('rm_id')['date_arrival'].apply(set)\n",
    "        \n",
    "        if len(dates_by_rm) > 1:\n",
    "            overlap = len(set.intersection(*dates_by_rm.values))\n",
    "            total = len(set.union(*dates_by_rm.values))\n",
    "            print(f\"    Date overlap: {overlap}/{total} = {overlap/total:.1%}\")\n",
    "\n",
    "# =============================================================================\n",
    "# HYPOTHESIS 3: VOLATILITY-BASED QUANTILE ADJUSTMENT\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPOTHESIS 3: VOLATILITY-BASED QUANTILE ADJUSTMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "volatility_analysis = []\n",
    "\n",
    "for rm_id in active_rms[:30]:  # Sample\n",
    "    rm_data = receivals_2024[receivals_2024['rm_id'] == rm_id].copy()\n",
    "    \n",
    "    if len(rm_data) < 5:\n",
    "        continue\n",
    "    \n",
    "    # Monthly aggregation\n",
    "    rm_monthly = rm_data.set_index('date_arrival').resample('M')['net_weight'].sum()\n",
    "    \n",
    "    if len(rm_monthly) > 2:\n",
    "        cv = rm_monthly.std() / rm_monthly.mean() if rm_monthly.mean() > 0 else np.inf\n",
    "        \n",
    "        # Calculate quantiles of actual deliveries\n",
    "        quantiles = [0.05, 0.10, 0.15, 0.20, 0.25, 0.30]\n",
    "        q_values = rm_monthly.quantile(quantiles).values\n",
    "        \n",
    "        volatility_analysis.append({\n",
    "            'rm_id': rm_id,\n",
    "            'cv': cv,\n",
    "            'mean_monthly': rm_monthly.mean(),\n",
    "            'std_monthly': rm_monthly.std(),\n",
    "            'q05': q_values[0] if len(q_values) > 0 else 0,\n",
    "            'q10': q_values[1] if len(q_values) > 1 else 0,\n",
    "            'q15': q_values[2] if len(q_values) > 2 else 0,\n",
    "            'q20': q_values[3] if len(q_values) > 3 else 0,\n",
    "            'q25': q_values[4] if len(q_values) > 4 else 0,\n",
    "            'q30': q_values[5] if len(q_values) > 5 else 0,\n",
    "            'zero_months': (rm_monthly == 0).sum(),\n",
    "            'total_months': len(rm_monthly)\n",
    "        })\n",
    "\n",
    "vol_df = pd.DataFrame(volatility_analysis)\n",
    "\n",
    "print(\"\\nVolatility Distribution:\")\n",
    "print(vol_df['cv'].describe())\n",
    "\n",
    "# Cluster by volatility\n",
    "vol_df['volatility_cluster'] = pd.qcut(vol_df['cv'], q=3, labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "print(\"\\nOptimal Quantiles by Volatility Cluster:\")\n",
    "for cluster in ['Low', 'Medium', 'High']:\n",
    "    cluster_data = vol_df[vol_df['volatility_cluster'] == cluster]\n",
    "    if len(cluster_data) > 0:\n",
    "        # Find which quantile best approximates 20% of mean\n",
    "        target = cluster_data['mean_monthly'].mean() * 0.2\n",
    "        \n",
    "        best_q = None\n",
    "        best_diff = np.inf\n",
    "        for q_col in ['q05', 'q10', 'q15', 'q20', 'q25', 'q30']:\n",
    "            diff = abs(cluster_data[q_col].mean() - target)\n",
    "            if diff < best_diff:\n",
    "                best_diff = diff\n",
    "                best_q = q_col\n",
    "        \n",
    "        print(f\"  {cluster} volatility (CV={cluster_data['cv'].mean():.2f}): Best quantile = {best_q}\")\n",
    "        print(f\"    Zero-month rate: {cluster_data['zero_months'].mean()/cluster_data['total_months'].mean():.1%}\")\n",
    "\n",
    "# =============================================================================\n",
    "# HYPOTHESIS 4: ZERO-INFLATED DISTRIBUTION\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPOTHESIS 4: ZERO-INFLATED DISTRIBUTION MODELING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "print(\"\\nAnalyzing delivery amount distributions (non-zero only):\")\n",
    "\n",
    "distribution_fits = []\n",
    "\n",
    "for rm_id in active_rms[:20]:\n",
    "    rm_data = receivals_2024[receivals_2024['rm_id'] == rm_id]\n",
    "    weights = rm_data['net_weight'].values\n",
    "    weights_nonzero = weights[weights > 0]\n",
    "    \n",
    "    if len(weights_nonzero) < 10:\n",
    "        continue\n",
    "    \n",
    "    # Fit various distributions\n",
    "    results = {\n",
    "        'rm_id': rm_id,\n",
    "        'zero_pct': 0,  # No zeros in delivery data\n",
    "        'mean': weights_nonzero.mean(),\n",
    "        'std': weights_nonzero.std(),\n",
    "        'skew': stats.skew(weights_nonzero),\n",
    "        'kurtosis': stats.kurtosis(weights_nonzero)\n",
    "    }\n",
    "    \n",
    "    # Test log-normal\n",
    "    log_weights = np.log(weights_nonzero + 1)\n",
    "    _, p_lognormal = stats.normaltest(log_weights)\n",
    "    results['lognormal_p'] = p_lognormal\n",
    "    \n",
    "    # Test exponential\n",
    "    _, p_exp = stats.kstest(weights_nonzero, 'expon', \n",
    "                            args=(weights_nonzero.min(), \n",
    "                                  weights_nonzero.mean() - weights_nonzero.min()))\n",
    "    results['exponential_p'] = p_exp\n",
    "    \n",
    "    distribution_fits.append(results)\n",
    "\n",
    "dist_df = pd.DataFrame(distribution_fits)\n",
    "\n",
    "print(\"\\nDistribution Characteristics:\")\n",
    "print(f\"Average skewness: {dist_df['skew'].mean():.2f}\")\n",
    "print(f\"Average kurtosis: {dist_df['kurtosis'].mean():.2f}\")\n",
    "print(f\"Log-normal fits (p>0.05): {(dist_df['lognormal_p'] > 0.05).sum()}/{len(dist_df)}\")\n",
    "print(f\"Exponential fits (p>0.05): {(dist_df['exponential_p'] > 0.05).sum()}/{len(dist_df)}\")\n",
    "\n",
    "# Analyze zero patterns at different aggregation levels\n",
    "print(\"\\nZero-inflation at different time aggregations:\")\n",
    "for window in [7, 14, 30]:\n",
    "    zero_rates = []\n",
    "    \n",
    "    for rm_id in active_rms[:30]:\n",
    "        rm_data = receivals_2024[receivals_2024['rm_id'] == rm_id].copy()\n",
    "        \n",
    "        # Create complete date range\n",
    "        date_range = pd.date_range(start='2024-01-01', end='2024-12-31', freq='D')\n",
    "        daily = rm_data.set_index('date_arrival').reindex(date_range, fill_value=0)['net_weight']\n",
    "        \n",
    "        # Rolling sum\n",
    "        rolling = daily.rolling(window).sum().dropna()\n",
    "        zero_rate = (rolling == 0).mean()\n",
    "        zero_rates.append(zero_rate)\n",
    "    \n",
    "    print(f\"  {window}-day windows: {np.mean(zero_rates):.1%} zero rate\")\n",
    "\n",
    "# =============================================================================\n",
    "# HYPOTHESIS 5: MICRO-SEASONALITY PATTERNS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPOTHESIS 5: MICRO-SEASONALITY PATTERNS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "receivals_2024['day_of_week'] = receivals_2024['date_arrival'].dt.dayofweek\n",
    "receivals_2024['day_of_month'] = receivals_2024['date_arrival'].dt.day\n",
    "receivals_2024['week_of_month'] = (receivals_2024['day_of_month'] - 1) // 7\n",
    "\n",
    "print(\"\\nDay-of-Week Effects:\")\n",
    "dow_stats = receivals_2024.groupby('day_of_week').agg({\n",
    "    'net_weight': ['sum', 'count', 'mean'],\n",
    "    'rm_id': 'nunique'\n",
    "})\n",
    "dow_stats.columns = ['total_weight', 'num_deliveries', 'avg_weight', 'unique_rms']\n",
    "dow_stats['pct_of_total'] = dow_stats['total_weight'] / dow_stats['total_weight'].sum() * 100\n",
    "dow_stats.index = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "print(dow_stats)\n",
    "\n",
    "print(\"\\nDay-of-Month Effects (grouped):\")\n",
    "receivals_2024['month_period'] = pd.cut(receivals_2024['day_of_month'], \n",
    "                                       bins=[0, 10, 20, 31], \n",
    "                                       labels=['Early', 'Mid', 'Late'])\n",
    "dom_stats = receivals_2024.groupby('month_period').agg({\n",
    "    'net_weight': ['sum', 'count', 'mean']\n",
    "})\n",
    "dom_stats.columns = ['total_weight', 'num_deliveries', 'avg_weight']\n",
    "dom_stats['pct_of_total'] = dom_stats['total_weight'] / dom_stats['total_weight'].sum() * 100\n",
    "print(dom_stats)\n",
    "\n",
    "print(\"\\nWeek-of-Month Effects:\")\n",
    "wom_stats = receivals_2024.groupby('week_of_month').agg({\n",
    "    'net_weight': ['sum', 'count']\n",
    "})\n",
    "wom_stats.columns = ['total_weight', 'num_deliveries']\n",
    "wom_stats['pct_of_total'] = wom_stats['total_weight'] / wom_stats['total_weight'].sum() * 100\n",
    "wom_stats.index = ['Week 1', 'Week 2', 'Week 3', 'Week 4', 'Week 5']\n",
    "print(wom_stats)\n",
    "\n",
    "# Monthly patterns\n",
    "print(\"\\nMonthly Patterns in 2024:\")\n",
    "monthly_stats = receivals_2024.groupby(receivals_2024['date_arrival'].dt.month).agg({\n",
    "    'net_weight': 'sum',\n",
    "    'rm_id': 'nunique'\n",
    "})\n",
    "monthly_stats.columns = ['total_weight', 'active_rms']\n",
    "monthly_stats['weight_per_rm'] = monthly_stats['total_weight'] / monthly_stats['active_rms']\n",
    "monthly_stats.index = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                       'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "print(monthly_stats)\n",
    "\n",
    "# =============================================================================\n",
    "# ADDITIONAL INSIGHTS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ADDITIONAL INSIGHTS FOR MODEL DESIGN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# RM-specific patterns\n",
    "print(\"\\nRM-Specific Delivery Patterns (Top 10 by volume):\")\n",
    "top_rms = receivals_2024.groupby('rm_id')['net_weight'].sum().nlargest(10).index\n",
    "\n",
    "for rm_id in top_rms:\n",
    "    rm_data = receivals_2024[receivals_2024['rm_id'] == rm_id]\n",
    "    \n",
    "    # Delivery frequency\n",
    "    days_active = (rm_data['date_arrival'].max() - rm_data['date_arrival'].min()).days + 1\n",
    "    delivery_frequency = len(rm_data) / days_active * 30  # Deliveries per 30 days\n",
    "    \n",
    "    # Typical delivery size\n",
    "    p20 = rm_data['net_weight'].quantile(0.2)\n",
    "    p50 = rm_data['net_weight'].quantile(0.5)\n",
    "    p80 = rm_data['net_weight'].quantile(0.8)\n",
    "    \n",
    "    print(f\"\\nRM {rm_id}:\")\n",
    "    print(f\"  Deliveries/month: {delivery_frequency:.1f}\")\n",
    "    print(f\"  Size quantiles - P20: {p20:,.0f}, P50: {p50:,.0f}, P80: {p80:,.0f}\")\n",
    "    print(f\"  P80/P20 ratio: {p80/p20:.1f}x\")\n",
    "\n",
    "# Test stationarity\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATIONARITY CHECK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compare first half vs second half of 2024\n",
    "first_half = receivals_2024[receivals_2024['date_arrival'] < '2024-07-01']\n",
    "second_half = receivals_2024[receivals_2024['date_arrival'] >= '2024-07-01']\n",
    "\n",
    "print(\"\\nFirst Half vs Second Half 2024:\")\n",
    "print(f\"First half total: {first_half['net_weight'].sum()/1e6:.1f}M kg\")\n",
    "print(f\"Second half total: {second_half['net_weight'].sum()/1e6:.1f}M kg\")\n",
    "print(f\"Ratio: {second_half['net_weight'].sum()/first_half['net_weight'].sum():.3f}\")\n",
    "\n",
    "# Per RM stationarity\n",
    "stationary_rms = []\n",
    "non_stationary_rms = []\n",
    "\n",
    "for rm_id in active_rms:\n",
    "    rm_first = first_half[first_half['rm_id'] == rm_id]['net_weight'].sum()\n",
    "    rm_second = second_half[second_half['rm_id'] == rm_id]['net_weight'].sum()\n",
    "    \n",
    "    if rm_first > 0 and rm_second > 0:\n",
    "        ratio = rm_second / rm_first\n",
    "        if 0.5 < ratio < 2.0:\n",
    "            stationary_rms.append(rm_id)\n",
    "        else:\n",
    "            non_stationary_rms.append((rm_id, ratio))\n",
    "\n",
    "print(f\"\\nStationary RMs (0.5 < ratio < 2.0): {len(stationary_rms)}/{len(active_rms)}\")\n",
    "print(f\"Non-stationary RMs: {len(non_stationary_rms)}/{len(active_rms)}\")\n",
    "\n",
    "if non_stationary_rms:\n",
    "    non_stationary_rms.sort(key=lambda x: abs(x[1] - 1), reverse=True)\n",
    "    print(\"\\nMost non-stationary RMs:\")\n",
    "    for rm_id, ratio in non_stationary_rms[:5]:\n",
    "        print(f\"  RM {rm_id}: {ratio:.2f}x change\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EDA COMPLETE - Ready for hypothesis evaluation\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
