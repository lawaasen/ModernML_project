{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1798021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] DATA CLEANING\n",
      "Clean receivals: 122383\n",
      "Using 11 training dates x 5 horizons\n",
      "Expected samples: ~3300 (only active rm_ids)\n",
      "Active rm_ids in 2024: 60\n",
      "Processing date 1/11: 2024-01-01...\n",
      "Processing date 2/11: 2024-02-01...\n",
      "Processing date 3/11: 2024-03-01...\n",
      "Processing date 4/11: 2024-04-01...\n",
      "Processing date 5/11: 2024-05-01...\n",
      "Processing date 6/11: 2024-06-01...\n",
      "Processing date 7/11: 2024-07-01...\n",
      "Processing date 8/11: 2024-08-01...\n",
      "Processing date 9/11: 2024-09-01...\n",
      "Processing date 10/11: 2024-10-01...\n",
      "Processing date 11/11: 2024-11-01...\n",
      "\n",
      "Generated 2725 training samples\n",
      "\n",
      "Training data statistics:\n",
      "             rm_id  forecast_horizon        month      quarter  \\\n",
      "count  2725.000000       2725.000000  2725.000000  2725.000000   \n",
      "mean   3082.660550         67.400000     6.264220     2.451376   \n",
      "std     771.694638         49.879385     3.184245     1.077879   \n",
      "min    2123.000000          7.000000     1.000000     1.000000   \n",
      "25%    2143.000000         30.000000     4.000000     2.000000   \n",
      "50%    3126.000000         60.000000     6.000000     2.000000   \n",
      "75%    3701.000000         90.000000     9.000000     3.000000   \n",
      "max    4481.000000        150.000000    11.000000     4.000000   \n",
      "\n",
      "       total_weight_365d  avg_weight_365d   count_365d  days_since_last  \\\n",
      "count       2.725000e+03      2725.000000  2725.000000      2725.000000   \n",
      "mean        1.639653e+06     14995.439451   114.519266        37.242202   \n",
      "std         3.155916e+06      8846.358038   202.142571        60.010095   \n",
      "min         0.000000e+00         0.000000     0.000000         0.000000   \n",
      "25%         5.000000e+04      5304.212121     3.000000         2.000000   \n",
      "50%         3.065070e+05     16938.000000    20.000000         9.000000   \n",
      "75%         1.224871e+06     23880.000000   153.000000        47.000000   \n",
      "max         1.728320e+07     25666.666667  1119.000000       364.000000   \n",
      "\n",
      "       total_weight_90d    count_90d  daily_rate_365d  future_po_quantity  \\\n",
      "count      2.725000e+03  2725.000000      2725.000000        2.725000e+03   \n",
      "mean       4.199874e+05    29.611009      4492.200573        1.826745e+06   \n",
      "std        8.330132e+05    54.853589      8646.344426        3.634202e+06   \n",
      "min        0.000000e+00     0.000000         0.000000        0.000000e+00   \n",
      "25%        1.486200e+04     1.000000       136.986301        0.000000e+00   \n",
      "50%        7.259000e+04     5.000000       839.745205        5.500000e+04   \n",
      "75%        3.259470e+05    39.000000      3355.810959        1.225000e+06   \n",
      "max        4.988811e+06   336.000000     47351.241096        2.296100e+07   \n",
      "\n",
      "       future_po_count        target  \n",
      "count      2725.000000  2.725000e+03  \n",
      "mean         13.802936  3.048785e+05  \n",
      "std          21.807235  7.993253e+05  \n",
      "min           0.000000  0.000000e+00  \n",
      "25%           0.000000  0.000000e+00  \n",
      "50%           2.000000  2.556000e+04  \n",
      "75%          19.000000  1.681800e+05  \n",
      "max         115.000000  8.900645e+06  \n",
      "\n",
      "Samples with target > 0: 1816 (66.6%)\n",
      "Training samples: 2180\n",
      "Validation samples: 545\n",
      "Training XGBoost (quantile=0.2)...\n",
      "[0]\tvalidation_0-quantile:66272.87527\n",
      "[100]\tvalidation_0-quantile:34569.84481\n",
      "[200]\tvalidation_0-quantile:30615.85082\n",
      "[299]\tvalidation_0-quantile:30504.99702\n",
      "\n",
      "[4] MAKING PREDICTIONS\n",
      "--------------------------------------------------------------------------------\n",
      "Pre-computed features for 203 rm_ids\n",
      "Processed 5000/30450...\n",
      "Processed 10000/30450...\n",
      "Processed 15000/30450...\n",
      "Processed 20000/30450...\n",
      "Processed 25000/30450...\n",
      "Processed 30000/30450...\n",
      "\n",
      "Prediction statistics:\n",
      "count    3.045000e+04\n",
      "mean     4.203081e+04\n",
      "std      1.915177e+05\n",
      "min      0.000000e+00\n",
      "25%      2.876423e+02\n",
      "50%      1.357464e+03\n",
      "75%      6.615583e+03\n",
      "max      2.824188e+06\n",
      "Name: predicted_weight, dtype: float64\n",
      "Predictions > 0: 24178\n",
      "\n",
      "[5] CREATING SUBMISSION\n",
      "--------------------------------------------------------------------------------\n",
      "Saved to 'xgboost_optimized_submission.csv'\n",
      "\n",
      "================================================================================\n",
      "COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import timedelta\n",
    "\n",
    "receivals = pd.read_csv('./Project_materials/data/kernel/receivals.csv')\n",
    "purchase_orders = pd.read_csv('./Project_materials/data/kernel/purchase_orders.csv')\n",
    "prediction_mapping = pd.read_csv('./Project_materials/data/prediction_mapping.csv')\n",
    "sample_submission = pd.read_csv('./Project_materials/data/sample_submission.csv')\n",
    "\n",
    "# Convert dates\n",
    "receivals['date_arrival'] = pd.to_datetime(receivals['date_arrival'], utc=True).dt.tz_localize(None)\n",
    "purchase_orders['delivery_date'] = pd.to_datetime(purchase_orders['delivery_date'], utc=True).dt.tz_localize(None)\n",
    "prediction_mapping['forecast_start_date'] = pd.to_datetime(prediction_mapping['forecast_start_date'])\n",
    "prediction_mapping['forecast_end_date'] = pd.to_datetime(prediction_mapping['forecast_end_date'])\n",
    "\n",
    "\n",
    "# clean data\n",
    "print(\"\\n[1] DATA CLEANING\")\n",
    "receivals = receivals[receivals['net_weight'] > 0]\n",
    "receivals = receivals[receivals['rm_id'].notna()]\n",
    "receivals = receivals.sort_values('date_arrival')  # Sort for efficiency\n",
    "print(f\"Clean receivals: {len(receivals)}\")\n",
    "\n",
    "# bare nyere dato, tok ekstremt lang tid med alle..\n",
    "train_dates = pd.date_range(start='2024-01-01', end='2024-11-30', freq='MS')  # Monthly start\n",
    "forecast_horizons = [7, 30, 60, 90, 150]  # Fewer horizons\n",
    "\n",
    "print(f\"Using {len(train_dates)} training dates x {len(forecast_horizons)} horizons\")\n",
    "print(f\"Expected samples: ~{len(train_dates) * 60 * len(forecast_horizons)} (only active rm_ids)\")\n",
    "\n",
    "training_data = []\n",
    "\n",
    "# Pre-compute which rm_ids are active in 2024\n",
    "active_rm_ids = receivals[receivals['date_arrival'] >= '2024-01-01']['rm_id'].unique()\n",
    "print(f\"Active rm_ids in 2024: {len(active_rm_ids)}\")\n",
    "\n",
    "for i, train_date in enumerate(train_dates):\n",
    "    print(f\"Processing date {i+1}/{len(train_dates)}: {train_date.date()}...\")\n",
    "    \n",
    "    # skip inactive ones for speed\n",
    "    for rm_id in active_rm_ids:\n",
    "        \n",
    "        # Get historical receivals before train_date\n",
    "        hist = receivals[\n",
    "            (receivals['rm_id'] == rm_id) &\n",
    "            (receivals['date_arrival'] < train_date)\n",
    "        ]\n",
    "        \n",
    "        if len(hist) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Pre-compute historical features\n",
    "        cutoff_365 = train_date - timedelta(days=365)\n",
    "        cutoff_90 = train_date - timedelta(days=90)\n",
    "        \n",
    "        recent_365 = hist[hist['date_arrival'] >= cutoff_365]\n",
    "        recent_90 = hist[hist['date_arrival'] >= cutoff_90]\n",
    "        \n",
    "        # Historical features\n",
    "        if len(recent_365) > 0:\n",
    "            total_365 = recent_365['net_weight'].sum()\n",
    "            avg_365 = recent_365['net_weight'].mean()\n",
    "            count_365 = len(recent_365)\n",
    "            days_since = (train_date - recent_365['date_arrival'].max()).days\n",
    "        else:\n",
    "            total_365 = avg_365 = count_365 = days_since = 0\n",
    "        \n",
    "        if len(recent_90) > 0:\n",
    "            total_90 = recent_90['net_weight'].sum()\n",
    "            count_90 = len(recent_90)\n",
    "        else:\n",
    "            total_90 = count_90 = 0\n",
    "        \n",
    "        daily_rate = total_365 / 365 if count_365 > 0 else 0\n",
    "        \n",
    "        # For each horizon\n",
    "        for horizon in forecast_horizons:\n",
    "            forecast_end = train_date + timedelta(days=horizon)\n",
    "            \n",
    "            # Get actual deliveries in window\n",
    "            actual = receivals[\n",
    "                (receivals['rm_id'] == rm_id) &\n",
    "                (receivals['date_arrival'] >= train_date) &\n",
    "                (receivals['date_arrival'] <= forecast_end)\n",
    "            ]\n",
    "            target = actual['net_weight'].sum()\n",
    "            \n",
    "            # Get future POs in window\n",
    "            rm_products = hist['product_id'].unique()\n",
    "            future_pos = purchase_orders[\n",
    "                (purchase_orders['product_id'].isin(rm_products)) &\n",
    "                (purchase_orders['delivery_date'] >= train_date) &\n",
    "                (purchase_orders['delivery_date'] <= forecast_end) &\n",
    "                (purchase_orders['status'] != 'Deleted')\n",
    "            ]\n",
    "            po_qty = future_pos['quantity'].sum() if len(future_pos) > 0 else 0\n",
    "            po_count = len(future_pos)\n",
    "            \n",
    "            # Create sample\n",
    "            training_data.append({\n",
    "                'rm_id': rm_id,\n",
    "                'forecast_horizon': horizon,\n",
    "                'month': train_date.month,\n",
    "                'quarter': train_date.quarter,\n",
    "                'total_weight_365d': total_365,\n",
    "                'avg_weight_365d': avg_365,\n",
    "                'count_365d': count_365,\n",
    "                'days_since_last': days_since,\n",
    "                'total_weight_90d': total_90,\n",
    "                'count_90d': count_90,\n",
    "                'daily_rate_365d': daily_rate,\n",
    "                'future_po_quantity': po_qty,\n",
    "                'future_po_count': po_count,\n",
    "                'target': target\n",
    "            })\n",
    "\n",
    "print(f\"\\nGenerated {len(training_data)} training samples\")\n",
    "train_df = pd.DataFrame(training_data)\n",
    "\n",
    "print(\"\\nTraining data statistics:\")\n",
    "print(train_df.describe())\n",
    "print(f\"\\nSamples with target > 0: {(train_df['target'] > 0).sum()} ({(train_df['target'] > 0).sum() / len(train_df) * 100:.1f}%)\")\n",
    "\n",
    "# TRAINING\n",
    "\n",
    "feature_cols = [c for c in train_df.columns if c != 'target']\n",
    "X = train_df[feature_cols]\n",
    "y = train_df['target']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "\n",
    "# XGBoost with quantile regression\n",
    "model = xgb.XGBRegressor(\n",
    "    objective='reg:quantileerror',\n",
    "    quantile_alpha=0.2,\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    tree_method='hist'  # raskere trening, fjern etterpå!\n",
    ")\n",
    "\n",
    "print(\"Training XGBoost (quantile=0.2)...\")\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "\n",
    "#PREDICTIOSNS\n",
    "print(\"\\n[4] MAKING PREDICTIONS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "forecast_start = pd.to_datetime('2025-01-01')\n",
    "\n",
    "# Pre-compute features for all rm_ids\n",
    "rm_features = {}\n",
    "\n",
    "for rm_id in prediction_mapping['rm_id'].unique():\n",
    "    hist = receivals[\n",
    "        (receivals['rm_id'] == rm_id) &\n",
    "        (receivals['date_arrival'] < forecast_start)\n",
    "    ]\n",
    "    \n",
    "    if len(hist) == 0:\n",
    "        rm_features[rm_id] = {\n",
    "            'total_weight_365d': 0,\n",
    "            'avg_weight_365d': 0,\n",
    "            'count_365d': 0,\n",
    "            'days_since_last': 999,\n",
    "            'total_weight_90d': 0,\n",
    "            'count_90d': 0,\n",
    "            'daily_rate_365d': 0,\n",
    "            'rm_products': []\n",
    "        }\n",
    "        continue\n",
    "    \n",
    "    cutoff_365 = forecast_start - timedelta(days=365)\n",
    "    cutoff_90 = forecast_start - timedelta(days=90)\n",
    "    \n",
    "    recent_365 = hist[hist['date_arrival'] >= cutoff_365]\n",
    "    recent_90 = hist[hist['date_arrival'] >= cutoff_90]\n",
    "    \n",
    "    if len(recent_365) > 0:\n",
    "        total_365 = recent_365['net_weight'].sum()\n",
    "        avg_365 = recent_365['net_weight'].mean()\n",
    "        count_365 = len(recent_365)\n",
    "        days_since = (forecast_start - recent_365['date_arrival'].max()).days\n",
    "    else:\n",
    "        total_365 = avg_365 = count_365 = days_since = 0\n",
    "    \n",
    "    if len(recent_90) > 0:\n",
    "        total_90 = recent_90['net_weight'].sum()\n",
    "        count_90 = len(recent_90)\n",
    "    else:\n",
    "        total_90 = count_90 = 0\n",
    "    \n",
    "    rm_features[rm_id] = {\n",
    "        'total_weight_365d': total_365,\n",
    "        'avg_weight_365d': avg_365,\n",
    "        'count_365d': count_365,\n",
    "        'days_since_last': days_since,\n",
    "        'total_weight_90d': total_90,\n",
    "        'count_90d': count_90,\n",
    "        'daily_rate_365d': total_365 / 365 if count_365 > 0 else 0,\n",
    "        'rm_products': hist['product_id'].unique()\n",
    "    }\n",
    "\n",
    "print(f\"Pre-computed features for {len(rm_features)} rm_ids\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = []\n",
    "\n",
    "for idx, row in prediction_mapping.iterrows():\n",
    "    rm_id = row['rm_id']\n",
    "    forecast_end = row['forecast_end_date']\n",
    "    horizon = (forecast_end - forecast_start).days + 1\n",
    "    \n",
    "    # Get pre-computed features\n",
    "    feat = rm_features[rm_id]\n",
    "    \n",
    "    # Get future POs\n",
    "    future_pos = purchase_orders[\n",
    "        (purchase_orders['product_id'].isin(feat['rm_products'])) &\n",
    "        (purchase_orders['delivery_date'] >= forecast_start) &\n",
    "        (purchase_orders['delivery_date'] <= forecast_end) &\n",
    "        (purchase_orders['status'] != 'Deleted')\n",
    "    ]\n",
    "    po_qty = future_pos['quantity'].sum() if len(future_pos) > 0 else 0\n",
    "    po_count = len(future_pos)\n",
    "    \n",
    "    # Create feature vector\n",
    "    feature_dict = {\n",
    "        'rm_id': rm_id,\n",
    "        'forecast_horizon': horizon,\n",
    "        'month': forecast_start.month,\n",
    "        'quarter': forecast_start.quarter,\n",
    "        'total_weight_365d': feat['total_weight_365d'],\n",
    "        'avg_weight_365d': feat['avg_weight_365d'],\n",
    "        'count_365d': feat['count_365d'],\n",
    "        'days_since_last': feat['days_since_last'],\n",
    "        'total_weight_90d': feat['total_weight_90d'],\n",
    "        'count_90d': feat['count_90d'],\n",
    "        'daily_rate_365d': feat['daily_rate_365d'],\n",
    "        'future_po_quantity': po_qty,\n",
    "        'future_po_count': po_count\n",
    "    }\n",
    "    \n",
    "    feature_vector = pd.DataFrame([feature_dict])[feature_cols]\n",
    "    pred = model.predict(feature_vector)[0]\n",
    "    pred = max(0, pred)\n",
    "    \n",
    "    predictions.append({'ID': row['ID'], 'predicted_weight': pred})\n",
    "    \n",
    "    if (idx + 1) % 5000 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(prediction_mapping)}...\")\n",
    "\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "\n",
    "print(\"\\nPrediction statistics:\")\n",
    "print(predictions_df['predicted_weight'].describe())\n",
    "print(f\"Predictions > 0: {(predictions_df['predicted_weight'] > 0).sum()}\")\n",
    "\n",
    "# SUBMISSION\n",
    "print(\"\\n[5] CREATING SUBMISSION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "submission = sample_submission.copy()\n",
    "submission['predicted_weight'] = predictions_df['predicted_weight'].values\n",
    "submission.to_csv('xgboost_optimized_submission.csv', index=False)\n",
    "print(\"Saved to 'xgboost_optimized_submission.csv'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
